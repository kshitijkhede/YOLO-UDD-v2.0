{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "header"
      },
      "source": [
        "# üåä YOLO-UDD v2.0 - Underwater Debris Detection\n",
        "\n",
        "**Complete Training Pipeline on Google Colab with GPU** ‚ö° **OPTIMIZED**\n",
        "\n",
        "## üöÄ Quick Start:\n",
        "1. **Upload Dataset**: Upload your TrashCAN dataset to Google Drive (COCO format supported)\n",
        "2. **Enable GPU**: Runtime ‚Üí Change runtime type ‚Üí GPU (T4 or better)\n",
        "3. **Update Path**: Dataset paths are pre-configured for `/content/drive/My Drive/trashcan_dataset/`\n",
        "4. **Run All**: Runtime ‚Üí Run all (or run cells sequentially)\n",
        "5. **Monitor**: Training takes ~30-60 minutes depending on GPU and dataset size\n",
        "\n",
        "## ‚ö° Performance Optimizations:\n",
        "- **No dataset copying**: Uses symlinks (saves 5-10 minutes setup time)\n",
        "- **Direct Drive training**: Results saved directly to Drive in real-time\n",
        "- **Auto-save checkpoints**: Every epoch saved automatically\n",
        "- **Session-safe**: Results preserved even if Colab disconnects\n",
        "\n",
        "## üìã Prerequisites:\n",
        "- TrashCAN dataset uploaded to Google Drive at: `/content/drive/My Drive/trashcan_dataset/`\n",
        "- Dataset structure: COCO format with `instances_train_trashcan.json` and `instances_val_trashcan.json`\n",
        "- Images in `original_data/images/` folder\n",
        "- Sufficient Drive storage (~2-3 GB for dataset + checkpoints)\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "step1"
      },
      "source": [
        "## Step 1: Setup Environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "clone_repo",
        "outputId": "6f36b6d5-bcc4-4ab4-87d6-bcf11bf523ac"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'YOLO-UDD-v2.0'...\n",
            "remote: Enumerating objects: 113, done.\u001b[K\n",
            "remote: Counting objects: 100% (113/113), done.\u001b[K\n",
            "remote: Compressing objects: 100% (79/79), done.\u001b[K\n",
            "remote: Total 113 (delta 43), reused 99 (delta 31), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (113/113), 83.24 KiB | 3.62 MiB/s, done.\n",
            "Resolving deltas: 100% (43/43), done.\n",
            "/content/YOLO-UDD-v2.0\n",
            "\n",
            "============================================================\n",
            "‚úì Repository cloned successfully!\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "# Clone repository\n",
        "import os\n",
        "import sys\n",
        "\n",
        "# Ensure we're in /content directory first\n",
        "try:\n",
        "    os.chdir('/content')\n",
        "except:\n",
        "    pass\n",
        "\n",
        "# Remove existing directory if present\n",
        "if os.path.exists('/content/YOLO-UDD-v2.0'):\n",
        "    import shutil\n",
        "    shutil.rmtree('/content/YOLO-UDD-v2.0')\n",
        "    print(\"‚úì Cleaned existing directory\")\n",
        "\n",
        "# Clone fresh\n",
        "print(\"Cloning repository...\")\n",
        "!git clone https://github.com/kshitijkhede/YOLO-UDD-v2.0.git /content/YOLO-UDD-v2.0\n",
        "\n",
        "# Verify clone succeeded\n",
        "if not os.path.exists('/content/YOLO-UDD-v2.0'):\n",
        "    raise FileNotFoundError(\"Failed to clone repository. Please check your internet connection.\")\n",
        "\n",
        "# Change to repo directory\n",
        "os.chdir('/content/YOLO-UDD-v2.0')\n",
        "sys.path.insert(0, '/content/YOLO-UDD-v2.0')\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"‚úì Repository cloned successfully!\")\n",
        "print(f\"‚úì Working directory: {os.getcwd()}\")\n",
        "print(\"=\"*60)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Verify repository structure\n",
        "import os\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"üìÇ Repository Structure\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "required_dirs = ['models', 'scripts', 'data', 'utils', 'configs']\n",
        "required_files = ['requirements.txt', 'models/__init__.py', 'scripts/train.py']\n",
        "\n",
        "for dir_name in required_dirs:\n",
        "    status = \"‚úì\" if os.path.exists(dir_name) else \"‚úó\"\n",
        "    print(f\"{status} {dir_name}/\")\n",
        "\n",
        "print()\n",
        "for file_name in required_files:\n",
        "    status = \"‚úì\" if os.path.exists(file_name) else \"‚úó\"\n",
        "    print(f\"{status} {file_name}\")\n",
        "\n",
        "print(\"=\"*60)\n",
        "\n",
        "# List models package\n",
        "if os.path.exists('models'):\n",
        "    print(\"\\nüì¶ Models package contents:\")\n",
        "    for item in os.listdir('models'):\n",
        "        print(f\"  - {item}\")\n",
        "print(\"=\"*60)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "check_gpu",
        "outputId": "be7f5f94-68b7-4dc8-e0b0-4b3f8bdb47fb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "GPU Status Check\n",
            "============================================================\n",
            "‚úì GPU Available: Tesla T4\n",
            "‚úì CUDA Version: 12.6\n",
            "‚úì GPU Memory: 15.83 GB\n",
            "name, driver_version, memory.total [MiB]\n",
            "Tesla T4, 550.54.15, 15360 MiB\n"
          ]
        }
      ],
      "source": [
        "# Check GPU availability\n",
        "import torch\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"GPU Status Check\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"‚úì GPU Available: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"‚úì CUDA Version: {torch.version.cuda}\")\n",
        "    print(f\"‚úì GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è  No GPU detected!\")\n",
        "    print(\"   Go to: Runtime ‚Üí Change runtime type ‚Üí GPU\")\n",
        "\n",
        "!nvidia-smi --query-gpu=name,driver_version,memory.total --format=csv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "step2"
      },
      "source": [
        "## Step 2: Install Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "install_deps",
        "outputId": "f2079347-9fc4-48b3-feba-ee8e87df55c6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Installing dependencies...\n",
            "\n",
            "============================================================\n",
            "‚úì All dependencies installed successfully!\n",
            "‚úì PyTorch: 2.8.0+cu126\n",
            "‚úì Albumentations: 2.0.8\n",
            "‚úì OpenCV: 4.12.0\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "# Install all required packages\n",
        "print(\"Installing dependencies...\")\n",
        "\n",
        "!pip install -q torch torchvision torchaudio\n",
        "!pip install -q albumentations opencv-python-headless\n",
        "!pip install -q tensorboard pyyaml tqdm\n",
        "\n",
        "# Verify installations\n",
        "import torch\n",
        "import albumentations as A\n",
        "import cv2\n",
        "import yaml\n",
        "from tqdm import tqdm\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"‚úì All dependencies installed successfully!\")\n",
        "print(f\"‚úì PyTorch: {torch.__version__}\")\n",
        "print(f\"‚úì Albumentations: {A.__version__}\")\n",
        "print(f\"‚úì OpenCV: {cv2.__version__}\")\n",
        "print(\"=\"*60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "step3"
      },
      "source": [
        "## Step 3: Setup Dataset Directories"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "setup_dataset",
        "outputId": "60f37adb-047f-4d42-e735-8114ee7f35df"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úì Cleaned up existing directories\n",
            "\n",
            "============================================================\n",
            "‚úì Dataset directories created successfully!\n",
            "‚úì Using dummy data (auto-generated during training)\n",
            "============================================================\n",
            "data/trashcan\n",
            "data/trashcan/annotations\n",
            "data/trashcan/images\n",
            "data/trashcan/images/val\n",
            "data/trashcan/images/test\n",
            "data/trashcan/images/train\n"
          ]
        }
      ],
      "source": [
        "# Setup Dataset (Google Drive) - OPTIMIZED (No Copying!)\n",
        "from google.colab import drive\n",
        "import os\n",
        "import shutil\n",
        "import json\n",
        "\n",
        "# Mount Google Drive\n",
        "print(\"Mounting Google Drive...\")\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"üìÅ Dataset Setup - TrashCAN COCO Format (OPTIMIZED)\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Your dataset path on Google Drive (handle spaces in path)\n",
        "DRIVE_DATASET_ROOT = '/content/drive/My Drive/TrashCan_dataset'\n",
        "\n",
        "# Alternative paths to check (in case of different Drive structure)\n",
        "alternative_paths = [\n",
        "    '/content/drive/MyDrive/TrashCan_dataset',\n",
        "    '/content/drive/My Drive/TrashCan_dataset',\n",
        "    '/content/drive/MyDrive/trashcan_dataset',\n",
        "    '/content/drive/My Drive/trashcan_dataset',\n",
        "    '/content/drive/Shareddrives/trashcan_dataset'\n",
        "]\n",
        "\n",
        "# Find the correct path\n",
        "found_path = None\n",
        "for path in alternative_paths:\n",
        "    if os.path.exists(path):\n",
        "        found_path = path\n",
        "        DRIVE_DATASET_ROOT = path\n",
        "        break\n",
        "\n",
        "if not found_path:\n",
        "    print(\"‚ùå Dataset NOT found at any of these locations:\")\n",
        "    for path in alternative_paths:\n",
        "        print(f\"   - {path}\")\n",
        "    print(\"\\nüìã Contents of your Drive root:\")\n",
        "    try:\n",
        "        for item in os.listdir('/content/drive/My Drive')[:20]:\n",
        "            print(f\"   - {item}\")\n",
        "    except:\n",
        "        for item in os.listdir('/content/drive/MyDrive')[:20]:\n",
        "            print(f\"   - {item}\")\n",
        "    raise FileNotFoundError(\"Please check your dataset location on Google Drive\")\n",
        "\n",
        "print(f\"‚úì Found dataset root: {DRIVE_DATASET_ROOT}\")\n",
        "\n",
        "# Set up paths - USE DRIVE DIRECTLY (NO COPYING!)\n",
        "DRIVE_IMAGES_PATH = os.path.join(DRIVE_DATASET_ROOT, 'original_data/images')\n",
        "DRIVE_ANNOTATIONS_PATH = os.path.join(DRIVE_DATASET_ROOT, 'original_data/annotations')\n",
        "TRAIN_JSON = os.path.join(DRIVE_DATASET_ROOT, 'original_data/instances_train_trashcan.json')\n",
        "VAL_JSON = os.path.join(DRIVE_DATASET_ROOT, 'original_data/instances_val_trashcan.json')\n",
        "\n",
        "# Create symlinks instead of copying (MUCH FASTER!)\n",
        "target_path = '/content/YOLO-UDD-v2.0/data/trashcan'\n",
        "if os.path.exists(target_path):\n",
        "    shutil.rmtree(target_path)\n",
        "os.makedirs(target_path, exist_ok=True)\n",
        "\n",
        "# Just copy the small JSON files\n",
        "print(\"\\nüìã Copying COCO annotations (fast)...\")\n",
        "for json_file, split in [(TRAIN_JSON, 'train'), (VAL_JSON, 'val')]:\n",
        "    if os.path.exists(json_file):\n",
        "        dest = os.path.join(target_path, f'instances_{split}_trashcan.json')\n",
        "        shutil.copy2(json_file, dest)\n",
        "        \n",
        "        # Load and show stats\n",
        "        with open(json_file, 'r') as f:\n",
        "            coco_data = json.load(f)\n",
        "            num_images = len(coco_data.get('images', []))\n",
        "            num_annotations = len(coco_data.get('annotations', []))\n",
        "            num_categories = len(coco_data.get('categories', []))\n",
        "            print(f\"  ‚úì {split:5s}: {num_images:4d} images, {num_annotations:5d} annotations, {num_categories} classes\")\n",
        "    else:\n",
        "        print(f\"  ‚ö†Ô∏è  {split}: JSON not found at {json_file}\")\n",
        "\n",
        "# Create symlink to images (NO COPYING - saves 5-10 minutes!)\n",
        "print(\"\\nüñºÔ∏è  Linking images (instant)...\")\n",
        "if os.path.exists(DRIVE_IMAGES_PATH):\n",
        "    target_images = os.path.join(target_path, 'images')\n",
        "    os.symlink(DRIVE_IMAGES_PATH, target_images)\n",
        "    \n",
        "    # Count images\n",
        "    total_images = len([f for f in os.listdir(target_images) if f.endswith(('.jpg', '.png', '.jpeg'))])\n",
        "    print(f\"  ‚úì Linked {total_images} images from Drive (no copy needed!)\")\n",
        "else:\n",
        "    print(f\"  ‚ö†Ô∏è  Images path not found: {DRIVE_IMAGES_PATH}\")\n",
        "\n",
        "# Link annotations if exists\n",
        "if os.path.exists(DRIVE_ANNOTATIONS_PATH):\n",
        "    target_annotations = os.path.join(target_path, 'annotations')\n",
        "    os.symlink(DRIVE_ANNOTATIONS_PATH, target_annotations)\n",
        "    print(f\"  ‚úì Linked annotations from Drive\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"‚úì Dataset setup complete (OPTIMIZED - No copying!)\")\n",
        "print(f\"‚úì Target path: {target_path}\")\n",
        "print(\"‚úì Time saved: ~5-10 minutes\")\n",
        "print(\"=\"*60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3.1: Inspect Google Drive Dataset Structure üîç"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# DIAGNOSTIC: Check Google Drive Dataset Structure\n",
        "import os\n",
        "import json\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"üîç GOOGLE DRIVE DATASET INSPECTION\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Check if Drive is mounted\n",
        "if not os.path.exists('/content/drive'):\n",
        "    print(\"‚ùå Google Drive not mounted! Run the previous cell first.\")\n",
        "else:\n",
        "    print(\"‚úì Google Drive is mounted\\n\")\n",
        "    \n",
        "    # Find the dataset\n",
        "    if 'DRIVE_DATASET_ROOT' in globals():\n",
        "        dataset_root = DRIVE_DATASET_ROOT\n",
        "    else:\n",
        "        # Try to find it\n",
        "        possible_paths = [\n",
        "            '/content/drive/MyDrive/TrashCan_dataset',\n",
        "            '/content/drive/My Drive/TrashCan_dataset',\n",
        "            '/content/drive/MyDrive/trashcan_dataset',\n",
        "            '/content/drive/My Drive/trashcan_dataset',\n",
        "        ]\n",
        "        dataset_root = None\n",
        "        for path in possible_paths:\n",
        "            if os.path.exists(path):\n",
        "                dataset_root = path\n",
        "                break\n",
        "    \n",
        "    if dataset_root and os.path.exists(dataset_root):\n",
        "        print(f\"üìÇ Dataset Root: {dataset_root}\\n\")\n",
        "        print(\"-\"*70)\n",
        "        \n",
        "        # Show full directory tree\n",
        "        def show_tree(path, prefix=\"\", max_depth=3, current_depth=0):\n",
        "            if current_depth >= max_depth:\n",
        "                return\n",
        "            \n",
        "            try:\n",
        "                items = sorted(os.listdir(path))\n",
        "                dirs = [i for i in items if os.path.isdir(os.path.join(path, i))]\n",
        "                files = [i for i in items if os.path.isfile(os.path.join(path, i))]\n",
        "                \n",
        "                # Show directories first\n",
        "                for i, d in enumerate(dirs):\n",
        "                    is_last = (i == len(dirs) - 1) and len(files) == 0\n",
        "                    print(f\"{prefix}{'‚îî‚îÄ‚îÄ ' if is_last else '‚îú‚îÄ‚îÄ '}üìÅ {d}/\")\n",
        "                    new_prefix = prefix + (\"    \" if is_last else \"‚îÇ   \")\n",
        "                    show_tree(os.path.join(path, d), new_prefix, max_depth, current_depth + 1)\n",
        "                \n",
        "                # Show files\n",
        "                for i, f in enumerate(files):\n",
        "                    is_last = i == len(files) - 1\n",
        "                    file_path = os.path.join(path, f)\n",
        "                    size = os.path.getsize(file_path)\n",
        "                    if size < 1024:\n",
        "                        size_str = f\"{size} B\"\n",
        "                    elif size < 1024*1024:\n",
        "                        size_str = f\"{size/1024:.1f} KB\"\n",
        "                    else:\n",
        "                        size_str = f\"{size/(1024*1024):.1f} MB\"\n",
        "                    \n",
        "                    # Special marking for JSON files\n",
        "                    icon = \"üìã\" if f.endswith('.json') else \"üìÑ\"\n",
        "                    print(f\"{prefix}{'‚îî‚îÄ‚îÄ ' if is_last else '‚îú‚îÄ‚îÄ '}{icon} {f} ({size_str})\")\n",
        "            except PermissionError:\n",
        "                print(f\"{prefix}    [Permission Denied]\")\n",
        "        \n",
        "        print(f\"üìÅ {os.path.basename(dataset_root)}/\")\n",
        "        show_tree(dataset_root, \"\", max_depth=4)\n",
        "        \n",
        "        print(\"\\n\" + \"-\"*70)\n",
        "        \n",
        "        # Check for annotation files specifically\n",
        "        print(\"\\nüîé Searching for annotation files...\\n\")\n",
        "        \n",
        "        annotation_files = []\n",
        "        for root, dirs, files in os.walk(dataset_root):\n",
        "            for file in files:\n",
        "                if 'annotation' in file.lower() or file.endswith('.json'):\n",
        "                    full_path = os.path.join(root, file)\n",
        "                    rel_path = os.path.relpath(full_path, dataset_root)\n",
        "                    size = os.path.getsize(full_path) / 1024\n",
        "                    annotation_files.append((rel_path, size, full_path))\n",
        "        \n",
        "        if annotation_files:\n",
        "            print(f\"Found {len(annotation_files)} annotation/JSON files:\")\n",
        "            for rel_path, size, full_path in annotation_files:\n",
        "                print(f\"  üìã {rel_path} ({size:.1f} KB)\")\n",
        "                \n",
        "                # Try to peek inside JSON files\n",
        "                if full_path.endswith('.json') and size > 0:\n",
        "                    try:\n",
        "                        with open(full_path, 'r') as f:\n",
        "                            data = json.load(f)\n",
        "                            if isinstance(data, dict):\n",
        "                                images = len(data.get('images', []))\n",
        "                                annotations = len(data.get('annotations', []))\n",
        "                                categories = len(data.get('categories', []))\n",
        "                                print(f\"      ‚Üí Images: {images}, Annotations: {annotations}, Categories: {categories}\")\n",
        "                    except:\n",
        "                        print(f\"      ‚Üí Could not parse JSON\")\n",
        "        else:\n",
        "            print(\"‚ö†Ô∏è  No annotation or JSON files found!\")\n",
        "        \n",
        "        print(\"\\n\" + \"=\"*70)\n",
        "        print(\"üí° Next Steps:\")\n",
        "        print(\"   1. If you see annotation files with actual data (annotations > 0),\")\n",
        "        print(\"      update the paths in the dataset setup cell above\")\n",
        "        print(\"   2. If annotations are empty, you need to get the correct dataset\")\n",
        "        print(\"   3. If annotations are in a different format, we may need to convert them\")\n",
        "        print(\"=\"*70)\n",
        "        \n",
        "    else:\n",
        "        print(\"‚ùå Dataset not found in Google Drive!\")\n",
        "        print(\"\\nüìã Contents of Drive root:\")\n",
        "        try:\n",
        "            items = os.listdir('/content/drive/MyDrive')[:30]\n",
        "            for item in items:\n",
        "                print(f\"   - {item}\")\n",
        "        except:\n",
        "            try:\n",
        "                items = os.listdir('/content/drive/My Drive')[:30]\n",
        "                for item in items:\n",
        "                    print(f\"   - {item}\")\n",
        "            except Exception as e:\n",
        "                print(f\"   Error listing Drive: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "step4"
      },
      "source": [
        "## Step 4: Test Model Architecture"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3.5: Check GPU Memory & Clear Cache (Important for Training!)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check GPU memory status and clear cache\n",
        "import torch\n",
        "import gc\n",
        "import subprocess\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"üîç GPU Memory Status Check\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    # Get GPU info\n",
        "    gpu_name = torch.cuda.get_device_name(0)\n",
        "    gpu_memory_total = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
        "    gpu_memory_allocated = torch.cuda.memory_allocated(0) / 1024**3\n",
        "    gpu_memory_reserved = torch.cuda.memory_reserved(0) / 1024**3\n",
        "    gpu_memory_free = gpu_memory_total - (gpu_memory_reserved)\n",
        "    \n",
        "    print(f\"GPU Device:       {gpu_name}\")\n",
        "    print(f\"Total Memory:     {gpu_memory_total:.2f} GB\")\n",
        "    print(f\"Allocated:        {gpu_memory_allocated:.2f} GB\")\n",
        "    print(f\"Reserved:         {gpu_memory_reserved:.2f} GB\")\n",
        "    print(f\"Free:             {gpu_memory_free:.2f} GB\")\n",
        "    \n",
        "    # Check for other processes using GPU\n",
        "    print(\"\\n\" + \"-\"*60)\n",
        "    print(\"Checking for other GPU processes...\")\n",
        "    try:\n",
        "        result = subprocess.run(['nvidia-smi', '--query-compute-apps=pid,used_memory', '--format=csv,noheader,nounits'], \n",
        "                              capture_output=True, text=True, timeout=5)\n",
        "        if result.stdout.strip():\n",
        "            print(\"‚ö†Ô∏è  Other processes using GPU:\")\n",
        "            print(result.stdout)\n",
        "        else:\n",
        "            print(\"‚úì No other GPU processes detected\")\n",
        "    except:\n",
        "        print(\"‚úì Could not check GPU processes (this is OK)\")\n",
        "    \n",
        "    # Clear GPU cache\n",
        "    print(\"\\n\" + \"-\"*60)\n",
        "    print(\"Clearing GPU cache...\")\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "    \n",
        "    # Show memory after clearing\n",
        "    gpu_memory_allocated_after = torch.cuda.memory_allocated(0) / 1024**3\n",
        "    gpu_memory_reserved_after = torch.cuda.memory_reserved(0) / 1024**3\n",
        "    gpu_memory_free_after = gpu_memory_total - gpu_memory_reserved_after\n",
        "    \n",
        "    print(f\"After clearing:\")\n",
        "    print(f\"  Allocated:      {gpu_memory_allocated_after:.2f} GB (freed {gpu_memory_allocated - gpu_memory_allocated_after:.2f} GB)\")\n",
        "    print(f\"  Reserved:       {gpu_memory_reserved_after:.2f} GB (freed {gpu_memory_reserved - gpu_memory_reserved_after:.2f} GB)\")\n",
        "    print(f\"  Free:           {gpu_memory_free_after:.2f} GB\")\n",
        "    \n",
        "    # Warning if low memory\n",
        "    if gpu_memory_free_after < 8.0:\n",
        "        print(\"\\n‚ö†Ô∏è  WARNING: Low GPU memory available!\")\n",
        "        print(\"   You may encounter out-of-memory errors during training.\")\n",
        "        print(\"   Solutions:\")\n",
        "        print(\"   1. Restart runtime (Runtime ‚Üí Restart runtime)\")\n",
        "        print(\"   2. Use smaller batch size (batch_size=2 or 4)\")\n",
        "        print(\"   3. Close other GPU-heavy notebooks\")\n",
        "    else:\n",
        "        print(\"\\n‚úì GPU memory looks good for training!\")\n",
        "    \n",
        "else:\n",
        "    print(\"‚ö†Ô∏è  GPU not available - will use CPU (slower)\")\n",
        "\n",
        "print(\"=\"*60)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "test_model",
        "outputId": "5e2a7500-4b01-428e-80c8-9b8d836d6323"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Building YOLO-UDD v2.0 model...\n",
            "\n",
            "============================================================\n",
            "YOLO-UDD v2.0 Model Information\n",
            "============================================================\n",
            "Architecture: YOLO-UDD v2.0\n",
            "Backbone: YOLOv9c\n",
            "Neck: PSEM-enhanced PANet + TAFM\n",
            "Head: SDWH\n",
            "Total Parameters: 60,627,051\n",
            "Trainable Parameters: 60,627,051\n",
            "Input Size: 640x640\n",
            "Output Classes: 3\n",
            "============================================================\n",
            "\n",
            "Testing forward pass...\n",
            "\n",
            "‚úì Forward pass successful!\n",
            "‚úì Number of detection scales: 3\n",
            "‚úì Turbidity score shape: torch.Size([2, 1, 1, 1])\n",
            "‚úì Device: cuda\n",
            "\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "# Import and test the model\n",
        "import sys\n",
        "import os\n",
        "\n",
        "# Ensure we're in the repo directory and add to path\n",
        "repo_root = '/content/YOLO-UDD-v2.0'\n",
        "os.chdir(repo_root)\n",
        "sys.path.insert(0, repo_root)\n",
        "\n",
        "from models import build_yolo_udd\n",
        "import torch\n",
        "\n",
        "print(\"Building YOLO-UDD v2.0 model...\\n\")\n",
        "\n",
        "# Build model\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = build_yolo_udd(num_classes=3, pretrained=None)\n",
        "model = model.to(device)\n",
        "\n",
        "# Get model info\n",
        "model_info = model.get_model_info()\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"YOLO-UDD v2.0 Model Information\")\n",
        "print(\"=\"*60)\n",
        "for key, value in model_info.items():\n",
        "    print(f\"{key}: {value}\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Test forward pass\n",
        "print(\"\\nTesting forward pass...\")\n",
        "dummy_input = torch.randn(2, 3, 640, 640).to(device)\n",
        "\n",
        "with torch.no_grad():\n",
        "    predictions, turb_score = model(dummy_input)\n",
        "\n",
        "print(f\"\\n‚úì Forward pass successful!\")\n",
        "print(f\"‚úì Number of detection scales: {len(predictions)}\")\n",
        "print(f\"‚úì Turbidity score shape: {turb_score.shape}\")\n",
        "print(f\"‚úì Device: {device}\")\n",
        "print(\"\\n\" + \"=\"*60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "step5"
      },
      "source": [
        "## Step 5: Configure Training Parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "config_training",
        "outputId": "a2d8b41e-0478-4eda-9338-100e051ebd92"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "üöÄ Training Configuration\n",
            "============================================================\n",
            "Batch Size:     16\n",
            "Epochs:         10\n",
            "Learning Rate:  0.01\n",
            "Num Workers:    2\n",
            "Save Directory: runs/train\n",
            "Device:         GPU (CUDA)\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "# Training configuration - Save to Drive\n",
        "import os\n",
        "from datetime import datetime\n",
        "import torch\n",
        "import gc\n",
        "\n",
        "# Clear GPU cache before training\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "    print(\"‚úì Cleared GPU cache\")\n",
        "\n",
        "# OPTIMIZED SETTINGS for faster training\n",
        "BATCH_SIZE = 8       # Increased from 4 (faster training, still safe for most GPUs)\n",
        "EPOCHS = 300         # Full training as per research paper specification\n",
        "LEARNING_RATE = 0.01\n",
        "NUM_WORKERS = 4      # Increased from 2 for faster data loading\n",
        "\n",
        "# Save directly to Google Drive (preserves results even if session disconnects!)\n",
        "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "SAVE_DIR = f'/content/drive/MyDrive/YOLO-UDD-Results/run_{timestamp}'\n",
        "os.makedirs(SAVE_DIR, exist_ok=True)\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"üöÄ Training Configuration (SPEED OPTIMIZED)\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Batch Size:     {BATCH_SIZE} (balanced for speed & memory)\")\n",
        "print(f\"Epochs:         {EPOCHS}\")\n",
        "print(f\"Learning Rate:  {LEARNING_RATE}\")\n",
        "print(f\"Num Workers:    {NUM_WORKERS} (increased for faster data loading)\")\n",
        "print(f\"Save Directory: {SAVE_DIR}\")\n",
        "print(f\"Device:         {'GPU (CUDA)' if torch.cuda.is_available() else 'CPU'}\")\n",
        "if torch.cuda.is_available():\n",
        "    gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
        "    gpu_allocated = torch.cuda.memory_allocated(0) / 1024**3\n",
        "    gpu_reserved = torch.cuda.memory_reserved(0) / 1024**3\n",
        "    gpu_free = gpu_memory - gpu_reserved\n",
        "    print(f\"GPU Memory:     {gpu_memory:.2f} GB total, {gpu_free:.2f} GB free\")\n",
        "    \n",
        "    # Auto-adjust batch size based on available memory\n",
        "    if gpu_free < 6.0:\n",
        "        print(\"\\n‚ö†Ô∏è  Low GPU memory detected! Reducing batch size to 4...\")\n",
        "        BATCH_SIZE = 4\n",
        "    elif gpu_free > 10.0:\n",
        "        print(\"\\n‚úì High GPU memory available! Batch size 8 is optimal\")\n",
        "    \n",
        "print(\"\\nüíæ Intermediate results will be saved to Drive automatically!\")\n",
        "print(\"   ‚úì Checkpoints saved after each epoch\")\n",
        "print(\"   ‚úì TensorBoard logs updated in real-time\")\n",
        "print(\"   ‚úì Results preserved even if session disconnects\")\n",
        "print(\"\\n‚ö° Speed Optimizations Applied:\")\n",
        "print(\"   ‚úì Batch size: 8 (faster than 4, safer than 16)\")\n",
        "print(\"   ‚úì Workers: 4 (parallel data loading)\")\n",
        "print(\"   ‚úì GPU cache cleared before training\")\n",
        "print(\"\\n‚è±Ô∏è  Estimated Training Time:\")\n",
        "iterations_per_epoch = 5769 // BATCH_SIZE\n",
        "total_time_minutes = (iterations_per_epoch * EPOCHS * 0.5) / 60  # ~0.5s per iteration\n",
        "print(f\"   ‚Ä¢ {iterations_per_epoch} iterations/epoch\")\n",
        "print(f\"   ‚Ä¢ ~{total_time_minutes:.1f} minutes for {EPOCHS} epochs on GPU\")\n",
        "print(\"=\"*60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "step6"
      },
      "source": [
        "## Step 6: Start Training üéØ\n",
        "\n",
        "**This will take approximately:**\n",
        "- GPU (T4): ~90-150 minutes for 300 epochs (full training)\n",
        "- GPU (V100): ~45-75 minutes for 300 epochs\n",
        "- GPU (A100): ~20-40 minutes for 300 epochs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5.75: Pre-Training Checklist (Run this before training to avoid slowdowns!) ‚ö°"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5.5: Verify Dataset (Optional - Run if training fails)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Diagnose Dataset Issues\n",
        "import os\n",
        "import json\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"üîç Dataset Diagnostic\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Check dataset structure\n",
        "target_path = '/content/YOLO-UDD-v2.0/data/trashcan'\n",
        "print(f\"\\nüìÇ Dataset location: {target_path}\")\n",
        "print(f\"   Exists: {os.path.exists(target_path)}\")\n",
        "\n",
        "if os.path.exists(target_path):\n",
        "    print(f\"\\nüìã Contents:\")\n",
        "    for item in os.listdir(target_path):\n",
        "        path = os.path.join(target_path, item)\n",
        "        if os.path.isdir(path):\n",
        "            count = len(os.listdir(path))\n",
        "            print(f\"   üìÅ {item}/ ({count} items)\")\n",
        "        else:\n",
        "            size = os.path.getsize(path) / 1024\n",
        "            print(f\"   üìÑ {item} ({size:.1f} KB)\")\n",
        "\n",
        "# Check JSON files\n",
        "print(f\"\\nüîé Analyzing COCO JSON files:\")\n",
        "for split in ['train', 'val']:\n",
        "    json_path = os.path.join(target_path, f'instances_{split}_trashcan.json')\n",
        "    if os.path.exists(json_path):\n",
        "        with open(json_path, 'r') as f:\n",
        "            data = json.load(f)\n",
        "        \n",
        "        images = data.get('images', [])\n",
        "        annotations = data.get('annotations', [])\n",
        "        categories = data.get('categories', [])\n",
        "        \n",
        "        print(f\"\\n  {split.upper()}:\")\n",
        "        print(f\"    Images:      {len(images)}\")\n",
        "        print(f\"    Annotations: {len(annotations)}\")\n",
        "        print(f\"    Categories:  {len(categories)}\")\n",
        "        \n",
        "        if len(annotations) == 0:\n",
        "            print(f\"    ‚ö†Ô∏è  WARNING: No annotations found!\")\n",
        "            if len(images) > 0:\n",
        "                print(f\"       Dataset has {len(images)} images but 0 annotations\")\n",
        "                print(f\"       This will cause training to fail\")\n",
        "        \n",
        "        if len(categories) > 0:\n",
        "            print(f\"    Categories: {[c.get('name', 'unknown') for c in categories[:5]]}\")\n",
        "        \n",
        "        # Check image paths\n",
        "        if len(images) > 0:\n",
        "            sample_img = images[0]\n",
        "            img_filename = sample_img.get('file_name', '')\n",
        "            img_path = os.path.join(target_path, 'images', img_filename)\n",
        "            print(f\"    Sample image: {img_filename}\")\n",
        "            print(f\"    Image exists: {os.path.exists(img_path)}\")\n",
        "    else:\n",
        "        print(f\"\\n  {split.upper()}: ‚ùå JSON file not found\")\n",
        "\n",
        "# Check images directory\n",
        "images_dir = os.path.join(target_path, 'images')\n",
        "if os.path.exists(images_dir):\n",
        "    all_files = os.listdir(images_dir)\n",
        "    image_files = [f for f in all_files if f.endswith(('.jpg', '.png', '.jpeg'))]\n",
        "    print(f\"\\nüì∏ Images directory:\")\n",
        "    print(f\"    Total files: {len(all_files)}\")\n",
        "    print(f\"    Image files: {len(image_files)}\")\n",
        "    if len(image_files) > 0:\n",
        "        print(f\"    Samples: {image_files[:3]}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"üí° If annotations = 0, your COCO JSON files are empty!\")\n",
        "print(\"   ‚Üí Check your Drive folder for correct annotation files\")\n",
        "print(\"   ‚Üí Make sure instances_train_trashcan.json has actual data\")\n",
        "print(\"=\"*60)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "run_training",
        "outputId": "27f6a5df-dd59-4e9e-8003-ee60f905a641"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "üöÄ Starting Training...\n",
            "============================================================\n",
            "Command: /usr/bin/python3 scripts/train.py --config configs/train_config.yaml --data-dir data/trashcan --batch-size 16 --epochs 10 --lr 0.01 --save-dir runs/train\n",
            "============================================================\n",
            "\n",
            "\n",
            "============================================================\n",
            "‚ö†Ô∏è  Training ended with errors\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "# Run training with progress monitoring\n",
        "import subprocess\n",
        "import sys\n",
        "import time\n",
        "\n",
        "# Build command\n",
        "cmd = [\n",
        "    sys.executable, 'scripts/train.py',\n",
        "    '--config', 'configs/train_config.yaml',\n",
        "    '--data-dir', 'data/trashcan',\n",
        "    '--batch-size', str(BATCH_SIZE),\n",
        "    '--epochs', str(EPOCHS),\n",
        "    '--lr', str(LEARNING_RATE),\n",
        "    '--save-dir', SAVE_DIR\n",
        "]\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"üöÄ Starting Training...\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Command: {' '.join(cmd)}\")\n",
        "print(\"=\"*60 + \"\\n\")\n",
        "\n",
        "# Track start time\n",
        "start_time = time.time()\n",
        "\n",
        "# Run training with full output (to see any errors)\n",
        "try:\n",
        "    result = subprocess.run(cmd, check=True)\n",
        "    \n",
        "    # Calculate elapsed time\n",
        "    elapsed_time = time.time() - start_time\n",
        "    elapsed_minutes = elapsed_time / 60\n",
        "    \n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"‚úì Training completed successfully!\")\n",
        "    print(f\"‚úì Total time: {elapsed_minutes:.1f} minutes ({elapsed_time:.0f} seconds)\")\n",
        "    print(\"=\"*60)\n",
        "    \n",
        "except subprocess.CalledProcessError as e:\n",
        "    elapsed_time = time.time() - start_time\n",
        "    elapsed_minutes = elapsed_time / 60\n",
        "    \n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"‚ö†Ô∏è  Training ended with errors\")\n",
        "    print(f\"‚ö†Ô∏è  Exit code: {e.returncode}\")\n",
        "    print(f\"‚ö†Ô∏è  Time before error: {elapsed_minutes:.1f} minutes\")\n",
        "    print(\"=\"*60)\n",
        "    \n",
        "    # Provide specific troubleshooting based on common errors\n",
        "    print(\"\\n\udd27 Troubleshooting Tips:\")\n",
        "    if \"out of memory\" in str(e).lower() or e.returncode == 1:\n",
        "        print(\"   1. Reduce batch size to 4 or 2 in Cell 15\")\n",
        "        print(\"   2. Restart runtime: Runtime ‚Üí Restart runtime\")\n",
        "        print(\"   3. Check GPU status in Cell 12 (Step 3.5)\")\n",
        "    elif \"file not found\" in str(e).lower():\n",
        "        print(\"   1. Run diagnostic Cell 18 (Step 5.5)\")\n",
        "        print(\"   2. Check dataset paths in Cell 9\")\n",
        "    else:\n",
        "        print(\"   1. Scroll up to see the full error message\")\n",
        "        print(\"   2. Check GPU memory in Cell 12\")\n",
        "        print(\"   3. Verify dataset in Cell 18\")\n",
        "    \n",
        "    print(\"\\nüí° Tip: If taking too long, check:\")\n",
        "    print(\"   ‚Ä¢ GPU is enabled (Runtime ‚Üí Change runtime type ‚Üí GPU)\")\n",
        "    print(\"   ‚Ä¢ No other processes using GPU (Cell 12 shows this)\")\n",
        "    print(\"   ‚Ä¢ Dataset images are loading correctly (check warnings above)\")\n",
        "    \n",
        "    raise"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "step7"
      },
      "source": [
        "## Step 7: View Training Results üìä"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "view_results",
        "outputId": "916747bf-9ca8-4cfc-9c66-7851bf01161c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "Training Results\n",
            "============================================================\n",
            "‚ö†Ô∏è  Checkpoint directory not found\n",
            "\n",
            "üìÅ Output Directory Structure:\n",
            "Directory not found\n",
            "\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import glob\n",
        "\n",
        "# Check for checkpoints\n",
        "checkpoint_dir = f'{SAVE_DIR}/checkpoints'\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"Training Results\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "if os.path.exists(checkpoint_dir):\n",
        "    checkpoints = glob.glob(f\"{checkpoint_dir}/*.pt\")\n",
        "    if checkpoints:\n",
        "        print(\"\\nüì¶ Available Checkpoints:\")\n",
        "        for ckpt in sorted(checkpoints):\n",
        "            size_mb = os.path.getsize(ckpt) / (1024 * 1024)\n",
        "            print(f\"   {os.path.basename(ckpt)}: {size_mb:.2f} MB\")\n",
        "    else:\n",
        "        print(\"‚ö†Ô∏è  No checkpoints found\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è  Checkpoint directory not found\")\n",
        "\n",
        "# Show directory structure\n",
        "print(\"\\nüìÅ Output Directory Structure:\")\n",
        "!ls -lh {SAVE_DIR}/ 2>/dev/null || echo \"Directory not found\"\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "step8"
      },
      "source": [
        "## Step 8: Launch TensorBoard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tensorboard"
      },
      "outputs": [],
      "source": [
        "# Load TensorBoard\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir {SAVE_DIR}/logs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "step9"
      },
      "source": [
        "## Step 9: Download Trained Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "download_model"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "import os\n",
        "\n",
        "best_model = f'{SAVE_DIR}/checkpoints/best.pt'\n",
        "latest_model = f'{SAVE_DIR}/checkpoints/latest.pt'\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"Download Models\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "if os.path.exists(best_model):\n",
        "    size_mb = os.path.getsize(best_model) / (1024 * 1024)\n",
        "    print(f\"\\nüì• Downloading best.pt ({size_mb:.2f} MB)...\")\n",
        "    files.download(best_model)\n",
        "    print(\"‚úì Download complete!\")\n",
        "else:\n",
        "    print(\"\\n‚ö†Ô∏è  best.pt not found\")\n",
        "\n",
        "if os.path.exists(latest_model):\n",
        "    size_mb = os.path.getsize(latest_model) / (1024 * 1024)\n",
        "    print(f\"\\nüì• Downloading latest.pt ({size_mb:.2f} MB)...\")\n",
        "    files.download(latest_model)\n",
        "    print(\"‚úì Download complete!\")\n",
        "else:\n",
        "    print(\"\\n‚ö†Ô∏è  latest.pt not found\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "step10"
      },
      "source": [
        "## Step 10: View Results in Google Drive ‚úÖ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "save_to_drive"
      },
      "outputs": [],
      "source": [
        "# Results are already on Google Drive!\n",
        "import os\n",
        "import glob\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"üìÅ Your Results on Google Drive\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# SAVE_DIR is already pointing to Drive from Step 5\n",
        "print(f\"\\n‚úì All results saved to: {SAVE_DIR}\")\n",
        "print(\"\\nüì¶ Contents:\")\n",
        "\n",
        "# List checkpoints\n",
        "checkpoint_dir = f'{SAVE_DIR}/checkpoints'\n",
        "if os.path.exists(checkpoint_dir):\n",
        "    checkpoints = glob.glob(f\"{checkpoint_dir}/*.pt\")\n",
        "    if checkpoints:\n",
        "        print(\"\\n  Checkpoints:\")\n",
        "        for ckpt in sorted(checkpoints):\n",
        "            size_mb = os.path.getsize(ckpt) / (1024 * 1024)\n",
        "            print(f\"    ‚Ä¢ {os.path.basename(ckpt)}: {size_mb:.2f} MB\")\n",
        "\n",
        "# List logs\n",
        "log_dir = f'{SAVE_DIR}/logs'\n",
        "if os.path.exists(log_dir):\n",
        "    print(f\"\\n  TensorBoard Logs: {log_dir}\")\n",
        "\n",
        "print(\"\\nüí° Access anytime from Google Drive:\")\n",
        "print(\"   My Drive ‚Üí YOLO-UDD-Results ‚Üí run_[timestamp]\")\n",
        "print(\"\\n\" + \"=\"*60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "complete"
      },
      "source": [
        "---\n",
        "\n",
        "## üéâ Training Complete!\n",
        "\n",
        "### What You've Done:\n",
        "- ‚úÖ Trained YOLO-UDD v2.0 on GPU\n",
        "- ‚úÖ Generated model checkpoints\n",
        "- ‚úÖ Created TensorBoard logs\n",
        "- ‚úÖ Downloaded trained models\n",
        "\n",
        "### Next Steps:\n",
        "1. **Use the model** for inference on new images\n",
        "2. **Evaluate performance** on test dataset\n",
        "3. **Fine-tune** by adjusting hyperparameters\n",
        "4. **Deploy** for real-world applications\n",
        "\n",
        "### Repository:\n",
        "https://github.com/kshitijkhede/YOLO-UDD-v2.0\n",
        "\n",
        "---\n",
        "\n",
        "**Need help?** Open an issue on GitHub or check the documentation.\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
