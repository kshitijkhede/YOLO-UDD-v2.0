{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "72c8a22b",
   "metadata": {},
   "source": [
    "# üåä YOLO-UDD v2.0 - Kaggle Training (CLEAN VERSION)\n",
    "\n",
    "**Simple 10-Step Training Pipeline** ‚ö°\n",
    "\n",
    "## üìã Before You Start:\n",
    "1. **Enable GPU**: Settings ‚Üí Accelerator ‚Üí **GPU T4 x2** ‚Üí Save\n",
    "2. **Add Dataset**: \n",
    "   - Upload TrashCAN dataset as Kaggle Dataset OR\n",
    "   - Use Google Drive link (already configured below)\n",
    "3. **Run All Cells**: Execute cells 1-3, restart kernel, then run all again\n",
    "\n",
    "## ‚è±Ô∏è Training Time:\n",
    "- **100 epochs**: ~10 hours\n",
    "- **Expected mAP**: 70-72%\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d81c1d15",
   "metadata": {},
   "source": [
    "## Cell 1: Clone Repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17af0860",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone YOLO-UDD v2.0 repository\n",
    "import os\n",
    "import sys\n",
    "\n",
    "WORK_DIR = '/kaggle/working'\n",
    "REPO_DIR = f'{WORK_DIR}/YOLO-UDD-v2.0'\n",
    "\n",
    "print(\"Cloning repository...\")\n",
    "os.chdir(WORK_DIR)\n",
    "\n",
    "# Remove old directory if exists\n",
    "if os.path.exists(REPO_DIR):\n",
    "    import shutil\n",
    "    shutil.rmtree(REPO_DIR)\n",
    "\n",
    "# Clone from GitHub\n",
    "!git clone https://github.com/kshitijkhede/YOLO-UDD-v2.0.git\n",
    "\n",
    "# Setup paths\n",
    "os.chdir(REPO_DIR)\n",
    "sys.path.insert(0, REPO_DIR)\n",
    "\n",
    "print(f\"‚úÖ Repository ready at: {REPO_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9cc3205",
   "metadata": {},
   "source": [
    "## Cell 2: Verify Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b05c1817",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify repository structure\n",
    "import os\n",
    "\n",
    "print(\"Checking repository structure...\\n\")\n",
    "\n",
    "required = {\n",
    "    'models/': 'Model architecture',\n",
    "    'scripts/': 'Training scripts', \n",
    "    'utils/': 'Utility functions',\n",
    "    'configs/': 'Configuration files',\n",
    "    'data/': 'Dataset handling',\n",
    "    'requirements.txt': 'Dependencies',\n",
    "    'scripts/train.py': 'Training script'\n",
    "}\n",
    "\n",
    "all_ok = True\n",
    "for path, desc in required.items():\n",
    "    if os.path.exists(path):\n",
    "        print(f\"‚úÖ {path} - {desc}\")\n",
    "    else:\n",
    "        print(f\"‚ùå {path} - MISSING!\")\n",
    "        all_ok = False\n",
    "\n",
    "if all_ok:\n",
    "    print(\"\\n‚úÖ Repository structure verified!\")\n",
    "else:\n",
    "    print(\"\\n‚ùå Some files missing! Re-run Cell 1.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8d95971",
   "metadata": {},
   "source": [
    "## Cell 3: Fix NumPy Compatibility ‚ö†Ô∏è CRITICAL\n",
    "\n",
    "**‚ö†Ô∏è YOU MUST RESTART KERNEL AFTER THIS CELL!**\n",
    "\n",
    "Kaggle has NumPy 2.x which crashes TensorFlow. This fixes it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88251a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIX NumPy compatibility issue\n",
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "print(f\"Current NumPy version: {np.__version__}\\n\")\n",
    "\n",
    "if np.__version__.startswith('2.'):\n",
    "    print(\"‚ö†Ô∏è  NumPy 2.x detected - This will crash TensorFlow!\")\n",
    "    print(\"Fixing by downgrading to NumPy 1.26.4...\\n\")\n",
    "    \n",
    "    # Uninstall NumPy 2.x\n",
    "    !{sys.executable} -m pip uninstall -y numpy\n",
    "    \n",
    "    # Install NumPy 1.26.4\n",
    "    !{sys.executable} -m pip install 'numpy==1.26.4' --force-reinstall --no-cache-dir\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"‚úÖ NumPy 1.26.4 installed!\")\n",
    "    print(\"=\"*60)\n",
    "    print(\"\\nüî¥ STOP! RESTART KERNEL NOW! üî¥\")\n",
    "    print(\"\\nSteps:\")\n",
    "    print(\"1. Click: Session ‚Üí Restart Session\")\n",
    "    print(\"2. Run ALL cells again from Cell 1\")\n",
    "    print(\"3. This cell will show 'NumPy 1.x OK' after restart\")\n",
    "    print(\"4. Then continue to Cell 4\")\n",
    "    print(\"\\nüí° Why? NumPy is loaded in memory. Restart loads new version.\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Force stop execution\n",
    "    raise SystemExit(\"‚õî RESTART KERNEL NOW!\")\n",
    "else:\n",
    "    print(f\"‚úÖ NumPy 1.x OK ({np.__version__})\")\n",
    "    print(\"‚úÖ Continue to Cell 4!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f3bab4e",
   "metadata": {},
   "source": [
    "## Cell 4: Check GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "643f1666",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU availability\n",
    "import torch\n",
    "\n",
    "print(\"Checking GPU...\\n\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    gpu_name = torch.cuda.get_device_name(0)\n",
    "    gpu_count = torch.cuda.device_count()\n",
    "    gpu_mem = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
    "    \n",
    "    print(f\"‚úÖ GPU: {gpu_name}\")\n",
    "    print(f\"‚úÖ Count: {gpu_count}\")\n",
    "    print(f\"‚úÖ Memory: {gpu_mem:.1f} GB\")\n",
    "    print(f\"‚úÖ CUDA: {torch.version.cuda}\")\n",
    "    print(f\"‚úÖ PyTorch: {torch.__version__}\")\n",
    "else:\n",
    "    print(\"‚ùå NO GPU DETECTED!\")\n",
    "    print(\"\\nFix: Settings ‚Üí Accelerator ‚Üí GPU T4 x2 ‚Üí Save\")\n",
    "    raise RuntimeError(\"GPU required for training!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8c0e048",
   "metadata": {},
   "source": [
    "## Cell 5: Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56d84841",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "print(\"Installing dependencies...\\n\")\n",
    "\n",
    "!pip install -q torch>=2.0.0 torchvision>=0.15.0\n",
    "!pip install -q albumentations>=1.3.0\n",
    "!pip install -q opencv-python-headless>=4.7.0\n",
    "!pip install -q pycocotools>=2.0.6\n",
    "!pip install -q tensorboard>=2.12.0\n",
    "!pip install -q tqdm pyyaml scikit-learn matplotlib seaborn\n",
    "\n",
    "print(\"\\n‚úÖ All dependencies installed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "263e253a",
   "metadata": {},
   "source": [
    "## Cell 6: Setup Dataset\n",
    "\n",
    "**Choose ONE method:**\n",
    "\n",
    "### Option A: Kaggle Dataset (Recommended)\n",
    "1. Upload TrashCAN dataset to Kaggle Datasets\n",
    "2. Add to notebook: \"+ Add Data\"\n",
    "3. Set `USE_KAGGLE_DATASET = True`\n",
    "4. Update `KAGGLE_DATASET_PATH`\n",
    "\n",
    "### Option B: Google Drive (Easiest - Already Configured!)\n",
    "1. Just set `USE_GDRIVE = True`\n",
    "2. File ID already configured below\n",
    "3. Downloads automatically (~2-3 min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4b9ee82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset configuration\n",
    "import os\n",
    "\n",
    "# ============================================\n",
    "# CHOOSE ONE METHOD (Set to True)\n",
    "# ============================================\n",
    "\n",
    "# Option A: Kaggle Dataset (Permanent)\n",
    "USE_KAGGLE_DATASET = False\n",
    "KAGGLE_DATASET_PATH = '/kaggle/input/trashcan-dataset'  # Update if needed\n",
    "\n",
    "# Option B: Google Drive (Already configured!)\n",
    "USE_GDRIVE = True  # ‚úÖ SET THIS TO TRUE\n",
    "GDRIVE_FILE_ID = '10PCbGqgVi0-XQn0EfGTTfSjwNS0JXR99'  # ‚úÖ Already set!\n",
    "\n",
    "# ============================================\n",
    "# Automatic setup\n",
    "# ============================================\n",
    "\n",
    "DATASET_PATH = None\n",
    "\n",
    "if USE_KAGGLE_DATASET:\n",
    "    print(\"Using Kaggle Dataset...\")\n",
    "    \n",
    "    if os.path.exists(KAGGLE_DATASET_PATH):\n",
    "        # Check if ZIP or folder\n",
    "        if os.path.isfile(KAGGLE_DATASET_PATH):\n",
    "            print(\"Extracting ZIP...\")\n",
    "            !unzip -q {KAGGLE_DATASET_PATH} -d /kaggle/working/\n",
    "            DATASET_PATH = '/kaggle/working/trashcan'\n",
    "        else:\n",
    "            # Check for trashcan subdirectory\n",
    "            trashcan_path = os.path.join(KAGGLE_DATASET_PATH, 'trashcan')\n",
    "            DATASET_PATH = trashcan_path if os.path.exists(trashcan_path) else KAGGLE_DATASET_PATH\n",
    "        \n",
    "        print(f\"‚úÖ Dataset at: {DATASET_PATH}\")\n",
    "    else:\n",
    "        print(f\"‚ùå NOT FOUND: {KAGGLE_DATASET_PATH}\")\n",
    "        print(\"\\nSetup:\")\n",
    "        print(\"1. Upload dataset to Kaggle Datasets\")\n",
    "        print(\"2. Add to notebook: '+ Add Data'\")\n",
    "        print(\"3. Update path above\")\n",
    "\n",
    "elif USE_GDRIVE:\n",
    "    print(\"Using Google Drive...\")\n",
    "    print(\"Installing gdown...\")\n",
    "    !pip install -q gdown\n",
    "    \n",
    "    print(f\"\\nDownloading dataset (File ID: {GDRIVE_FILE_ID})...\")\n",
    "    !gdown --id {GDRIVE_FILE_ID} -O /kaggle/working/trashcan.zip\n",
    "    \n",
    "    if os.path.exists('/kaggle/working/trashcan.zip'):\n",
    "        size_mb = os.path.getsize('/kaggle/working/trashcan.zip') / 1024 / 1024\n",
    "        print(f\"‚úÖ Downloaded: {size_mb:.1f} MB\")\n",
    "        \n",
    "        print(\"Extracting...\")\n",
    "        !unzip -q /kaggle/working/trashcan.zip -d /kaggle/working/\n",
    "        \n",
    "        if os.path.exists('/kaggle/working/trashcan'):\n",
    "            DATASET_PATH = '/kaggle/working/trashcan'\n",
    "            print(f\"‚úÖ Extracted to: {DATASET_PATH}\")\n",
    "        else:\n",
    "            print(\"‚ùå Extraction failed\")\n",
    "    else:\n",
    "        print(\"‚ùå Download failed!\")\n",
    "        print(\"Check: File ID, sharing settings, internet enabled\")\n",
    "\n",
    "else:\n",
    "    print(\"‚ùå NO METHOD SELECTED!\")\n",
    "    print(\"Set USE_KAGGLE_DATASET or USE_GDRIVE to True above\")\n",
    "\n",
    "# Verify dataset\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "if DATASET_PATH and os.path.exists(DATASET_PATH):\n",
    "    print(f\"‚úÖ DATASET READY: {DATASET_PATH}\\n\")\n",
    "    \n",
    "    # Show structure\n",
    "    for item in ['images', 'annotations']:\n",
    "        path = os.path.join(DATASET_PATH, item)\n",
    "        if os.path.exists(path):\n",
    "            if item == 'images':\n",
    "                # Count subdirectories\n",
    "                for subdir in ['train', 'val', 'test']:\n",
    "                    subpath = os.path.join(path, subdir)\n",
    "                    if os.path.exists(subpath):\n",
    "                        count = len([f for f in os.listdir(subpath) if f.endswith(('.jpg', '.png'))])\n",
    "                        print(f\"  üìÅ {item}/{subdir}: {count:,} images\")\n",
    "            else:\n",
    "                # Count annotation files\n",
    "                for subdir in ['train', 'val', 'test']:\n",
    "                    subpath = os.path.join(path, subdir)\n",
    "                    if os.path.exists(subpath):\n",
    "                        files = [f for f in os.listdir(subpath) if f.endswith('.json')]\n",
    "                        print(f\"  üìÅ {item}/{subdir}: {len(files)} annotation files\")\n",
    "    \n",
    "    print(\"\\n‚úÖ Dataset structure verified!\")\n",
    "else:\n",
    "    print(\"‚ùå DATASET NOT READY!\")\n",
    "    print(\"Fix: Choose a method above and re-run this cell\")\n",
    "\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8128a4dd",
   "metadata": {},
   "source": [
    "## Cell 7: Test Model Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f569f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test model import and build\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "\n",
    "# Ensure we're in repo directory\n",
    "REPO_DIR = '/kaggle/working/YOLO-UDD-v2.0'\n",
    "os.chdir(REPO_DIR)\n",
    "if REPO_DIR not in sys.path:\n",
    "    sys.path.insert(0, REPO_DIR)\n",
    "\n",
    "print(\"Building YOLO-UDD v2.0 model...\\n\")\n",
    "\n",
    "# Import and build model\n",
    "from models.yolo_udd import build_yolo_udd\n",
    "\n",
    "model = build_yolo_udd(num_classes=22)  # TrashCAN has 22 classes\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"‚úÖ Model: YOLO-UDD v2.0\")\n",
    "print(f\"‚úÖ Classes: 22\")\n",
    "print(f\"‚úÖ Device: {device}\")\n",
    "print(f\"‚úÖ Parameters: {total_params:,} total, {trainable_params:,} trainable\")\n",
    "\n",
    "# Test forward pass\n",
    "print(\"\\nTesting forward pass...\")\n",
    "x = torch.randn(1, 3, 640, 640).to(device)\n",
    "with torch.no_grad():\n",
    "    predictions, turb_score = model(x)\n",
    "\n",
    "print(f\"‚úÖ Forward pass OK!\")\n",
    "print(f\"‚úÖ Turbidity score: {turb_score.item():.4f}\")\n",
    "print(f\"‚úÖ Detection scales: {len(predictions)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "189bf40d",
   "metadata": {},
   "source": [
    "## Cell 8: Configure Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bface977",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configuration\n",
    "import os\n",
    "\n",
    "# Hyperparameters\n",
    "EPOCHS = 100\n",
    "BATCH_SIZE = 8\n",
    "LEARNING_RATE = 0.01\n",
    "SAVE_DIR = '/kaggle/working/runs/train'\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"Training Configuration\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Epochs: {EPOCHS}\")\n",
    "print(f\"Batch Size: {BATCH_SIZE}\")\n",
    "print(f\"Learning Rate: {LEARNING_RATE}\")\n",
    "print(f\"Dataset: {DATASET_PATH}\")\n",
    "print(f\"Save Directory: {SAVE_DIR}\")\n",
    "print(f\"\\nEstimated Time: ~10 hours\")\n",
    "print(f\"Expected mAP: 70-72%\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Create save directory\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "print(f\"\\n‚úÖ Configuration ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c2780d9",
   "metadata": {},
   "source": [
    "## Cell 9: Start Training ‚ö°\n",
    "\n",
    "**‚è±Ô∏è This will take ~10 hours**\n",
    "\n",
    "Training will:\n",
    "- Save checkpoints automatically\n",
    "- Show progress in real-time\n",
    "- Save best model as `best.pt`\n",
    "- Save results to `/kaggle/working/runs/train/`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f25e4e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start training with CORRECT arguments\n",
    "print(\"=\"*60)\n",
    "print(\"üöÄ STARTING TRAINING\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Training for {EPOCHS} epochs (~10 hours)\")\n",
    "print(f\"Expected mAP: 70-72%\")\n",
    "print(\"=\"*60)\n",
    "print()\n",
    "\n",
    "# Run training script - FIXED ARGUMENTS!\n",
    "!python scripts/train.py \\\n",
    "    --config configs/train_config.yaml \\\n",
    "    --data-dir {DATASET_PATH} \\\n",
    "    --epochs {EPOCHS} \\\n",
    "    --batch-size {BATCH_SIZE} \\\n",
    "    --lr {LEARNING_RATE} \\\n",
    "    --save-dir {SAVE_DIR}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38565a45",
   "metadata": {},
   "source": [
    "## Cell 10: Check Results & Download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc1e6ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check training results\n",
    "import os\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"Training Results\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "if os.path.exists(SAVE_DIR):\n",
    "    print(f\"\\nüìÅ Results: {SAVE_DIR}\\n\")\n",
    "    \n",
    "    # List all files\n",
    "    for root, dirs, files in os.walk(SAVE_DIR):\n",
    "        level = root.replace(SAVE_DIR, '').count(os.sep)\n",
    "        indent = '  ' * level\n",
    "        print(f\"{indent}{os.path.basename(root)}/\")\n",
    "        sub_indent = '  ' * (level + 1)\n",
    "        for file in files:\n",
    "            size_mb = os.path.getsize(os.path.join(root, file)) / (1024*1024)\n",
    "            print(f\"{sub_indent}{file} ({size_mb:.1f} MB)\")\n",
    "    \n",
    "    # Check for best checkpoint\n",
    "    best_pt = os.path.join(SAVE_DIR, 'best.pt')\n",
    "    if os.path.exists(best_pt):\n",
    "        size_mb = os.path.getsize(best_pt) / (1024*1024)\n",
    "        print(f\"\\n‚úÖ BEST MODEL: {best_pt} ({size_mb:.1f} MB)\")\n",
    "        print(\"\\nüì• Download from Output section!\")\n",
    "        print(\"\\nüéâ Training complete! Expected mAP: 70-72%\")\n",
    "    else:\n",
    "        print(\"\\n‚ö†Ô∏è  best.pt not found - training may have failed\")\n",
    "else:\n",
    "    print(f\"\\n‚ùå Results directory not found: {SAVE_DIR}\")\n",
    "    print(\"Training may not have started or failed early.\")\n",
    "\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üéâ ALL DONE!\")\n",
    "print(\"=\"*60)\n",
    "print(\"Next steps:\")\n",
    "print(\"1. Download best.pt from Output folder\")\n",
    "print(\"2. Use for evaluation or inference\")\n",
    "print(\"3. Expected performance: 70-72% mAP@50:95\")\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
