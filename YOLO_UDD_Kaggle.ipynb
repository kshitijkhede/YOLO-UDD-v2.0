{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a82cd800",
   "metadata": {},
   "source": [
    "# \ud83c\udf0a YOLO-UDD v2.0 - Kaggle Training\n",
    "\n",
    "**Simple 6-Step Training - No Crashes, No Loops!** \u26a1\n",
    "\n",
    "## \ud83d\udccb Before You Start:\n",
    "1. **Enable GPU**: Settings \u2192 Accelerator \u2192 **GPU T4 x2** \u2192 Save\n",
    "2. **Dataset**: Google Drive link already configured (automatic download)\n",
    "3. **Run**: Execute cells 1-6 in order OR click \"Run All\"\n",
    "\n",
    "## \u23f1\ufe0f Training Info:\n",
    "- **Time**: ~10 hours (100 epochs)\n",
    "- **Expected mAP**: 70-72%\n",
    "- **No restarts needed!** \u2705\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8830adbe",
   "metadata": {},
   "source": [
    "## Cell 1: Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38942eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete environment setup\n",
    "import os\n",
    "import sys\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"\ud83d\udd27 CELL 1: Environment Setup\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Check and fix NumPy version FIRST (before any other imports)\n",
    "print(\"\\n[Step 1/3] Checking NumPy version...\")\n",
    "try:\n",
    "    import numpy as np\n",
    "    numpy_ver = np.__version__\n",
    "    \n",
    "    if numpy_ver.startswith('2.'):\n",
    "        print(f\"  \u26a0\ufe0f  NumPy {numpy_ver} detected (will cause TensorFlow crashes)\")\n",
    "        print(\"  \ud83d\udd27 Installing NumPy 1.26.4...\")\n",
    "        \n",
    "        # Use pip directly with quiet mode\n",
    "        !pip uninstall -y numpy > /dev/null 2>&1\n",
    "        !pip install -q numpy==1.26.4\n",
    "        \n",
    "        print(\"  \u2705 NumPy 1.26.4 installed\")\n",
    "        print(\"  \u2139\ufe0f  If you see import errors later, just re-run this cell\")\n",
    "    else:\n",
    "        print(f\"  \u2705 NumPy {numpy_ver} OK\")\n",
    "except Exception as e:\n",
    "    print(f\"  \u26a0\ufe0f  NumPy check issue: {e}\")\n",
    "    print(\"  \ud83d\udce6 Installing NumPy 1.26.4...\")\n",
    "    !pip install -q numpy==1.26.4\n",
    "    print(\"  \u2705 Installed\")\n",
    "\n",
    "# Setup directories\n",
    "print(\"\\n[Step 2/3] Setting up directories...\")\n",
    "WORK_DIR = '/kaggle/working'\n",
    "REPO_DIR = f'{WORK_DIR}/YOLO-UDD-v2.0'\n",
    "\n",
    "os.chdir(WORK_DIR)\n",
    "print(f\"  \u2705 Working directory: {WORK_DIR}\")\n",
    "\n",
    "# Clone repository\n",
    "print(\"\\n[Step 3/3] Cloning repository...\")\n",
    "if os.path.exists(REPO_DIR):\n",
    "    import shutil\n",
    "    shutil.rmtree(REPO_DIR)\n",
    "\n",
    "!git clone -q https://github.com/kshitijkhede/YOLO-UDD-v2.0.git\n",
    "\n",
    "if os.path.exists(REPO_DIR):\n",
    "    os.chdir(REPO_DIR)\n",
    "    if REPO_DIR not in sys.path:\n",
    "        sys.path.insert(0, REPO_DIR)\n",
    "    print(f\"  \u2705 Repository ready: {REPO_DIR}\")\n",
    "else:\n",
    "    raise Exception(\"Clone failed!\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"\u2705 Cell 1 Complete - Environment Ready!\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d42fbc1c",
   "metadata": {},
   "source": [
    "## Cell 2: Verify & Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d42523a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify setup and install dependencies",
    "import os",
    "import torch",
    "",
    "print(\"=\"*70)",
    "print(\"\ufffd\ufffd CELL 2: Verification & Dependencies\")",
    "print(\"=\"*70)",
    "",
    "# Verify repository structure",
    "print(\"\\n[Step 1/4] Verifying repository...\")",
    "required = ['models/', 'scripts/', 'utils/', 'configs/', 'scripts/train.py']",
    "all_ok = True",
    "for item in required:",
    "    if os.path.exists(item):",
    "        print(f\"  \u2705 {item}\")",
    "    else:",
    "        print(f\"  \u274c {item} MISSING\")",
    "        all_ok = False",
    "",
    "if not all_ok:",
    "    raise Exception(\"Repository incomplete! Re-run Cell 1\")",
    "",
    "# Check GPU",
    "print(\"\\n[Step 2/4] Checking GPU...\")",
    "if torch.cuda.is_available():",
    "    print(f\"  \u2705 GPU: {torch.cuda.get_device_name(0)}\")",
    "    print(f\"  \u2705 Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")",
    "else:",
    "    print(\"  \u274c NO GPU! Enable: Settings \u2192 GPU T4 x2\")",
    "    raise RuntimeError(\"GPU required!\")",
    "",
    "# Verify NumPy version",
    "print(\"\\n[Step 3/4] Verifying NumPy...\")",
    "import numpy as np",
    "print(f\"  \ud83d\udce6 NumPy version: {np.__version__}\")",
    "if np.__version__.startswith('2.'):",
    "    print(f\"  \u274c ERROR: NumPy {np.__version__} still active!\")",
    "    print(f\"  \ud83d\udd27 FIX: Re-run Cell 1, then restart kernel\")",
    "    raise RuntimeError(\"NumPy 2.x detected! Re-run Cell 1 and restart kernel.\")",
    "else:",
    "    print(f\"  \u2705 NumPy {np.__version__} OK\")",
    "",
    "# Install dependencies - FORCE REINSTALL to rebuild against NumPy 1.x",
    "print(\"\\n[Step 4/4] Installing dependencies (takes ~3 min)...\")",
    "print(\"  \ud83d\udd27 Force reinstalling packages to rebuild against NumPy 1.26.4...\")",
    "",
    "# Uninstall problematic packages first",
    "!pip uninstall -y tensorboard tensorflow keras scikit-learn matplotlib > /dev/null 2>&1",
    "",
    "# Install with force reinstall to rebuild against NumPy 1.x",
    "!pip install -q --force-reinstall --no-cache-dir tensorboard scikit-learn matplotlib seaborn",
    "!pip install -q torch>=2.0.0 torchvision>=0.15.0 albumentations>=1.3.0 \\",
    "    opencv-python-headless>=4.7.0 pycocotools>=2.0.6 tqdm pyyaml",
    "",
    "print(\"  \u2705 All dependencies installed and rebuilt against NumPy 1.26.4\")",
    "",
    "print(\"\\n\" + \"=\"*70)",
    "print(\"\u2705 Cell 2 Complete - System Ready!\")",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86fc8028",
   "metadata": {},
   "source": [
    "## Cell 3: Setup Dataset\n",
    "\n",
    "**Dataset will download automatically from Google Drive (~170 MB, takes 2-3 min)**\n",
    "\n",
    "Alternative: Upload your own dataset to Kaggle and set `USE_KAGGLE_DATASET = True`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b863aad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup Kaggle Dataset - AUTOMATIC!\n",
    "import os\n",
    "import json\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"\ud83d\udce6 CELL 3: Setup Dataset from Kaggle\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# On Kaggle, datasets are automatically mounted at /kaggle/input/\n",
    "KAGGLE_DATASET_PATH = '/kaggle/input/trashcan'\n",
    "\n",
    "print(\"\\n[Step 1/2] Checking Kaggle dataset...\")\n",
    "\n",
    "if os.path.exists(KAGGLE_DATASET_PATH):\n",
    "    print(f\"  \u2705 Found Kaggle dataset at: {KAGGLE_DATASET_PATH}\")\n",
    "    \n",
    "    # Show structure\n",
    "    print(\"\\n[Step 2/2] Verifying dataset structure...\")\n",
    "    for item in sorted(os.listdir(KAGGLE_DATASET_PATH)):\n",
    "        path = os.path.join(KAGGLE_DATASET_PATH, item)\n",
    "        if os.path.isdir(path):\n",
    "            count = len(os.listdir(path))\n",
    "            print(f\"  \ud83d\udcc1 {item}/ ({count} items)\")\n",
    "    \n",
    "    # Verify annotations\n",
    "    print(\"\\n  \ud83d\udd0d Checking annotations...\")\n",
    "    for split in ['train', 'val']:\n",
    "        json_path = os.path.join(KAGGLE_DATASET_PATH, 'annotations', f'{split}.json')\n",
    "        if os.path.exists(json_path):\n",
    "            with open(json_path) as f:\n",
    "                data = json.load(f)\n",
    "            imgs = len(data.get('images', []))\n",
    "            anns = len(data.get('annotations', []))\n",
    "            print(f\"    \u2705 {split}.json: {imgs:,} images, {anns:,} annotations\")\n",
    "        else:\n",
    "            print(f\"    \u274c {split}.json not found!\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"\u2705 Dataset ready!\")\n",
    "    print(f\"\ud83d\udcc2 Path: {KAGGLE_DATASET_PATH}\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "else:\n",
    "    print(f\"  \u274c ERROR: Kaggle dataset not found!\")\n",
    "    print(f\"  \ud83d\udcc2 Expected path: {KAGGLE_DATASET_PATH}\")\n",
    "    print(\"\\n  \ud83d\udca1 Solutions:\")\n",
    "    print(\"     1. Add dataset: Click '+ Add Data' \u2192 Search 'trashcan' \u2192 Add\")\n",
    "    print(\"     2. Check dataset name matches: kshitijkhede/trashcan\")\n",
    "    print(\"     3. Refresh kernel if just added\")\n",
    "    raise FileNotFoundError(\"Dataset not found. Please add the dataset to your notebook.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17f5b365",
   "metadata": {},
   "source": [
    "## Cell 4: Build & Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a49ee2e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build and test YOLO-UDD model\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"\ud83c\udfd7\ufe0f  CELL 4: Build Model\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Ensure correct paths\n",
    "REPO_DIR = '/kaggle/working/YOLO-UDD-v2.0'\n",
    "os.chdir(REPO_DIR)\n",
    "if REPO_DIR not in sys.path:\n",
    "    sys.path.insert(0, REPO_DIR)\n",
    "\n",
    "print(\"\\n[Step 1/2] Building model...\")\n",
    "from models.yolo_udd import build_yolo_udd\n",
    "\n",
    "model = build_yolo_udd(num_classes=22)  # TrashCAN has 22 classes\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"  \u2705 Model: YOLO-UDD v2.0\")\n",
    "print(f\"  \u2705 Classes: 22\")\n",
    "print(f\"  \u2705 Device: {device}\")\n",
    "print(f\"  \u2705 Parameters: {total_params:,}\")\n",
    "\n",
    "# Test forward pass\n",
    "print(\"\\n[Step 2/2] Testing model...\")\n",
    "x = torch.randn(1, 3, 640, 640).to(device)\n",
    "with torch.no_grad():\n",
    "    predictions, turb_score = model(x)\n",
    "\n",
    "print(f\"  \u2705 Forward pass successful\")\n",
    "print(f\"  \u2705 Turbidity score: {turb_score.item():.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"\u2705 Cell 4 Complete - Model Ready!\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47f0baad",
   "metadata": {},
   "source": [
    "## Cell 5: Start Training \u26a1\n",
    "\n",
    "**\u23f1\ufe0f This will take ~10 hours for 100 epochs**\n",
    "\n",
    "Training automatically:\n",
    "- Saves checkpoints every epoch\n",
    "- Shows progress with progress bar\n",
    "- Saves best model as `best.pt`\n",
    "- Results saved to `/kaggle/working/runs/train/`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e82fa8c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start training\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"\ud83d\ude80 CELL 5: Starting Training\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Training parameters\n",
    "EPOCHS = 100\n",
    "BATCH_SIZE = 8\n",
    "LEARNING_RATE = 0.01\n",
    "SAVE_DIR = '/kaggle/working/runs/train'\n",
    "DATASET_PATH = '/kaggle/input/trashcan'  # Kaggle auto-mount path\n",
    "\n",
    "print(f\"\\n\ud83d\udcca Training Configuration:\")\n",
    "print(f\"   Epochs:       {EPOCHS}\")\n",
    "print(f\"   Batch Size:   {BATCH_SIZE}\")\n",
    "print(f\"   Learning Rate: {LEARNING_RATE}\")\n",
    "print(f\"   Dataset:      {DATASET_PATH}\")\n",
    "print(f\"   Save Dir:     {SAVE_DIR}\")\n",
    "\n",
    "# Build training command\n",
    "cmd = [\n",
    "    sys.executable, 'scripts/train.py',\n",
    "    '--config', 'configs/train_config.yaml',\n",
    "    '--data-dir', DATASET_PATH,\n",
    "    '--batch-size', str(BATCH_SIZE),\n",
    "    '--epochs', str(EPOCHS),\n",
    "    '--lr', str(LEARNING_RATE),\n",
    "    '--save-dir', SAVE_DIR\n",
    "]\n",
    "\n",
    "print(\"\\n\ud83c\udfaf Starting training...\")\n",
    "print(\"   This will take ~10 hours for 100 epochs\")\n",
    "print(\"   Progress will be shown below\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "# Run training\n",
    "result = subprocess.run(cmd)\n",
    "\n",
    "if result.returncode == 0:\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"\u2705 Training completed successfully!\")\n",
    "    print(f\"\ud83d\udcc2 Results saved to: {SAVE_DIR}\")\n",
    "    print(\"=\"*70)\n",
    "else:\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"\u274c Training failed - see error above\")\n",
    "    print(\"=\"*70)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "701bf285",
   "metadata": {},
   "source": [
    "## Cell 6: Check Results & Download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f06a0c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check training results\n",
    "import os\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"\ud83d\udcca CELL 6: Results\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "if os.path.exists(SAVE_DIR):\n",
    "    print(f\"\\n\ud83d\udcc1 Results: {SAVE_DIR}\\n\")\n",
    "    \n",
    "    # List files\n",
    "    for root, dirs, files in os.walk(SAVE_DIR):\n",
    "        level = root.replace(SAVE_DIR, '').count(os.sep)\n",
    "        indent = '  ' * level\n",
    "        print(f\"{indent}{os.path.basename(root)}/\")\n",
    "        sub_indent = '  ' * (level + 1)\n",
    "        for file in files:\n",
    "            size = os.path.getsize(os.path.join(root, file)) / (1024*1024)\n",
    "            print(f\"{sub_indent}{file} ({size:.1f} MB)\")\n",
    "    \n",
    "    # Check for best checkpoint\n",
    "    best_pt = os.path.join(SAVE_DIR, 'best.pt')\n",
    "    if os.path.exists(best_pt):\n",
    "        size = os.path.getsize(best_pt) / (1024*1024)\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"\u2705 TRAINING COMPLETE!\")\n",
    "        print(\"=\"*70)\n",
    "        print(f\"\\n\ud83c\udfc6 Best Model: {best_pt}\")\n",
    "        print(f\"\ud83d\udce6 Size: {size:.1f} MB\")\n",
    "        print(f\"\\n\ud83d\udce5 DOWNLOAD: Check 'Output' section in right sidebar \u2192\")\n",
    "        print(f\"\ud83c\udfaf Expected Performance: 70-72% mAP@50:95\")\n",
    "        print(f\"\\n\ud83c\udf89 Success! Model ready for deployment!\")\n",
    "        print(\"=\"*70)\n",
    "    else:\n",
    "        print(\"\\n\u26a0\ufe0f  best.pt not found - check if training completed\")\n",
    "else:\n",
    "    print(f\"\\n\u274c Results not found: {SAVE_DIR}\")\n",
    "    print(\"Training may have failed or not started.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}