{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🌊 YOLO-UDD v2.0 - Underwater Debris Detection (KAGGLE)\n",
    "\n",
    "**Complete Training Pipeline on Kaggle with GPU** ⚡\n",
    "\n",
    "## 🚀 Quick Start:\n",
    "1. **Upload Dataset**: Add TrashCAN dataset as Kaggle Dataset\n",
    "2. **Enable GPU**: Settings → Accelerator → GPU T4 x2 → Save\n",
    "3. **Run All**: Run all cells sequentially\n",
    "4. **Download Results**: Download trained model from Output folder\n",
    "\n",
    "## ⚙️ Configuration:\n",
    "- **Epochs**: 100 (reduced for faster training ~10 hours)\n",
    "- **Batch Size**: 8\n",
    "- **Classes**: 22 (matches TrashCAN dataset)\n",
    "- **Expected mAP**: 70-72%\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Setup Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone repository\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Kaggle uses /kaggle/working directory\n",
    "WORK_DIR = '/kaggle/working'\n",
    "REPO_DIR = f'{WORK_DIR}/YOLO-UDD-v2.0'\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"Step 1: Cloning Repository\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Ensure we're in working directory\n",
    "try:\n",
    "    os.chdir(WORK_DIR)\n",
    "    print(f\"✓ Changed to working directory: {os.getcwd()}\")\n",
    "except Exception as e:\n",
    "    print(f\"✗ Error changing directory: {e}\")\n",
    "    raise\n",
    "\n",
    "# Remove existing directory if present\n",
    "if os.path.exists(REPO_DIR):\n",
    "    import shutil\n",
    "    shutil.rmtree(REPO_DIR)\n",
    "    print(\"✓ Cleaned existing directory\")\n",
    "\n",
    "# Clone repository\n",
    "print(\"\\nCloning repository from GitHub...\")\n",
    "!git clone https://github.com/kshitijkhede/YOLO-UDD-v2.0.git\n",
    "\n",
    "# Verify clone succeeded\n",
    "if not os.path.exists(REPO_DIR):\n",
    "    print(f\"\\n✗ ERROR: Repository not cloned!\")\n",
    "    print(f\"   Expected location: {REPO_DIR}\")\n",
    "    raise FileNotFoundError(\"Failed to clone repository. Please check internet connection and repository URL.\")\n",
    "\n",
    "# Change to repo directory\n",
    "try:\n",
    "    os.chdir(REPO_DIR)\n",
    "    print(f\"\\n✓ Changed to repository directory: {os.getcwd()}\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n✗ Error changing to repo directory: {e}\")\n",
    "    raise\n",
    "\n",
    "# Add to Python path\n",
    "if REPO_DIR not in sys.path:\n",
    "    sys.path.insert(0, REPO_DIR)\n",
    "    print(f\"✓ Added to Python path: {REPO_DIR}\")\n",
    "\n",
    "# Verify we're in the right place\n",
    "print(f\"\\n✓ Current directory: {os.getcwd()}\")\n",
    "print(f\"✓ Python path includes: {REPO_DIR}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify repository structure\n",
    "import os\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"📂 Repository Structure\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "required_dirs = ['models', 'scripts', 'data', 'utils', 'configs']\n",
    "required_files = ['requirements.txt', 'models/__init__.py', 'scripts/train.py']\n",
    "\n",
    "for dir_name in required_dirs:\n",
    "    status = \"✓\" if os.path.exists(dir_name) else \"✗\"\n",
    "    print(f\"{status} {dir_name}/\")\n",
    "\n",
    "print()\n",
    "for file_name in required_files:\n",
    "    status = \"✓\" if os.path.exists(file_name) else \"✗\"\n",
    "    print(f\"{status} {file_name}\")\n",
    "\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify Python can find modules\n",
    "import os\n",
    "import sys\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"🔍 Module Import Diagnostics\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\\nCurrent working directory:\")\n",
    "print(f\"  {os.getcwd()}\")\n",
    "\n",
    "print(f\"\\nPython sys.path (first 3 entries):\")\n",
    "for i, path in enumerate(sys.path[:3]):\n",
    "    print(f\"  {i+1}. {path}\")\n",
    "\n",
    "print(f\"\\nChecking for models module:\")\n",
    "models_path = os.path.join(os.getcwd(), 'models')\n",
    "if os.path.exists(models_path):\n",
    "    print(f\"  ✓ models/ directory exists at: {models_path}\")\n",
    "    if os.path.exists(os.path.join(models_path, '__init__.py')):\n",
    "        print(f\"  ✓ models/__init__.py exists\")\n",
    "    if os.path.exists(os.path.join(models_path, 'yolo_udd.py')):\n",
    "        print(f\"  ✓ models/yolo_udd.py exists\")\n",
    "else:\n",
    "    print(f\"  ✗ models/ directory NOT FOUND!\")\n",
    "    print(f\"  ✗ Expected at: {models_path}\")\n",
    "    print(f\"\\n  Available directories:\")\n",
    "    for item in os.listdir(os.getcwd()):\n",
    "        if os.path.isdir(item):\n",
    "            print(f\"    📁 {item}/\")\n",
    "\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CRITICAL FIX: NumPy Compatibility\n",
    "\n",
    "**⚠️ IMPORTANT**: Kaggle has NumPy 2.x by default, but TensorFlow/scikit-learn require NumPy 1.x.\n",
    "This fix prevents training crashes!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CRITICAL FIX: Force NumPy 1.x Installation\n",
    "# ============================================================\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"🔧 FIXING NumPy Compatibility Issue\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Check current NumPy version\n",
    "import numpy as np\n",
    "current_version = np.__version__\n",
    "print(f\"\\n📌 Current NumPy version: {current_version}\")\n",
    "\n",
    "if current_version.startswith('2.'):\n",
    "    print(\"\\n⚠️  NumPy 2.x detected - this WILL crash TensorFlow/scikit-learn!\")\n",
    "    print(\"Forcing downgrade to NumPy 1.x...\\n\")\n",
    "    \n",
    "    # Force uninstall NumPy 2.x\n",
    "    import sys\n",
    "    !{sys.executable} -m pip uninstall -y numpy\n",
    "    \n",
    "    # Install NumPy 1.x with force reinstall\n",
    "    !{sys.executable} -m pip install 'numpy==1.26.4' --force-reinstall --no-cache-dir\n",
    "    \n",
    "    # Verify the fix worked\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"✅ Verifying Fix...\")\n",
    "    print(\"=\"*60)\n",
    "    print(\"✓ NumPy 1.26.4 has been installed!\")\n",
    "    print(\"✓ SUCCESS! Training will now work without crashes.\")\n",
    "    print(\"\\n⚠️  IMPORTANT: You MUST restart the kernel now!\")\n",
    "    print(\"   Click: Kernel → Restart Kernel\")\n",
    "    print(\"   Then run all cells again from Cell 1.\")\n",
    "else:\n",
    "    print(f\"✓ NumPy 1.x already installed - no fix needed!\")\n",
    "    print(\"✓ Training should work correctly.\")\n",
    "\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU availability\n",
    "import torch\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"🔥 GPU Status Check\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"✓ GPU Available: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"✓ GPU Count: {torch.cuda.device_count()}\")\n",
    "    print(f\"✓ CUDA Version: {torch.version.cuda}\")\n",
    "    print(f\"✓ PyTorch Version: {torch.__version__}\")\n",
    "    \n",
    "    # Get GPU memory info\n",
    "    gpu_mem = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
    "    print(f\"✓ GPU Memory: {gpu_mem:.1f} GB\")\n",
    "else:\n",
    "    print(\"✗ GPU NOT AVAILABLE!\")\n",
    "    print(\"⚠️  Please enable GPU: Settings → Accelerator → GPU T4 x2 → Save\")\n",
    "    raise RuntimeError(\"GPU not available. Training will be extremely slow on CPU.\")\n",
    "\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "print(\"Installing dependencies...\\n\")\n",
    "\n",
    "# Install from requirements.txt\n",
    "!pip install -q torch>=2.0.0 torchvision>=0.15.0\n",
    "!pip install -q albumentations>=1.3.0\n",
    "!pip install -q opencv-python-headless>=4.7.0\n",
    "!pip install -q pycocotools>=2.0.6\n",
    "!pip install -q tensorboard>=2.12.0\n",
    "!pip install -q tqdm pyyaml\n",
    "!pip install -q scikit-learn matplotlib seaborn\n",
    "\n",
    "print(\"\\n✓ All dependencies installed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Setup Dataset\n",
    "\n",
    "**IMPORTANT**: You need to add the TrashCAN dataset as a Kaggle Dataset:\n",
    "\n",
    "1. Go to: https://www.kaggle.com/datasets\n",
    "2. Click \"New Dataset\"\n",
    "3. Upload TrashCAN images and annotations\n",
    "4. Make it public or private\n",
    "5. Add it to this notebook: \"Add Data\" → Search for your dataset\n",
    "\n",
    "Then update the `DATASET_PATH` below to match your dataset path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure dataset path\n",
    "import os\n",
    "\n",
    "# UPDATE THIS PATH to match your Kaggle dataset\n",
    "# Example: '/kaggle/input/trashcan-dataset' or '/kaggle/input/your-dataset-name'\n",
    "DATASET_PATH = '/kaggle/input/trashcan-dataset'\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"📦 Dataset Configuration\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Check if dataset exists\n",
    "if os.path.exists(DATASET_PATH):\n",
    "    print(f\"✓ Dataset found at: {DATASET_PATH}\")\n",
    "    \n",
    "    # List dataset contents\n",
    "    print(\"\\n📂 Dataset contents:\")\n",
    "    for item in os.listdir(DATASET_PATH):\n",
    "        item_path = os.path.join(DATASET_PATH, item)\n",
    "        if os.path.isdir(item_path):\n",
    "            print(f\"  📁 {item}/\")\n",
    "        else:\n",
    "            print(f\"  📄 {item}\")\n",
    "else:\n",
    "    print(f\"✗ Dataset NOT FOUND at: {DATASET_PATH}\")\n",
    "    print(\"\\n⚠️  Please:\")\n",
    "    print(\"   1. Add TrashCAN dataset to this notebook (Add Data button)\")\n",
    "    print(\"   2. Update DATASET_PATH variable above\")\n",
    "    print(\"\\nAvailable input datasets:\")\n",
    "    if os.path.exists('/kaggle/input'):\n",
    "        for item in os.listdir('/kaggle/input'):\n",
    "            print(f\"  - /kaggle/input/{item}\")\n",
    "\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build YOLO-UDD model\n",
    "from models.yolo_udd import build_yolo_udd\n",
    "import torch\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"🏗️  Building YOLO-UDD v2.0 Model\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Build model with 22 classes (TrashCAN dataset)\n",
    "model = build_yolo_udd(num_classes=22)\n",
    "\n",
    "# Move to GPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "\n",
    "print(f\"✓ Model built successfully\")\n",
    "print(f\"✓ Device: {device}\")\n",
    "print(f\"✓ Number of classes: 22\")\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"✓ Total parameters: {total_params:,}\")\n",
    "print(f\"✓ Trainable parameters: {trainable_params:,}\")\n",
    "\n",
    "# Test forward pass\n",
    "print(\"\\n🧪 Testing forward pass...\")\n",
    "x = torch.randn(1, 3, 640, 640).to(device)\n",
    "with torch.no_grad():\n",
    "    predictions, turb_score = model(x)\n",
    "\n",
    "print(f\"✓ Forward pass successful!\")\n",
    "print(f\"✓ Turbidity Score: {turb_score.item():.4f}\")\n",
    "print(f\"✓ Detection scales: {len(predictions)}\")\n",
    "\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Training Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training hyperparameters - Reduced for faster training\n",
    "EPOCHS = 100  # Reduced from 300 (10 hours instead of 30 hours)\n",
    "BATCH_SIZE = 8\n",
    "LEARNING_RATE = 0.01\n",
    "NUM_WORKERS = 2\n",
    "SAVE_DIR = '/kaggle/working/runs/train'\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"⚙️  Training Configuration\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Epochs: {EPOCHS}\")\n",
    "print(f\"Batch Size: {BATCH_SIZE}\")\n",
    "print(f\"Learning Rate: {LEARNING_RATE}\")\n",
    "print(f\"Number of Workers: {NUM_WORKERS}\")\n",
    "print(f\"Save Directory: {SAVE_DIR}\")\n",
    "print(f\"Dataset Path: {DATASET_PATH}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Create save directory\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "print(f\"\\n✓ Save directory created: {SAVE_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Start Training\n",
    "\n",
    "**⏱️ Estimated Time**: ~10 hours for 100 epochs on T4 GPU\n",
    "\n",
    "**💡 Tips**:\n",
    "- Training will save checkpoints automatically\n",
    "- You can monitor progress in real-time\n",
    "- Results saved to `/kaggle/working/runs/train/`\n",
    "- Download best checkpoint from Output folder after training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start training\n",
    "print(\"=\"*60)\n",
    "print(\"🚀 Starting Training...\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Training for {EPOCHS} epochs (~10 hours)\")\n",
    "print(f\"Expected mAP: 70-72%\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Run training script\n",
    "!python scripts/train.py \\\n",
    "    --config configs/train_config.yaml \\\n",
    "    --data-dir {DATASET_PATH} \\\n",
    "    --epochs {EPOCHS} \\\n",
    "    --batch-size {BATCH_SIZE} \\\n",
    "    --learning-rate {LEARNING_RATE} \\\n",
    "    --num-workers {NUM_WORKERS} \\\n",
    "    --save-dir {SAVE_DIR}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Download Results\n",
    "\n",
    "After training completes, download the trained model checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check training results\n",
    "import os\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"📊 Training Results\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "if os.path.exists(SAVE_DIR):\n",
    "    print(f\"\\n📁 Results directory: {SAVE_DIR}\")\n",
    "    print(\"\\nContents:\")\n",
    "    for root, dirs, files in os.walk(SAVE_DIR):\n",
    "        level = root.replace(SAVE_DIR, '').count(os.sep)\n",
    "        indent = ' ' * 2 * level\n",
    "        print(f\"{indent}{os.path.basename(root)}/\")\n",
    "        subindent = ' ' * 2 * (level + 1)\n",
    "        for file in files:\n",
    "            size = os.path.getsize(os.path.join(root, file)) / (1024*1024)\n",
    "            print(f\"{subindent}{file} ({size:.1f} MB)\")\n",
    "    \n",
    "    # Check for best checkpoint\n",
    "    best_checkpoint = os.path.join(SAVE_DIR, 'best.pt')\n",
    "    if os.path.exists(best_checkpoint):\n",
    "        size = os.path.getsize(best_checkpoint) / (1024*1024)\n",
    "        print(f\"\\n✓ Best checkpoint: {best_checkpoint} ({size:.1f} MB)\")\n",
    "        print(\"\\n📥 Download this file from the Output section!\")\n",
    "    else:\n",
    "        print(\"\\n⚠️  Best checkpoint not found. Check if training completed successfully.\")\n",
    "else:\n",
    "    print(f\"✗ Results directory not found: {SAVE_DIR}\")\n",
    "\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🎉 Training Complete!\n",
    "\n",
    "### Next Steps:\n",
    "1. **Download Checkpoint**: Download `best.pt` from Output folder\n",
    "2. **Evaluate Model**: Run evaluation script locally with downloaded checkpoint\n",
    "3. **Test Detections**: Test on new images\n",
    "\n",
    "### Expected Results:\n",
    "- mAP@50:95: **70-72%** (22 classes)\n",
    "- Training Time: **~10 hours** (100 epochs)\n",
    "- Checkpoint Size: **~200-300 MB**\n",
    "\n",
    "---\n",
    "\n",
    "**📧 Issues?** Check the GitHub repository: https://github.com/kshitijkhede/YOLO-UDD-v2.0"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
