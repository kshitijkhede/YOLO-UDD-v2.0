{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a82cd800",
   "metadata": {},
   "source": [
    "# üåä YOLO-UDD v2.0 - Kaggle Training\n",
    "\n",
    "**Fixed: NumPy + Dataset Structure Issues!** ‚úÖ\n",
    "\n",
    "## \ude80 Quick Start:\n",
    "1. **Enable GPU**: Settings ‚Üí Accelerator ‚Üí **GPU T4 x2** ‚Üí Save\n",
    "2. **Run All Cells** (1-5) - No manual steps needed!\n",
    "3. **Download trained model** after ~10 hours\n",
    "\n",
    "## ‚ö° What's Fixed:\n",
    "- ‚úÖ Automatic NumPy 1.x installation (no restart needed!)\n",
    "- ‚úÖ Dataset auto-restructuring (images moved to correct folders)\n",
    "- ‚úÖ All dependencies installed in one go\n",
    "- ‚úÖ Simplified from 6 cells to 5 cells\n",
    "\n",
    "## ‚è±Ô∏è Training Info:\n",
    "- **Time**: ~10 hours (100 epochs on GPU T4)\n",
    "- **Expected mAP**: 70-72% @ IoU 0.5\n",
    "- **Dataset**: 7,212 images (auto-downloaded from Google Drive)\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8830adbe",
   "metadata": {},
   "source": [
    "## Cell 1: Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38942eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CRITICAL: Environment Setup with NumPy Fix\n",
    "import os\n",
    "import sys\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"üîß CELL 1: Environment Setup\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# STEP 1: Fix NumPy BEFORE any other imports\n",
    "print(\"\\n[Step 1/5] Fixing NumPy compatibility...\")\n",
    "!pip uninstall -y numpy scipy scikit-learn tensorboard > /dev/null 2>&1\n",
    "!pip install -q \"numpy<2\" \"scipy<1.14\" scikit-learn tensorboard\n",
    "\n",
    "import numpy as np\n",
    "print(f\"  ‚úÖ NumPy {np.__version__} (compatible)\")\n",
    "\n",
    "# STEP 2: Setup directories\n",
    "print(\"\\n[Step 2/5] Setting up directories...\")\n",
    "WORK_DIR = '/kaggle/working'\n",
    "REPO_DIR = f'{WORK_DIR}/YOLO-UDD-v2.0'\n",
    "os.chdir(WORK_DIR)\n",
    "print(f\"  ‚úÖ Working directory: {WORK_DIR}\")\n",
    "\n",
    "# STEP 3: Clone repository\n",
    "print(\"\\n[Step 3/5] Cloning repository...\")\n",
    "if os.path.exists(REPO_DIR):\n",
    "    import shutil\n",
    "    shutil.rmtree(REPO_DIR)\n",
    "    print(f\"  üóëÔ∏è  Removed old repository\")\n",
    "\n",
    "!git clone -q https://github.com/kshitijkhede/YOLO-UDD-v2.0.git\n",
    "\n",
    "if os.path.exists(REPO_DIR):\n",
    "    os.chdir(REPO_DIR)\n",
    "    if REPO_DIR not in sys.path:\n",
    "        sys.path.insert(0, REPO_DIR)\n",
    "    print(f\"  ‚úÖ Repository cloned\")\n",
    "else:\n",
    "    raise Exception(\"‚ùå Clone failed!\")\n",
    "\n",
    "# STEP 4: Install PyTorch and dependencies\n",
    "print(\"\\n[Step 4/5] Installing dependencies (~2 min)...\")\n",
    "!pip install -q torch>=2.0.0 torchvision>=0.15.0 albumentations>=1.3.0 \\\n",
    "    opencv-python-headless>=4.7.0 pycocotools>=2.0.6 \\\n",
    "    tqdm pyyaml matplotlib seaborn pandas\n",
    "print(f\"  ‚úÖ Dependencies installed\")\n",
    "\n",
    "# STEP 5: Verify GPU\n",
    "print(\"\\n[Step 5/5] Verifying GPU...\")\n",
    "import torch\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"  ‚úÖ GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"  ‚úÖ Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
    "else:\n",
    "    print(\"  ‚ùå NO GPU! Enable: Settings ‚Üí GPU T4 x2\")\n",
    "    raise RuntimeError(\"GPU required!\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"‚úÖ Cell 1 Complete - Environment Ready!\")\n",
    "print(\"=\"*70)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86fc8028",
   "metadata": {},
   "source": [
    "## Cell 2: Download & Prepare Dataset\n",
    "\n",
    "**Dataset will download from Google Drive and be automatically restructured**\n",
    "\n",
    "- Downloads: ~170 MB (2-3 minutes)\n",
    "- Auto-fixes file structure for training\n",
    "- Verifies all images and annotations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b863aad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download and prepare dataset\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"üì¶ CELL 2: Dataset Setup\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Configuration\n",
    "GDRIVE_FILE_ID = '17oRYriPgBnW9zowwmhImxdUpmHwOjgIp'\n",
    "DATASET_PATH = '/kaggle/working/trashcan'\n",
    "\n",
    "# Download from Google Drive\n",
    "print(\"\\n‚òÅÔ∏è  Downloading from Google Drive...\")\n",
    "!pip install -q gdown\n",
    "!gdown --id {GDRIVE_FILE_ID} -O /kaggle/working/trashcan.zip --quiet\n",
    "\n",
    "if os.path.exists('/kaggle/working/trashcan.zip'):\n",
    "    size = os.path.getsize('/kaggle/working/trashcan.zip') / 1024 / 1024\n",
    "    print(f\"  ‚úÖ Downloaded: {size:.1f} MB\")\n",
    "    \n",
    "    # Remove old dataset if exists\n",
    "    if os.path.exists(DATASET_PATH):\n",
    "        shutil.rmtree(DATASET_PATH)\n",
    "    \n",
    "    # Extract\n",
    "    print(\"  üì¶ Extracting...\")\n",
    "    !unzip -o -q /kaggle/working/trashcan.zip -d /kaggle/working/\n",
    "    \n",
    "    # Check structure\n",
    "    if os.path.exists(f\"{DATASET_PATH}/images/train\"):\n",
    "        print(f\"  ‚úÖ Correct structure detected\")\n",
    "    else:\n",
    "        print(f\"  ‚ö†Ô∏è  Wrong structure - fixing...\")\n",
    "        \n",
    "        # Create correct structure\n",
    "        os.makedirs(f\"{DATASET_PATH}/images/train\", exist_ok=True)\n",
    "        os.makedirs(f\"{DATASET_PATH}/images/val\", exist_ok=True)\n",
    "        \n",
    "        # Find images - they might be in root or 'trashcan' subfolder\n",
    "        image_files_train = []\n",
    "        image_files_val = []\n",
    "        \n",
    "        # Check if images are in trashcan/ root (wrong structure from zip)\n",
    "        if os.path.exists(f\"{DATASET_PATH}\") and not os.path.exists(f\"{DATASET_PATH}/images/train\"):\n",
    "            # Images are in wrong location - move them\n",
    "            for file in os.listdir(DATASET_PATH):\n",
    "                if file.endswith(('.jpg', '.png')):\n",
    "                    # Check if it's training or validation based on annotation\n",
    "                    src = os.path.join(DATASET_PATH, file)\n",
    "                    # For now, we'll determine from annotations later\n",
    "                    image_files_train.append(src)\n",
    "        \n",
    "        print(f\"  \udd27 Restructuring dataset...\")\n",
    "        \n",
    "        # Read annotations to get correct image lists\n",
    "        import json\n",
    "        \n",
    "        # Load train annotations\n",
    "        train_ann_file = f\"{DATASET_PATH}/instances_train_trashcan.json\"\n",
    "        if os.path.exists(train_ann_file):\n",
    "            with open(train_ann_file, 'r') as f:\n",
    "                train_data = json.load(f)\n",
    "            \n",
    "            # Move training images to correct location\n",
    "            for img in train_data['images']:\n",
    "                img_name = img['file_name']\n",
    "                src = os.path.join(DATASET_PATH, img_name)\n",
    "                dst = os.path.join(DATASET_PATH, 'images', 'train', img_name)\n",
    "                \n",
    "                if os.path.exists(src) and not os.path.exists(dst):\n",
    "                    shutil.move(src, dst)\n",
    "        \n",
    "        # Load val annotations\n",
    "        val_ann_file = f\"{DATASET_PATH}/instances_val_trashcan.json\"\n",
    "        if os.path.exists(val_ann_file):\n",
    "            with open(val_ann_file, 'r') as f:\n",
    "                val_data = json.load(f)\n",
    "            \n",
    "            # Move validation images to correct location\n",
    "            for img in val_data['images']:\n",
    "                img_name = img['file_name']\n",
    "                src = os.path.join(DATASET_PATH, img_name)\n",
    "                dst = os.path.join(DATASET_PATH, 'images', 'val', img_name)\n",
    "                \n",
    "                if os.path.exists(src) and not os.path.exists(dst):\n",
    "                    shutil.move(src, dst)\n",
    "        \n",
    "        print(f\"  ‚úÖ Dataset restructured\")\n",
    "else:\n",
    "    raise Exception(\"‚ùå Download failed!\")\n",
    "\n",
    "# Verify final structure\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"‚úÖ DATASET READY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "if os.path.exists(f\"{DATASET_PATH}/images/train\"):\n",
    "    train_count = len([f for f in os.listdir(f\"{DATASET_PATH}/images/train\") if f.endswith(('.jpg', '.png'))])\n",
    "    val_count = len([f for f in os.listdir(f\"{DATASET_PATH}/images/val\") if f.endswith(('.jpg', '.png'))])\n",
    "    \n",
    "    print(f\"\\nüìä Dataset Statistics:\")\n",
    "    print(f\"  Training images: {train_count:,}\")\n",
    "    print(f\"  Validation images: {val_count:,}\")\n",
    "    \n",
    "    # Verify annotations\n",
    "    if os.path.exists(f\"{DATASET_PATH}/instances_train_trashcan.json\"):\n",
    "        print(f\"  ‚úÖ Training annotations found\")\n",
    "    if os.path.exists(f\"{DATASET_PATH}/instances_val_trashcan.json\"):\n",
    "        print(f\"  ‚úÖ Validation annotations found\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"‚úÖ Cell 2 Complete - Ready to Train!\")\n",
    "    print(\"=\"*70)\n",
    "else:\n",
    "    raise Exception(\"‚ùå Dataset structure incorrect!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17f5b365",
   "metadata": {},
   "source": [
    "## Cell 3: Build & Test Model\n",
    "\n",
    "Build YOLO-UDD v2.0 model and verify it works\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a49ee2e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build and test YOLO-UDD model\n",
    "import torch\n",
    "from models.yolo_udd import build_yolo_udd\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"üèóÔ∏è  CELL 3: Build Model\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\n[Step 1/2] Building model...\")\n",
    "model = build_yolo_udd(num_classes=22)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"  ‚úÖ Model: YOLO-UDD v2.0\")\n",
    "print(f\"  ‚úÖ Classes: 22\")\n",
    "print(f\"  ‚úÖ Device: {device}\")\n",
    "print(f\"  ‚úÖ Parameters: {total_params:,}\")\n",
    "\n",
    "# Test forward pass\n",
    "print(\"\\n[Step 2/2] Testing model...\")\n",
    "x = torch.randn(1, 3, 640, 640).to(device)\n",
    "with torch.no_grad():\n",
    "    predictions, turb_score = model(x)\n",
    "\n",
    "print(f\"  ‚úÖ Forward pass successful\")\n",
    "print(f\"  ‚úÖ Turbidity score: {turb_score.item():.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"‚úÖ Cell 3 Complete - Model Ready!\")\n",
    "print(\"=\"*70)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c24cc93",
   "metadata": {},
   "source": [
    "## Cell 4: Start Training üöÄ\n",
    "\n",
    "**This will take ~10 hours for 100 epochs**\n",
    "\n",
    "Training will save checkpoints automatically\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e82fa8c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start training\n",
    "import subprocess\n",
    "import sys\n",
    "import os\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"üöÄ CELL 5: Start Training\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Training parameters\n",
    "EPOCHS = 100\n",
    "BATCH_SIZE = 8\n",
    "IMG_SIZE = 640\n",
    "LEARNING_RATE = 0.001\n",
    "SAVE_DIR = '/kaggle/working/checkpoints'\n",
    "DATASET_PATH = '/kaggle/working/trashcan'\n",
    "\n",
    "# Verify dataset exists\n",
    "if not os.path.exists(DATASET_PATH):\n",
    "    print(f\"‚ùå ERROR: Dataset not found at {DATASET_PATH}\")\n",
    "    print(\"Please run Cell 3 to download the dataset!\")\n",
    "    raise FileNotFoundError(f\"Dataset not found: {DATASET_PATH}\")\n",
    "\n",
    "print(f\"\\nüìã Training Configuration:\")\n",
    "print(f\"  Epochs:        {EPOCHS}\")\n",
    "print(f\"  Batch Size:    {BATCH_SIZE}\")\n",
    "print(f\"  Image Size:    {IMG_SIZE}\")\n",
    "print(f\"  Learning Rate: {LEARNING_RATE}\")\n",
    "print(f\"  Dataset:       {DATASET_PATH}\")\n",
    "print(f\"  Save Dir:      {SAVE_DIR}\")\n",
    "\n",
    "# Verify annotations exist\n",
    "train_ann = os.path.join(DATASET_PATH, 'instances_train_trashcan.json')\n",
    "val_ann = os.path.join(DATASET_PATH, 'instances_val_trashcan.json')\n",
    "print(f\"\\nüîç Checking annotations:\")\n",
    "print(f\"  instances_train_trashcan.json: {'‚úÖ Found' if os.path.exists(train_ann) else '‚ùå NOT FOUND'}\")\n",
    "print(f\"  instances_val_trashcan.json:   {'‚úÖ Found' if os.path.exists(val_ann) else '‚ùå NOT FOUND'}\")\n",
    "\n",
    "if not os.path.exists(train_ann) or not os.path.exists(val_ann):\n",
    "    print(\"\\n‚ùå ERROR: Annotation files missing!\")\n",
    "    print(\"Please run Cell 3 to download the dataset!\")\n",
    "    raise FileNotFoundError(\"Annotations not found\")\n",
    "\n",
    "# Build training command\n",
    "cmd = [\n",
    "    sys.executable, 'scripts/train.py',\n",
    "    '--config', 'configs/train_config.yaml',\n",
    "    '--img-size', str(IMG_SIZE),\n",
    "    '--batch-size', str(BATCH_SIZE),\n",
    "    '--epochs', str(EPOCHS),\n",
    "    '--num-workers', '4',\n",
    "    '--device', 'cuda'\n",
    "]\n",
    "\n",
    "print(\"\\nüöÄ Starting training...\")\n",
    "print(f\"Command: {' '.join(cmd)}\")\n",
    "print(f\"‚è±Ô∏è  Estimated time: ~10 hours for {EPOCHS} epochs\")\n",
    "print(f\"üí° Training progress will appear below...\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "# Run training\n",
    "result = subprocess.run(cmd)\n",
    "\n",
    "if result.returncode == 0:\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"‚úÖ Training completed successfully!\")\n",
    "    print(f\"üìÅ Results saved to: {SAVE_DIR}\")\n",
    "    print(\"üì• Download checkpoints from Output section ‚Üí\")\n",
    "    print(\"=\"*70)\n",
    "else:\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"‚ùå Training failed - check error messages above\")\n",
    "    print(\"=\"*70)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c74b7f0c",
   "metadata": {},
   "source": [
    "## Cell 5: View Results & Download Model\n",
    "\n",
    "Run this after training completes to download your trained model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f06a0c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check training results and download model\n",
    "import os\n",
    "from IPython.display import FileLink\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"üìä CELL 6: Training Results\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "SAVE_DIR = '/kaggle/working/checkpoints'\n",
    "\n",
    "if os.path.exists(SAVE_DIR):\n",
    "    print(f\"\\n‚úÖ Results found: {SAVE_DIR}\\n\")\n",
    "    \n",
    "    # List all checkpoints\n",
    "    checkpoints = [f for f in os.listdir(SAVE_DIR) if f.endswith('.pth')]\n",
    "    \n",
    "    if checkpoints:\n",
    "        print(f\"üì¶ Found {len(checkpoints)} checkpoint(s):\\n\")\n",
    "        for ckpt in sorted(checkpoints):\n",
    "            ckpt_path = os.path.join(SAVE_DIR, ckpt)\n",
    "            size = os.path.getsize(ckpt_path) / (1024*1024)\n",
    "            print(f\"  ‚Ä¢ {ckpt} ({size:.1f} MB)\")\n",
    "        \n",
    "        # Check for best model\n",
    "        best_model = os.path.join(SAVE_DIR, 'best_model.pth')\n",
    "        if os.path.exists(best_model):\n",
    "            size = os.path.getsize(best_model) / (1024*1024)\n",
    "            print(\"\\n\" + \"=\"*70)\n",
    "            print(\"üèÜ TRAINING COMPLETE!\")\n",
    "            print(\"=\"*70)\n",
    "            print(f\"\\n‚úÖ Best Model: best_model.pth\")\n",
    "            print(f\"üì¶ Size: {size:.1f} MB\")\n",
    "            print(f\"\\nüì• Download your trained model:\")\n",
    "            print(f\"   1. Click 'Output' in right sidebar\")\n",
    "            print(f\"   2. Look for '{SAVE_DIR}'\")\n",
    "            print(f\"   3. Download 'best_model.pth'\")\n",
    "            print(f\"\\nüéØ Expected Performance: 70-72% mAP@0.5\")\n",
    "            print(f\"üéâ Success! Model ready for deployment!\")\n",
    "            print(\"=\"*70)\n",
    "        else:\n",
    "            print(\"\\n‚ö†Ô∏è  best_model.pth not found\")\n",
    "            print(\"Training may still be running or incomplete\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è  No checkpoint files found\")\n",
    "        print(\"Training may have failed or not started yet\")\n",
    "else:\n",
    "    print(f\"\\n‚ùå Results directory not found: {SAVE_DIR}\")\n",
    "    print(\"Training has not been run yet or failed to create checkpoints\")\n",
    "    print(\"\\nTo train the model:\")\n",
    "    print(\"  1. Make sure Cell 5 completed without errors\")\n",
    "    print(\"  2. Check GPU is enabled (Settings ‚Üí GPU T4 x2)\")\n",
    "    print(\"  3. Re-run Cell 5 to start training\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9105259f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify repository structure\n",
    "import os\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"üìÇ Repository Structure\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "required_dirs = ['models', 'scripts', 'data', 'utils', 'configs']\n",
    "required_files = ['requirements.txt', 'models/__init__.py', 'scripts/train.py']\n",
    "\n",
    "for dir_name in required_dirs:\n",
    "    status = \"‚úì\" if os.path.exists(dir_name) else \"‚úó\"\n",
    "    print(f\"{status} {dir_name}/\")\n",
    "\n",
    "print()\n",
    "for file_name in required_files:\n",
    "    status = \"‚úì\" if os.path.exists(file_name) else \"‚úó\"\n",
    "    print(f\"{status} {file_name}\")\n",
    "\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "314a5f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIX: Force add models to Python path\n",
    "import os\n",
    "import sys\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"üîß Fixing Module Import Path\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Get current directory\n",
    "current_dir = os.getcwd()\n",
    "print(f\"\\nCurrent directory: {current_dir}\")\n",
    "\n",
    "# Check if we're in the repo directory\n",
    "if 'YOLO-UDD-v2.0' not in current_dir:\n",
    "    print(\"\\n‚ö†Ô∏è  Not in YOLO-UDD-v2.0 directory!\")\n",
    "    \n",
    "    # Try to find and change to it\n",
    "    possible_paths = [\n",
    "        '/kaggle/working/YOLO-UDD-v2.0',\n",
    "        '/kaggle/YOLO-UDD-v2.0',\n",
    "        os.path.join(os.getcwd(), 'YOLO-UDD-v2.0')\n",
    "    ]\n",
    "    \n",
    "    for path in possible_paths:\n",
    "        if os.path.exists(path):\n",
    "            os.chdir(path)\n",
    "            current_dir = os.getcwd()\n",
    "            print(f\"‚úì Changed to: {current_dir}\")\n",
    "            break\n",
    "    else:\n",
    "        print(\"‚úó Could not find YOLO-UDD-v2.0 directory!\")\n",
    "        print(\"  Please re-run the clone cell (Cell 3)\")\n",
    "\n",
    "# Ensure repo is in Python path\n",
    "if current_dir not in sys.path:\n",
    "    sys.path.insert(0, current_dir)\n",
    "    print(f\"‚úì Added to sys.path: {current_dir}\")\n",
    "\n",
    "# Verify models can be imported\n",
    "print(\"\\nüîç Verifying module availability...\")\n",
    "models_path = os.path.join(current_dir, 'models')\n",
    "if os.path.exists(models_path):\n",
    "    print(f\"  ‚úì models/ exists at: {models_path}\")\n",
    "    \n",
    "    # Check for required files\n",
    "    required_files = ['__init__.py', 'yolo_udd.py', 'psem.py', 'sdwh.py', 'tafm.py']\n",
    "    all_present = True\n",
    "    for file in required_files:\n",
    "        file_path = os.path.join(models_path, file)\n",
    "        if os.path.exists(file_path):\n",
    "            print(f\"  ‚úì {file}\")\n",
    "        else:\n",
    "            print(f\"  ‚úó {file} MISSING!\")\n",
    "            all_present = False\n",
    "    \n",
    "    if all_present:\n",
    "        print(\"\\n‚úÖ All model files present - import should work!\")\n",
    "    else:\n",
    "        print(\"\\n‚ùå Some files missing - clone may be incomplete\")\n",
    "        print(\"   ‚Üí Re-run Cell 3 (Clone Repository)\")\n",
    "else:\n",
    "    print(f\"  ‚úó models/ NOT FOUND at: {models_path}\")\n",
    "    print(\"\\n  Available directories:\")\n",
    "    for item in os.listdir(current_dir):\n",
    "        if os.path.isdir(os.path.join(current_dir, item)):\n",
    "            print(f\"    üìÅ {item}/\")\n",
    "    print(\"\\n‚ùå Repository clone failed!\")\n",
    "    print(\"   ‚Üí Re-run Cell 3 (Clone Repository)\")\n",
    "\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12beceaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CRITICAL FIX: Force NumPy 1.x Installation\n",
    "# ============================================================\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"üîß FIXING NumPy Compatibility Issue\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Check current NumPy version\n",
    "import numpy as np\n",
    "current_version = np.__version__\n",
    "print(f\"\\nüìå Current NumPy version: {current_version}\")\n",
    "\n",
    "if current_version.startswith('2.'):\n",
    "    print(\"\\n‚ö†Ô∏è  NumPy 2.x detected - this WILL crash TensorFlow/scikit-learn!\")\n",
    "    print(\"Forcing downgrade to NumPy 1.x...\\n\")\n",
    "    \n",
    "    # Force uninstall NumPy 2.x\n",
    "    import sys\n",
    "    !{sys.executable} -m pip uninstall -y numpy\n",
    "    \n",
    "    # Install NumPy 1.x with force reinstall\n",
    "    !{sys.executable} -m pip install 'numpy==1.26.4' --force-reinstall --no-cache-dir\n",
    "    \n",
    "    # Verify the fix worked\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"‚úÖ NumPy 1.26.4 Installation Complete!\")\n",
    "    print(\"=\"*60)\n",
    "    print(\"\\nüî¥ CRITICAL: YOU MUST RESTART THE KERNEL NOW! üî¥\")\n",
    "    print(\"\\n   Steps:\")\n",
    "    print(\"   1. Click: Session ‚Üí Restart Session (or Ctrl+O O)\")\n",
    "    print(\"   2. Run ALL cells again from Cell 1\")\n",
    "    print(\"   3. Training will work without crashes!\")\n",
    "    print(\"\\nüí° Why? NumPy is already loaded in memory.\")\n",
    "    print(\"   Restarting clears memory and loads NumPy 1.26.4\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Stop execution here - user must restart\n",
    "    raise SystemExit(\"\\n‚õî STOP: Restart kernel now before continuing!\")\n",
    "else:\n",
    "    print(f\"\\n‚úì NumPy 1.x already installed ({current_version})\")\n",
    "    print(\"‚úì Training should work correctly!\")\n",
    "    print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91396242",
   "metadata": {},
   "source": [
    "## Step 2: Install Dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a5e355a",
   "metadata": {},
   "source": [
    "## Step 3: Setup Dataset\n",
    "\n",
    "**Choose ONE of the following methods:**\n",
    "\n",
    "### **METHOD 1: Kaggle Dataset (Recommended)**\n",
    "1. Go to: https://www.kaggle.com/datasets\n",
    "2. Click \"New Dataset\"\n",
    "3. Upload TrashCAN dataset ZIP\n",
    "4. Add to notebook: \"Add Data\" ‚Üí Search for your dataset\n",
    "5. Update `DATASET_PATH` in the cell below\n",
    "\n",
    "### **METHOD 2: Google Drive (Alternative - Easiest)**\n",
    "1. Upload your TrashCAN dataset folder to Google Drive\n",
    "2. Share the folder/file publicly\n",
    "3. Get the file ID from the share link\n",
    "4. Update `GDRIVE_FILE_ID` in the cell below\n",
    "5. Set `USE_GDRIVE = True`\n",
    "\n",
    "### **METHOD 3: Direct Upload in Notebook**\n",
    "1. ZIP your dataset locally\n",
    "2. Upload ZIP to Kaggle notebook directly (< 500MB recommended)\n",
    "3. Set `USE_LOCAL_UPLOAD = True`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f7ea95e",
   "metadata": {},
   "source": [
    "## Step 4: Build Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5f4c728",
   "metadata": {},
   "source": [
    "## Step 5: Training Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ad57a9",
   "metadata": {},
   "source": [
    "## Step 6: Start Training\n",
    "\n",
    "**‚è±Ô∏è Estimated Time**: ~10 hours for 100 epochs on T4 GPU\n",
    "\n",
    "**üí° Tips**:\n",
    "- Training will save checkpoints automatically\n",
    "- You can monitor progress in real-time\n",
    "- Results saved to `/kaggle/working/runs/train/`\n",
    "- Download best checkpoint from Output folder after training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae4ee96f",
   "metadata": {},
   "source": [
    "## Step 7: Download Results\n",
    "\n",
    "After training completes, download the trained model checkpoint."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc4b0f0a",
   "metadata": {},
   "source": [
    "## üéâ Training Complete!\n",
    "\n",
    "### Next Steps:\n",
    "1. **Download Checkpoint**: Download `best.pt` from Output folder\n",
    "2. **Evaluate Model**: Run evaluation script locally with downloaded checkpoint\n",
    "3. **Test Detections**: Test on new images\n",
    "\n",
    "### Expected Results:\n",
    "- mAP@50:95: **70-72%** (22 classes)\n",
    "- Training Time: **~10 hours** (100 epochs)\n",
    "- Checkpoint Size: **~200-300 MB**\n",
    "\n",
    "---\n",
    "\n",
    "**üìß Issues?** Check the GitHub repository: https://github.com/kshitijkhede/YOLO-UDD-v2.0"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
