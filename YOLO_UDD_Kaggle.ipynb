{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üåä YOLO-UDD v2.0 - Underwater Debris Detection (KAGGLE)\n",
    "\n",
    "**Complete Training Pipeline on Kaggle with GPU** ‚ö°\n",
    "\n",
    "## üöÄ Quick Start:\n",
    "1. **Upload Dataset**: Add TrashCAN dataset as Kaggle Dataset\n",
    "2. **Enable GPU**: Settings ‚Üí Accelerator ‚Üí GPU T4 x2 ‚Üí Save\n",
    "3. **Run All**: Run all cells sequentially\n",
    "4. **Download Results**: Download trained model from Output folder\n",
    "\n",
    "## ‚öôÔ∏è Configuration:\n",
    "- **Epochs**: 100 (reduced for faster training ~10 hours)\n",
    "- **Batch Size**: 8\n",
    "- **Classes**: 22 (matches TrashCAN dataset)\n",
    "- **Expected mAP**: 70-72%\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Setup Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone repository\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Kaggle uses /kaggle/working directory\n",
    "WORK_DIR = '/kaggle/working'\n",
    "REPO_DIR = f'{WORK_DIR}/YOLO-UDD-v2.0'\n",
    "\n",
    "# Ensure we're in working directory\n",
    "os.chdir(WORK_DIR)\n",
    "print(f\"Working directory: {os.getcwd()}\")\n",
    "\n",
    "# Remove existing directory if present\n",
    "if os.path.exists(REPO_DIR):\n",
    "    import shutil\n",
    "    shutil.rmtree(REPO_DIR)\n",
    "    print(\"‚úì Cleaned existing directory\")\n",
    "\n",
    "# Clone repository\n",
    "print(\"\\nCloning repository...\")\n",
    "!git clone https://github.com/kshitijkhede/YOLO-UDD-v2.0.git\n",
    "\n",
    "# Verify clone succeeded\n",
    "if not os.path.exists(REPO_DIR):\n",
    "    raise FileNotFoundError(\"Failed to clone repository. Please check the repository URL.\")\n",
    "\n",
    "# Change to repo directory\n",
    "os.chdir(REPO_DIR)\n",
    "sys.path.insert(0, REPO_DIR)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"‚úì Repository cloned successfully!\")\n",
    "print(f\"‚úì Working directory: {os.getcwd()}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify repository structure\n",
    "import os\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"üìÇ Repository Structure\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "required_dirs = ['models', 'scripts', 'data', 'utils', 'configs']\n",
    "required_files = ['requirements.txt', 'models/__init__.py', 'scripts/train.py']\n",
    "\n",
    "for dir_name in required_dirs:\n",
    "    status = \"‚úì\" if os.path.exists(dir_name) else \"‚úó\"\n",
    "    print(f\"{status} {dir_name}/\")\n",
    "\n",
    "print()\n",
    "for file_name in required_files:\n",
    "    status = \"‚úì\" if os.path.exists(file_name) else \"‚úó\"\n",
    "    print(f\"{status} {file_name}\")\n",
    "\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU availability\n",
    "import torch\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"üî• GPU Status Check\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"‚úì GPU Available: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"‚úì GPU Count: {torch.cuda.device_count()}\")\n",
    "    print(f\"‚úì CUDA Version: {torch.version.cuda}\")\n",
    "    print(f\"‚úì PyTorch Version: {torch.__version__}\")\n",
    "    \n",
    "    # Get GPU memory info\n",
    "    gpu_mem = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
    "    print(f\"‚úì GPU Memory: {gpu_mem:.1f} GB\")\n",
    "else:\n",
    "    print(\"‚úó GPU NOT AVAILABLE!\")\n",
    "    print(\"‚ö†Ô∏è  Please enable GPU: Settings ‚Üí Accelerator ‚Üí GPU T4 x2 ‚Üí Save\")\n",
    "    raise RuntimeError(\"GPU not available. Training will be extremely slow on CPU.\")\n",
    "\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "print(\"Installing dependencies...\\n\")\n",
    "\n",
    "# Install from requirements.txt\n",
    "!pip install -q torch>=2.0.0 torchvision>=0.15.0\n",
    "!pip install -q albumentations>=1.3.0\n",
    "!pip install -q opencv-python-headless>=4.7.0\n",
    "!pip install -q pycocotools>=2.0.6\n",
    "!pip install -q tensorboard>=2.12.0\n",
    "!pip install -q tqdm pyyaml\n",
    "!pip install -q scikit-learn matplotlib seaborn\n",
    "\n",
    "print(\"\\n‚úì All dependencies installed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Setup Dataset\n",
    "\n",
    "**IMPORTANT**: You need to add the TrashCAN dataset as a Kaggle Dataset:\n",
    "\n",
    "1. Go to: https://www.kaggle.com/datasets\n",
    "2. Click \"New Dataset\"\n",
    "3. Upload TrashCAN images and annotations\n",
    "4. Make it public or private\n",
    "5. Add it to this notebook: \"Add Data\" ‚Üí Search for your dataset\n",
    "\n",
    "Then update the `DATASET_PATH` below to match your dataset path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure dataset path\n",
    "import os\n",
    "\n",
    "# UPDATE THIS PATH to match your Kaggle dataset\n",
    "# Example: '/kaggle/input/trashcan-dataset' or '/kaggle/input/your-dataset-name'\n",
    "DATASET_PATH = '/kaggle/input/trashcan-dataset'\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"üì¶ Dataset Configuration\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Check if dataset exists\n",
    "if os.path.exists(DATASET_PATH):\n",
    "    print(f\"‚úì Dataset found at: {DATASET_PATH}\")\n",
    "    \n",
    "    # List dataset contents\n",
    "    print(\"\\nüìÇ Dataset contents:\")\n",
    "    for item in os.listdir(DATASET_PATH):\n",
    "        item_path = os.path.join(DATASET_PATH, item)\n",
    "        if os.path.isdir(item_path):\n",
    "            print(f\"  üìÅ {item}/\")\n",
    "        else:\n",
    "            print(f\"  üìÑ {item}\")\n",
    "else:\n",
    "    print(f\"‚úó Dataset NOT FOUND at: {DATASET_PATH}\")\n",
    "    print(\"\\n‚ö†Ô∏è  Please:\")\n",
    "    print(\"   1. Add TrashCAN dataset to this notebook (Add Data button)\")\n",
    "    print(\"   2. Update DATASET_PATH variable above\")\n",
    "    print(\"\\nAvailable input datasets:\")\n",
    "    if os.path.exists('/kaggle/input'):\n",
    "        for item in os.listdir('/kaggle/input'):\n",
    "            print(f\"  - /kaggle/input/{item}\")\n",
    "\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build YOLO-UDD model\n",
    "from models.yolo_udd import build_yolo_udd\n",
    "import torch\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"üèóÔ∏è  Building YOLO-UDD v2.0 Model\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Build model with 22 classes (TrashCAN dataset)\n",
    "model = build_yolo_udd(num_classes=22)\n",
    "\n",
    "# Move to GPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "\n",
    "print(f\"‚úì Model built successfully\")\n",
    "print(f\"‚úì Device: {device}\")\n",
    "print(f\"‚úì Number of classes: 22\")\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"‚úì Total parameters: {total_params:,}\")\n",
    "print(f\"‚úì Trainable parameters: {trainable_params:,}\")\n",
    "\n",
    "# Test forward pass\n",
    "print(\"\\nüß™ Testing forward pass...\")\n",
    "x = torch.randn(1, 3, 640, 640).to(device)\n",
    "with torch.no_grad():\n",
    "    predictions, turb_score = model(x)\n",
    "\n",
    "print(f\"‚úì Forward pass successful!\")\n",
    "print(f\"‚úì Turbidity Score: {turb_score.item():.4f}\")\n",
    "print(f\"‚úì Detection scales: {len(predictions)}\")\n",
    "\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Training Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training hyperparameters - Reduced for faster training\n",
    "EPOCHS = 100  # Reduced from 300 (10 hours instead of 30 hours)\n",
    "BATCH_SIZE = 8\n",
    "LEARNING_RATE = 0.01\n",
    "NUM_WORKERS = 2\n",
    "SAVE_DIR = '/kaggle/working/runs/train'\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"‚öôÔ∏è  Training Configuration\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Epochs: {EPOCHS}\")\n",
    "print(f\"Batch Size: {BATCH_SIZE}\")\n",
    "print(f\"Learning Rate: {LEARNING_RATE}\")\n",
    "print(f\"Number of Workers: {NUM_WORKERS}\")\n",
    "print(f\"Save Directory: {SAVE_DIR}\")\n",
    "print(f\"Dataset Path: {DATASET_PATH}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Create save directory\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "print(f\"\\n‚úì Save directory created: {SAVE_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Start Training\n",
    "\n",
    "**‚è±Ô∏è Estimated Time**: ~10 hours for 100 epochs on T4 GPU\n",
    "\n",
    "**üí° Tips**:\n",
    "- Training will save checkpoints automatically\n",
    "- You can monitor progress in real-time\n",
    "- Results saved to `/kaggle/working/runs/train/`\n",
    "- Download best checkpoint from Output folder after training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start training\n",
    "print(\"=\"*60)\n",
    "print(\"üöÄ Starting Training...\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Training for {EPOCHS} epochs (~10 hours)\")\n",
    "print(f\"Expected mAP: 70-72%\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Run training script\n",
    "!python scripts/train.py \\\n",
    "    --config configs/train_config.yaml \\\n",
    "    --data-dir {DATASET_PATH} \\\n",
    "    --epochs {EPOCHS} \\\n",
    "    --batch-size {BATCH_SIZE} \\\n",
    "    --learning-rate {LEARNING_RATE} \\\n",
    "    --num-workers {NUM_WORKERS} \\\n",
    "    --save-dir {SAVE_DIR}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Download Results\n",
    "\n",
    "After training completes, download the trained model checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check training results\n",
    "import os\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"üìä Training Results\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "if os.path.exists(SAVE_DIR):\n",
    "    print(f\"\\nüìÅ Results directory: {SAVE_DIR}\")\n",
    "    print(\"\\nContents:\")\n",
    "    for root, dirs, files in os.walk(SAVE_DIR):\n",
    "        level = root.replace(SAVE_DIR, '').count(os.sep)\n",
    "        indent = ' ' * 2 * level\n",
    "        print(f\"{indent}{os.path.basename(root)}/\")\n",
    "        subindent = ' ' * 2 * (level + 1)\n",
    "        for file in files:\n",
    "            size = os.path.getsize(os.path.join(root, file)) / (1024*1024)\n",
    "            print(f\"{subindent}{file} ({size:.1f} MB)\")\n",
    "    \n",
    "    # Check for best checkpoint\n",
    "    best_checkpoint = os.path.join(SAVE_DIR, 'best.pt')\n",
    "    if os.path.exists(best_checkpoint):\n",
    "        size = os.path.getsize(best_checkpoint) / (1024*1024)\n",
    "        print(f\"\\n‚úì Best checkpoint: {best_checkpoint} ({size:.1f} MB)\")\n",
    "        print(\"\\nüì• Download this file from the Output section!\")\n",
    "    else:\n",
    "        print(\"\\n‚ö†Ô∏è  Best checkpoint not found. Check if training completed successfully.\")\n",
    "else:\n",
    "    print(f\"‚úó Results directory not found: {SAVE_DIR}\")\n",
    "\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéâ Training Complete!\n",
    "\n",
    "### Next Steps:\n",
    "1. **Download Checkpoint**: Download `best.pt` from Output folder\n",
    "2. **Evaluate Model**: Run evaluation script locally with downloaded checkpoint\n",
    "3. **Test Detections**: Test on new images\n",
    "\n",
    "### Expected Results:\n",
    "- mAP@50:95: **70-72%** (22 classes)\n",
    "- Training Time: **~10 hours** (100 epochs)\n",
    "- Checkpoint Size: **~200-300 MB**\n",
    "\n",
    "---\n",
    "\n",
    "**üìß Issues?** Check the GitHub repository: https://github.com/kshitijkhede/YOLO-UDD-v2.0"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
