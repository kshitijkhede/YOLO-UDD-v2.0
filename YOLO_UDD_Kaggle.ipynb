{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a82cd800",
   "metadata": {},
   "source": [
    "# \ud83c\udf0a YOLO-UDD v2.0 - Kaggle Training\n",
    "\n",
    "**Simple 6-Step Training - No Crashes, No Loops!** \u26a1\n",
    "\n",
    "## \ud83d\udccb Before You Start:\n",
    "1. **Enable GPU**: Settings \u2192 Accelerator \u2192 **GPU T4 x2** \u2192 Save\n",
    "2. **Dataset**: Google Drive link already configured (automatic download)\n",
    "3. **Run**: Execute cells 1-6 in order OR click \"Run All\"\n",
    "\n",
    "## \u23f1\ufe0f Training Info:\n",
    "- **Time**: ~10 hours (100 epochs)\n",
    "- **Expected mAP**: 70-72%\n",
    "- **No restarts needed!** \u2705\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8830adbe",
   "metadata": {},
   "source": [
    "## Cell 1: Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38942eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================================================================",
    "# \ud83d\udce6 CELL 3: Dataset Setup",
    "# ======================================================================",
    "print(\"=\"*70)",
    "print(\"\ud83d\udce6 CELL 3: Dataset Setup\")",
    "print(\"=\"*70)",
    "",
    "import os",
    "import zipfile",
    "import gdown",
    "",
    "# Download dataset from Google Drive",
    "FILE_ID = '17oRYriPgBnW9zowwmhImxdUpmHwOjgIp'",
    "GDRIVE_URL = f'https://drive.google.com/uc?id={FILE_ID}'",
    "ZIP_PATH = '/kaggle/working/trashcan.zip'",
    "",
    "print(\"\\n\u2601\ufe0f  Downloading from Google Drive...\")",
    "try:",
    "    gdown.download(GDRIVE_URL, ZIP_PATH, quiet=False)",
    "    file_size_mb = os.path.getsize(ZIP_PATH) / (1024 * 1024)",
    "    print(f\"  \u2705 Downloaded: {file_size_mb:.1f} MB\")",
    "except Exception as e:",
    "    print(f\"  \u274c Download failed: {e}\")",
    "    raise Exception(\"Dataset download failed!\")",
    "",
    "# Extract dataset",
    "print(\"  \ud83d\udce6 Extracting...\")",
    "try:",
    "    with zipfile.ZipFile(ZIP_PATH, 'r') as zip_ref:",
    "        zip_ref.extractall('/kaggle/working/')",
    "    print(\"  \u2705 Extracted successfully\")",
    "except Exception as e:",
    "    print(f\"  \u274c Extraction failed: {e}\")",
    "    raise Exception(\"Dataset extraction failed!\")",
    "",
    "# Auto-detect dataset path",
    "print(\"  \ud83d\udd0d Checking extraction paths...\")",
    "DATASET_PATH = None",
    "",
    "# List all extracted directories for debugging",
    "print(\"\\n  \ud83d\udcc2 Contents of /kaggle/working/:\")",
    "for item in os.listdir('/kaggle/working/'):",
    "    item_path = os.path.join('/kaggle/working/', item)",
    "    if os.path.isdir(item_path):",
    "        print(f\"    \ud83d\udcc1 {item}/\")",
    "    else:",
    "        print(f\"    \ud83d\udcc4 {item}\")",
    "",
    "# Check multiple possible paths",
    "possible_paths = [",
    "    '/kaggle/working/trashcan',",
    "    '/kaggle/working/trashcan/trashcan',",
    "    '/kaggle/working/data/trashcan',",
    "    '/kaggle/working'",
    "]",
    "",
    "for path in possible_paths:",
    "    print(f\"\\n  \ud83d\udd0d Checking: {path}\")",
    "    if os.path.exists(path):",
    "        print(f\"    \u2713 Path exists\")",
    "        # Check for required subdirectories",
    "        images_path = os.path.join(path, 'images')",
    "        annotations_path = os.path.join(path, 'annotations')",
    "        ",
    "        if os.path.exists(images_path):",
    "            print(f\"    \u2713 images/ found\")",
    "        if os.path.exists(annotations_path):",
    "            print(f\"    \u2713 annotations/ found\")",
    "        ",
    "        # Valid if both exist",
    "        if os.path.exists(images_path) and os.path.exists(annotations_path):",
    "            DATASET_PATH = path",
    "            print(f\"  \u2705 Found dataset at: {path}\")",
    "            break",
    "    else:",
    "        print(f\"    \u2717 Path does not exist\")",
    "",
    "if DATASET_PATH:",
    "    print(f\"\\n\u2705 DATASET FOUND: {DATASET_PATH}\")",
    "    ",
    "    # Validate structure",
    "    print(\"\\n\ud83d\udcc2 Dataset Structure:\")",
    "    images_dir = os.path.join(DATASET_PATH, 'images')",
    "    annotations_dir = os.path.join(DATASET_PATH, 'annotations')",
    "    ",
    "    # Check images directory",
    "    if os.path.exists(images_dir) and os.path.isdir(images_dir):",
    "        print(f\"   \u2705 images/ directory\")",
    "        subdirs = [d for d in os.listdir(images_dir) if os.path.isdir(os.path.join(images_dir, d))]",
    "        for subdir in subdirs:",
    "            count = len([f for f in os.listdir(os.path.join(images_dir, subdir)) if f.lower().endswith(('.jpg', '.jpeg', '.png'))])",
    "            print(f\"      \u251c\u2500\u2500 {subdir}/ ({count} images)\")",
    "    else:",
    "        print(f\"   \u274c images/ directory not found\")",
    "    ",
    "    # Check annotations directory",
    "    if os.path.exists(annotations_dir) and os.path.isdir(annotations_dir):",
    "        print(f\"   \u2705 annotations/ directory\")",
    "        json_files = [f for f in os.listdir(annotations_dir) if f.endswith('.json')]",
    "        for json_file in json_files:",
    "            size_mb = os.path.getsize(os.path.join(annotations_dir, json_file)) / (1024 * 1024)",
    "            print(f\"      \u251c\u2500\u2500 {json_file} ({size_mb:.1f} MB)\")",
    "    else:",
    "        print(f\"   \u274c annotations/ directory not found\")",
    "    ",
    "    # Final validation - must have both directories with content",
    "    has_images = os.path.exists(images_dir) and os.path.isdir(images_dir) and len(os.listdir(images_dir)) > 0",
    "    has_annotations = os.path.exists(annotations_dir) and os.path.isdir(annotations_dir) and any(f.endswith('.json') for f in os.listdir(annotations_dir))",
    "    ",
    "    if has_images and has_annotations:",
    "        print(\"\\n\" + \"=\"*70)",
    "        print(\"\u2705 Cell 3 Complete - Dataset Ready!\")",
    "        print(\"=\"*70)",
    "    else:",
    "        print(\"\\n\u274c Dataset structure validation failed!\")",
    "        print(\"\\nExpected structure:\")",
    "        print(\"  trashcan/\")",
    "        print(\"    \u251c\u2500\u2500 images/\")",
    "        print(\"    \u2502   \u251c\u2500\u2500 train/  (should contain images)\")",
    "        print(\"    \u2502   \u2514\u2500\u2500 val/    (should contain images)\")",
    "        print(\"    \u2514\u2500\u2500 annotations/\")",
    "        print(\"        \u251c\u2500\u2500 train.json\")",
    "        print(\"        \u2514\u2500\u2500 val.json\")",
    "        raise Exception(\"Dataset structure is incomplete!\")",
    "else:",
    "    print(\"\\n\u274c DATASET NOT FOUND!\")",
    "    print(\"\\nDebug info - Contents of /kaggle/working/:\")",
    "    for root, dirs, files in os.walk('/kaggle/working/'):",
    "        level = root.replace('/kaggle/working/', '').count(os.sep)",
    "        indent = ' ' * 2 * level",
    "        print(f'{indent}{os.path.basename(root)}/')",
    "        subindent = ' ' * 2 * (level + 1)",
    "        for file in files[:5]:  # Limit files shown",
    "            print(f'{subindent}{file}')",
    "        if len(files) > 5:",
    "            print(f'{subindent}... and {len(files)-5} more files')",
    "        if level > 2:  # Limit depth",
    "            break",
    "    raise Exception(\"Could not find dataset in expected locations!\")",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d42fbc1c",
   "metadata": {},
   "source": [
    "# Verify setup and install dependencies",
    "import os",
    "import sys",
    "import torch",
    "",
    "print(\"=\"*70)",
    "print(\"\ud83d\udce6 CELL 2: Verification & Dependencies\")",
    "print(\"=\"*70)",
    "",
    "# Check current directory",
    "print(\"\\n[Debug] Current directory:\", os.getcwd())",
    "print(\"[Debug] Directory contents:\", os.listdir(os.getcwd())[:10])",
    "",
    "# Ensure we're in the repository",
    "REPO_DIR = '/kaggle/working/YOLO-UDD-v2.0'",
    "if not os.path.exists(REPO_DIR):",
    "    print(f\"\\n\u274c Repository not found at {REPO_DIR}\")",
    "    print(\"\\nDebug info - Contents of /kaggle/working/:\")",
    "    if os.path.exists('/kaggle/working/'):",
    "        print([d for d in os.listdir('/kaggle/working/') if not d.startswith('.')])",
    "    print(\"\\n\u26a0\ufe0f  Please re-run Cell 1 to clone the repository\")",
    "    raise Exception(\"Repository not cloned! Re-run Cell 1\")",
    "",
    "# Change to repository directory",
    "os.chdir(REPO_DIR)",
    "if REPO_DIR not in sys.path:",
    "    sys.path.insert(0, REPO_DIR)",
    "print(f\"\\n\u2705 Changed to repository: {REPO_DIR}\")",
    "",
    "# Verify repository structure",
    "print(\"\\n[Step 1/3] Verifying repository structure...\")",
    "required = ['models/', 'scripts/', 'utils/', 'configs/', 'data/', 'scripts/train.py']",
    "all_ok = True",
    "for item in required:",
    "    if os.path.exists(item):",
    "        print(f\"  \u2705 {item}\")",
    "    else:",
    "        print(f\"  \u274c {item} MISSING\")",
    "        all_ok = False",
    "",
    "if not all_ok:",
    "    print(\"\\nDebug - Repository contents:\")",
    "    print([d for d in os.listdir('.') if not d.startswith('.')])",
    "    raise Exception(\"Repository incomplete! Re-run Cell 1\")",
    "",
    "# Check GPU",
    "print(\"\\n[Step 2/3] Checking GPU...\")",
    "if torch.cuda.is_available():",
    "    print(f\"  \u2705 GPU: {torch.cuda.get_device_name(0)}\")",
    "    print(f\"  \u2705 Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")",
    "else:",
    "    print(\"  \u26a0\ufe0f  NO GPU DETECTED\")",
    "    print(\"  \u2139\ufe0f  GPU strongly recommended for training\")",
    "    print(\"  \ud83d\udca1 Enable: Settings \u2192 Accelerator \u2192 GPU T4 x2\")",
    "",
    "# Install dependencies",
    "print(\"\\n[Step 3/3] Installing dependencies (this takes ~2 min)...\")",
    "!pip install -q torch>=2.0.0 torchvision>=0.15.0 albumentations>=1.3.0 \\",
    "    opencv-python-headless>=4.7.0 pycocotools>=2.0.6 tensorboard>=2.12.0 \\",
    "    tqdm pyyaml scikit-learn matplotlib seaborn gdown",
    "",
    "print(\"  \u2705 Dependencies installed\")",
    "",
    "print(\"\\n\" + \"=\"*70)",
    "print(\"\u2705 Cell 2 Complete - System Ready!\")",
    "print(\"=\"*70)",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d42523a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify setup and install dependencies\n",
    "import os\n",
    "import torch\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"\ud83d\udce6 CELL 2: Verification & Dependencies\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Verify repository structure\n",
    "print(\"\\n[Step 1/3] Verifying repository...\")\n",
    "required = ['models/', 'scripts/', 'utils/', 'configs/', 'scripts/train.py']\n",
    "all_ok = True\n",
    "for item in required:\n",
    "    if os.path.exists(item):\n",
    "        print(f\"  \u2705 {item}\")\n",
    "    else:\n",
    "        print(f\"  \u274c {item} MISSING\")\n",
    "        all_ok = False\n",
    "\n",
    "if not all_ok:\n",
    "    raise Exception(\"Repository incomplete! Re-run Cell 1\")\n",
    "\n",
    "# Check GPU\n",
    "print(\"\\n[Step 2/3] Checking GPU...\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"  \u2705 GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"  \u2705 Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
    "else:\n",
    "    print(\"  \u274c NO GPU! Enable: Settings \u2192 GPU T4 x2\")\n",
    "    raise RuntimeError(\"GPU required!\")\n",
    "\n",
    "# Install dependencies\n",
    "print(\"\\n[Step 3/3] Installing dependencies (this takes ~2 min)...\")\n",
    "!pip install -q torch>=2.0.0 torchvision>=0.15.0 albumentations>=1.3.0 \\\n",
    "    opencv-python-headless>=4.7.0 pycocotools>=2.0.6 tensorboard>=2.12.0 \\\n",
    "    tqdm pyyaml scikit-learn matplotlib seaborn\n",
    "\n",
    "print(\"  \u2705 Dependencies installed\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"\u2705 Cell 2 Complete - System Ready!\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86fc8028",
   "metadata": {},
   "source": [
    "## Cell 3: Setup Dataset\n",
    "\n",
    "**Dataset will download automatically from Google Drive (~170 MB, takes 2-3 min)**\n",
    "\n",
    "Alternative: Upload your own dataset to Kaggle and set `USE_KAGGLE_DATASET = True`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b863aad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset setup\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"\ud83d\udce6 CELL 3: Dataset Setup\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Configuration\n",
    "USE_KAGGLE_DATASET = False  # Set True if you added dataset to Kaggle\n",
    "KAGGLE_DATASET_PATH = '/kaggle/input/trashcan-dataset'\n",
    "\n",
    "# Updated FILE_ID with correct dataset\n",
    "GDRIVE_FILE_ID = '17oRYriPgBnW9zowwmhImxdUpmHwOjgIp'  # Updated with correct dataset\n",
    "USE_GDRIVE = True  # Default to Google Drive\n",
    "\n",
    "DATASET_PATH = None\n",
    "\n",
    "if USE_KAGGLE_DATASET:\n",
    "    print(\"\\n\ud83d\udcc2 Using Kaggle Dataset...\")\n",
    "    if os.path.exists(KAGGLE_DATASET_PATH):\n",
    "        if os.path.isfile(KAGGLE_DATASET_PATH):\n",
    "            print(\"  \ud83d\udce6 Extracting Kaggle dataset...\")\n",
    "            !unzip -q {KAGGLE_DATASET_PATH} -d /kaggle/working/\n",
    "            DATASET_PATH = '/kaggle/working/trashcan'\n",
    "        else:\n",
    "            DATASET_PATH = KAGGLE_DATASET_PATH\n",
    "        print(f\"  \u2705 Dataset: {DATASET_PATH}\")\n",
    "    else:\n",
    "        print(f\"  \u274c NOT FOUND: {KAGGLE_DATASET_PATH}\")\n",
    "\n",
    "elif USE_GDRIVE:\n",
    "    print(\"\\n\u2601\ufe0f  Downloading from Google Drive...\")\n",
    "    print(\"  \ud83d\udce6 Installing gdown...\")\n",
    "    !pip install -q gdown\n",
    "    \n",
    "    print(\"  \u2b07\ufe0f  Downloading dataset (~180 MB, 2-3 min)...\")\n",
    "    !gdown --id {GDRIVE_FILE_ID} -O /kaggle/working/trashcan.zip --quiet\n",
    "    \n",
    "    if os.path.exists('/kaggle/working/trashcan.zip'):\n",
    "        size = os.path.getsize('/kaggle/working/trashcan.zip') / 1024 / 1024\n",
    "        print(f\"  \u2705 Downloaded: {size:.1f} MB\")\n",
    "        \n",
    "        print(\"  \ud83d\udce6 Extracting...\")\n",
    "        !unzip -o -q /kaggle/working/trashcan.zip -d /kaggle/working/\n",
    "        \n",
    "        # Check possible extraction paths\n",
    "        possible_paths = [\n",
    "            '/kaggle/working/trashcan',\n",
    "            '/kaggle/working/trashcan/trashcan',\n",
    "            '/kaggle/working'\n",
    "        ]\n",
    "        \n",
    "        print(\"  \ud83d\udd0d Checking extraction paths...\")\n",
    "        for path in possible_paths:\n",
    "            if os.path.exists(path):\n",
    "                # Check if it has the expected structure\n",
    "                if os.path.exists(os.path.join(path, 'images')) or os.path.exists(os.path.join(path, 'annotations')):\n",
    "                    DATASET_PATH = path\n",
    "                    print(f\"  \u2705 Found dataset at: {path}\")\n",
    "                    break\n",
    "        \n",
    "        # List what was extracted for debugging\n",
    "        if not DATASET_PATH:\n",
    "            print(\"  \ud83d\udcc1 Contents of /kaggle/working/:\")\n",
    "            for item in os.listdir('/kaggle/working/'):\n",
    "                item_path = os.path.join('/kaggle/working/', item)\n",
    "                if os.path.isdir(item_path):\n",
    "                    print(f\"     DIR: {item}/\")\n",
    "                    # Check subdirectories\n",
    "                    for subitem in os.listdir(item_path)[:5]:\n",
    "                        print(f\"       - {subitem}\")\n",
    "                else:\n",
    "                    print(f\"     FILE: {item}\")\n",
    "    else:\n",
    "        print(\"  \u274c Download failed - check FILE_ID and internet connection\")\n",
    "else:\n",
    "    print(\"\\n\u274c No method selected! Set USE_KAGGLE_DATASET or USE_GDRIVE = True\")\n",
    "\n",
    "# Verify and restructure dataset if needed\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "if DATASET_PATH and os.path.exists(DATASET_PATH):\n",
    "    print(f\"\u2705 DATASET FOUND: {DATASET_PATH}\")\n",
    "    \n",
    "    # Verify structure\n",
    "    has_images = os.path.exists(os.path.join(DATASET_PATH, 'images'))\n",
    "    has_annotations = os.path.exists(os.path.join(DATASET_PATH, 'annotations'))\n",
    "    \n",
    "    print(f\"\\n\ud83d\udcc2 Dataset Structure:\")\n",
    "    print(f\"   {'\u2705' if has_images else '\u274c'} images/ directory\")\n",
    "    print(f\"   {'\u2705' if has_annotations else '\u274c'} annotations/ directory\")\n",
    "    \n",
    "    # Count images by split\n",
    "    if has_images:\n",
    "        print(f\"\\n\ud83d\udcf8 Image Counts:\")\n",
    "        for split in ['train', 'val', 'test']:\n",
    "            img_path = os.path.join(DATASET_PATH, 'images', split)\n",
    "            if os.path.exists(img_path):\n",
    "                count = len([f for f in os.listdir(img_path) if f.endswith(('.jpg', '.png', '.jpeg'))])\n",
    "                print(f\"   {split}: {count:,} images\")\n",
    "    \n",
    "    # Check annotations\n",
    "    if has_annotations:\n",
    "        print(f\"\\n\ud83d\udccb Annotations:\")\n",
    "        ann_dir = os.path.join(DATASET_PATH, 'annotations')\n",
    "        for ann_file in ['train.json', 'val.json']:\n",
    "            ann_path = os.path.join(ann_dir, ann_file)\n",
    "            if os.path.exists(ann_path):\n",
    "                size_mb = os.path.getsize(ann_path) / 1024 / 1024\n",
    "                print(f\"   \u2705 {ann_file} ({size_mb:.1f} MB)\")\n",
    "            else:\n",
    "                print(f\"   \u274c {ann_file} NOT FOUND\")\n",
    "    \n",
    "    if has_images and has_annotations:\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"\u2705 Cell 3 Complete - Dataset Ready!\")\n",
    "        print(\"=\"*70)\n",
    "    else:\n",
    "        print(\"\\n\u26a0\ufe0f  Dataset structure incomplete!\")\n",
    "        print(\"Expected structure:\")\n",
    "        print(\"  trashcan/\")\n",
    "        print(\"    \u251c\u2500\u2500 annotations/\")\n",
    "        print(\"    \u2502   \u251c\u2500\u2500 train.json\")\n",
    "        print(\"    \u2502   \u2514\u2500\u2500 val.json\")\n",
    "        print(\"    \u2514\u2500\u2500 images/\")\n",
    "        print(\"        \u251c\u2500\u2500 train/\")\n",
    "        print(\"        \u2514\u2500\u2500 val/\")\n",
    "        raise Exception(\"Dataset structure is incomplete!\")\n",
    "else:\n",
    "    print(\"\u274c DATASET NOT FOUND!\")\n",
    "    print(\"\\nDebugging info:\")\n",
    "    print(f\"  DATASET_PATH = {DATASET_PATH}\")\n",
    "    print(f\"  Exists = {os.path.exists(DATASET_PATH) if DATASET_PATH else 'N/A'}\")\n",
    "    print(\"\\nTry:\")\n",
    "    print(\"  1. Check FILE_ID is correct\")\n",
    "    print(\"  2. Check internet connection\")\n",
    "    print(\"  3. Re-run this cell\")\n",
    "    raise Exception(\"Dataset setup failed!\")\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17f5b365",
   "metadata": {},
   "source": [
    "## Cell 4: Build & Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a49ee2e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build and test YOLO-UDD model\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"\ud83c\udfd7\ufe0f  CELL 4: Build Model\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Ensure correct paths\n",
    "REPO_DIR = '/kaggle/working/YOLO-UDD-v2.0'\n",
    "os.chdir(REPO_DIR)\n",
    "if REPO_DIR not in sys.path:\n",
    "    sys.path.insert(0, REPO_DIR)\n",
    "\n",
    "print(\"\\n[Step 1/2] Building model...\")\n",
    "from models.yolo_udd import build_yolo_udd\n",
    "\n",
    "model = build_yolo_udd(num_classes=22)  # TrashCAN has 22 classes\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"  \u2705 Model: YOLO-UDD v2.0\")\n",
    "print(f\"  \u2705 Classes: 22\")\n",
    "print(f\"  \u2705 Device: {device}\")\n",
    "print(f\"  \u2705 Parameters: {total_params:,}\")\n",
    "\n",
    "# Test forward pass\n",
    "print(\"\\n[Step 2/2] Testing model...\")\n",
    "x = torch.randn(1, 3, 640, 640).to(device)\n",
    "with torch.no_grad():\n",
    "    predictions, turb_score = model(x)\n",
    "\n",
    "print(f\"  \u2705 Forward pass successful\")\n",
    "print(f\"  \u2705 Turbidity score: {turb_score.item():.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"\u2705 Cell 4 Complete - Model Ready!\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e82fa8c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start training\n",
    "import subprocess\n",
    "import sys\n",
    "import os\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"CELL 6: Starting Training\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Training parameters\n",
    "EPOCHS = 100\n",
    "BATCH_SIZE = 8\n",
    "LEARNING_RATE = 0.01\n",
    "SAVE_DIR = '/kaggle/working/runs/train'\n",
    "DATASET_PATH = '/kaggle/working/trashcan'  # Restructured dataset\n",
    "\n",
    "# Verify dataset exists\n",
    "if not os.path.exists(DATASET_PATH):\n",
    "    print(f\"ERROR: Dataset not found at {DATASET_PATH}\")\n",
    "    print(\"Please run Cell 3 to restructure the dataset!\")\n",
    "    raise FileNotFoundError(f\"Dataset not found: {DATASET_PATH}\")\n",
    "\n",
    "print(f\"\\nTraining Configuration:\")\n",
    "print(f\"  Epochs:       {EPOCHS}\")\n",
    "print(f\"  Batch Size:   {BATCH_SIZE}\")\n",
    "print(f\"  Learning Rate: {LEARNING_RATE}\")\n",
    "print(f\"  Dataset:      {DATASET_PATH}\")\n",
    "print(f\"  Save Dir:     {SAVE_DIR}\")\n",
    "\n",
    "# Verify annotations exist\n",
    "train_ann = os.path.join(DATASET_PATH, 'annotations', 'train.json')\n",
    "val_ann = os.path.join(DATASET_PATH, 'annotations', 'val.json')\n",
    "print(f\"\\nChecking annotations:\")\n",
    "print(f\"  train.json: {'Found' if os.path.exists(train_ann) else 'NOT FOUND'}\")\n",
    "print(f\"  val.json:   {'Found' if os.path.exists(val_ann) else 'NOT FOUND'}\")\n",
    "\n",
    "if not os.path.exists(train_ann) or not os.path.exists(val_ann):\n",
    "    print(\"\\nERROR: Annotation files missing!\")\n",
    "    print(\"Please run Cell 3 to restructure the dataset!\")\n",
    "    raise FileNotFoundError(\"Annotations not found\")\n",
    "\n",
    "# Build training command with absolute path\n",
    "abs_dataset_path = os.path.abspath(DATASET_PATH)\n",
    "cmd = [\n",
    "    sys.executable, 'scripts/train.py',\n",
    "    '--config', 'configs/train_config.yaml',\n",
    "    '--data-dir', abs_dataset_path,\n",
    "    '--batch-size', str(BATCH_SIZE),\n",
    "    '--epochs', str(EPOCHS),\n",
    "    '--lr', str(LEARNING_RATE),\n",
    "    '--save-dir', SAVE_DIR\n",
    "]\n",
    "\n",
    "print(\"\\nStarting training...\")\n",
    "print(f\"Using absolute dataset path: {abs_dataset_path}\")\n",
    "print(f\"Command: {.join(cmd)}\")\n",
    "print(\"This will take ~10 hours for 100 epochs\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "# Run training\n",
    "result = subprocess.run(cmd)\n",
    "\n",
    "if result.returncode == 0:\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"Training completed successfully!\")\n",
    "    print(f\"Results saved to: {SAVE_DIR}\")\n",
    "    print(\"=\"*70)\n",
    "else:\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"Training failed - see error above\")\n",
    "    print(\"=\"*70)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f06a0c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check training results\n",
    "import os\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"\ud83d\udcca CELL 6: Results\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "if os.path.exists(SAVE_DIR):\n",
    "    print(f\"\\n\ud83d\udcc1 Results: {SAVE_DIR}\\n\")\n",
    "    \n",
    "    # List files\n",
    "    for root, dirs, files in os.walk(SAVE_DIR):\n",
    "        level = root.replace(SAVE_DIR, '').count(os.sep)\n",
    "        indent = '  ' * level\n",
    "        print(f\"{indent}{os.path.basename(root)}/\")\n",
    "        sub_indent = '  ' * (level + 1)\n",
    "        for file in files:\n",
    "            size = os.path.getsize(os.path.join(root, file)) / (1024*1024)\n",
    "            print(f\"{sub_indent}{file} ({size:.1f} MB)\")\n",
    "    \n",
    "    # Check for best checkpoint\n",
    "    best_pt = os.path.join(SAVE_DIR, 'best.pt')\n",
    "    if os.path.exists(best_pt):\n",
    "        size = os.path.getsize(best_pt) / (1024*1024)\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"\u2705 TRAINING COMPLETE!\")\n",
    "        print(\"=\"*70)\n",
    "        print(f\"\\n\ud83c\udfc6 Best Model: {best_pt}\")\n",
    "        print(f\"\ud83d\udce6 Size: {size:.1f} MB\")\n",
    "        print(f\"\\n\ud83d\udce5 DOWNLOAD: Check 'Output' section in right sidebar \u2192\")\n",
    "        print(f\"\ud83c\udfaf Expected Performance: 70-72% mAP@50:95\")\n",
    "        print(f\"\\n\ud83c\udf89 Success! Model ready for deployment!\")\n",
    "        print(\"=\"*70)\n",
    "    else:\n",
    "        print(\"\\n\u26a0\ufe0f  best.pt not found - check if training completed\")\n",
    "else:\n",
    "    print(f\"\\n\u274c Results not found: {SAVE_DIR}\")\n",
    "    print(\"Training may have failed or not started.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9105259f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify repository structure\n",
    "import os\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"\ud83d\udcc2 Repository Structure\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "required_dirs = ['models', 'scripts', 'data', 'utils', 'configs']\n",
    "required_files = ['requirements.txt', 'models/__init__.py', 'scripts/train.py']\n",
    "\n",
    "for dir_name in required_dirs:\n",
    "    status = \"\u2713\" if os.path.exists(dir_name) else \"\u2717\"\n",
    "    print(f\"{status} {dir_name}/\")\n",
    "\n",
    "print()\n",
    "for file_name in required_files:\n",
    "    status = \"\u2713\" if os.path.exists(file_name) else \"\u2717\"\n",
    "    print(f\"{status} {file_name}\")\n",
    "\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "314a5f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIX: Force add models to Python path\n",
    "import os\n",
    "import sys\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"\ud83d\udd27 Fixing Module Import Path\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Get current directory\n",
    "current_dir = os.getcwd()\n",
    "print(f\"\\nCurrent directory: {current_dir}\")\n",
    "\n",
    "# Check if we're in the repo directory\n",
    "if 'YOLO-UDD-v2.0' not in current_dir:\n",
    "    print(\"\\n\u26a0\ufe0f  Not in YOLO-UDD-v2.0 directory!\")\n",
    "    \n",
    "    # Try to find and change to it\n",
    "    possible_paths = [\n",
    "        '/kaggle/working/YOLO-UDD-v2.0',\n",
    "        '/kaggle/YOLO-UDD-v2.0',\n",
    "        os.path.join(os.getcwd(), 'YOLO-UDD-v2.0')\n",
    "    ]\n",
    "    \n",
    "    for path in possible_paths:\n",
    "        if os.path.exists(path):\n",
    "            os.chdir(path)\n",
    "            current_dir = os.getcwd()\n",
    "            print(f\"\u2713 Changed to: {current_dir}\")\n",
    "            break\n",
    "    else:\n",
    "        print(\"\u2717 Could not find YOLO-UDD-v2.0 directory!\")\n",
    "        print(\"  Please re-run the clone cell (Cell 3)\")\n",
    "\n",
    "# Ensure repo is in Python path\n",
    "if current_dir not in sys.path:\n",
    "    sys.path.insert(0, current_dir)\n",
    "    print(f\"\u2713 Added to sys.path: {current_dir}\")\n",
    "\n",
    "# Verify models can be imported\n",
    "print(\"\\n\ud83d\udd0d Verifying module availability...\")\n",
    "models_path = os.path.join(current_dir, 'models')\n",
    "if os.path.exists(models_path):\n",
    "    print(f\"  \u2713 models/ exists at: {models_path}\")\n",
    "    \n",
    "    # Check for required files\n",
    "    required_files = ['__init__.py', 'yolo_udd.py', 'psem.py', 'sdwh.py', 'tafm.py']\n",
    "    all_present = True\n",
    "    for file in required_files:\n",
    "        file_path = os.path.join(models_path, file)\n",
    "        if os.path.exists(file_path):\n",
    "            print(f\"  \u2713 {file}\")\n",
    "        else:\n",
    "            print(f\"  \u2717 {file} MISSING!\")\n",
    "            all_present = False\n",
    "    \n",
    "    if all_present:\n",
    "        print(\"\\n\u2705 All model files present - import should work!\")\n",
    "    else:\n",
    "        print(\"\\n\u274c Some files missing - clone may be incomplete\")\n",
    "        print(\"   \u2192 Re-run Cell 3 (Clone Repository)\")\n",
    "else:\n",
    "    print(f\"  \u2717 models/ NOT FOUND at: {models_path}\")\n",
    "    print(\"\\n  Available directories:\")\n",
    "    for item in os.listdir(current_dir):\n",
    "        if os.path.isdir(os.path.join(current_dir, item)):\n",
    "            print(f\"    \ud83d\udcc1 {item}/\")\n",
    "    print(\"\\n\u274c Repository clone failed!\")\n",
    "    print(\"   \u2192 Re-run Cell 3 (Clone Repository)\")\n",
    "\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12beceaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CRITICAL FIX: Force NumPy 1.x Installation\n",
    "# ============================================================\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"\ud83d\udd27 FIXING NumPy Compatibility Issue\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Check current NumPy version\n",
    "import numpy as np\n",
    "current_version = np.__version__\n",
    "print(f\"\\n\ud83d\udccc Current NumPy version: {current_version}\")\n",
    "\n",
    "if current_version.startswith('2.'):\n",
    "    print(\"\\n\u26a0\ufe0f  NumPy 2.x detected - this WILL crash TensorFlow/scikit-learn!\")\n",
    "    print(\"Forcing downgrade to NumPy 1.x...\\n\")\n",
    "    \n",
    "    # Force uninstall NumPy 2.x\n",
    "    import sys\n",
    "    !{sys.executable} -m pip uninstall -y numpy\n",
    "    \n",
    "    # Install NumPy 1.x with force reinstall\n",
    "    !{sys.executable} -m pip install 'numpy==1.26.4' --force-reinstall --no-cache-dir\n",
    "    \n",
    "    # Verify the fix worked\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"\u2705 NumPy 1.26.4 Installation Complete!\")\n",
    "    print(\"=\"*60)\n",
    "    print(\"\\n\ud83d\udd34 CRITICAL: YOU MUST RESTART THE KERNEL NOW! \ud83d\udd34\")\n",
    "    print(\"\\n   Steps:\")\n",
    "    print(\"   1. Click: Session \u2192 Restart Session (or Ctrl+O O)\")\n",
    "    print(\"   2. Run ALL cells again from Cell 1\")\n",
    "    print(\"   3. Training will work without crashes!\")\n",
    "    print(\"\\n\ud83d\udca1 Why? NumPy is already loaded in memory.\")\n",
    "    print(\"   Restarting clears memory and loads NumPy 1.26.4\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Stop execution here - user must restart\n",
    "    raise SystemExit(\"\\n\u26d4 STOP: Restart kernel now before continuing!\")\n",
    "else:\n",
    "    print(f\"\\n\u2713 NumPy 1.x already installed ({current_version})\")\n",
    "    print(\"\u2713 Training should work correctly!\")\n",
    "    print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91396242",
   "metadata": {},
   "source": [
    "## Step 2: Install Dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a5e355a",
   "metadata": {},
   "source": [
    "## Step 3: Setup Dataset\n",
    "\n",
    "**Choose ONE of the following methods:**\n",
    "\n",
    "### **METHOD 1: Kaggle Dataset (Recommended)**\n",
    "1. Go to: https://www.kaggle.com/datasets\n",
    "2. Click \"New Dataset\"\n",
    "3. Upload TrashCAN dataset ZIP\n",
    "4. Add to notebook: \"Add Data\" \u2192 Search for your dataset\n",
    "5. Update `DATASET_PATH` in the cell below\n",
    "\n",
    "### **METHOD 2: Google Drive (Alternative - Easiest)**\n",
    "1. Upload your TrashCAN dataset folder to Google Drive\n",
    "2. Share the folder/file publicly\n",
    "3. Get the file ID from the share link\n",
    "4. Update `GDRIVE_FILE_ID` in the cell below\n",
    "5. Set `USE_GDRIVE = True`\n",
    "\n",
    "### **METHOD 3: Direct Upload in Notebook**\n",
    "1. ZIP your dataset locally\n",
    "2. Upload ZIP to Kaggle notebook directly (< 500MB recommended)\n",
    "3. Set `USE_LOCAL_UPLOAD = True`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f7ea95e",
   "metadata": {},
   "source": [
    "## Step 4: Build Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5f4c728",
   "metadata": {},
   "source": [
    "## Step 5: Training Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ad57a9",
   "metadata": {},
   "source": [
    "## Step 6: Start Training\n",
    "\n",
    "**\u23f1\ufe0f Estimated Time**: ~10 hours for 100 epochs on T4 GPU\n",
    "\n",
    "**\ud83d\udca1 Tips**:\n",
    "- Training will save checkpoints automatically\n",
    "- You can monitor progress in real-time\n",
    "- Results saved to `/kaggle/working/runs/train/`\n",
    "- Download best checkpoint from Output folder after training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae4ee96f",
   "metadata": {},
   "source": [
    "## Step 7: Download Results\n",
    "\n",
    "After training completes, download the trained model checkpoint."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc4b0f0a",
   "metadata": {},
   "source": [
    "## \ud83c\udf89 Training Complete!\n",
    "\n",
    "### Next Steps:\n",
    "1. **Download Checkpoint**: Download `best.pt` from Output folder\n",
    "2. **Evaluate Model**: Run evaluation script locally with downloaded checkpoint\n",
    "3. **Test Detections**: Test on new images\n",
    "\n",
    "### Expected Results:\n",
    "- mAP@50:95: **70-72%** (22 classes)\n",
    "- Training Time: **~10 hours** (100 epochs)\n",
    "- Checkpoint Size: **~200-300 MB**\n",
    "\n",
    "---\n",
    "\n",
    "**\ud83d\udce7 Issues?** Check the GitHub repository: https://github.com/kshitijkhede/YOLO-UDD-v2.0"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}