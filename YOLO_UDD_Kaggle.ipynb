{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a82cd800",
   "metadata": {},
   "source": [
    "# üåä YOLO-UDD v2.0 - Kaggle Training\n",
    "\n",
    "**Simple 6-Step Training - No Crashes, No Loops!** ‚ö°\n",
    "\n",
    "## ‚ö†Ô∏è CRITICAL: NumPy Compatibility Issue\n",
    "**If Cell 1 shows NumPy 2.x detected:**\n",
    "1. Cell 1 will install NumPy 1.26.4 and STOP\n",
    "2. **YOU MUST RESTART THE KERNEL** (Session ‚Üí Restart Session)\n",
    "3. Then re-run ALL cells from Cell 1\n",
    "4. This is required because NumPy is already loaded in memory\n",
    "\n",
    "## üìã Before You Start:\n",
    "1. **Enable GPU**: Settings ‚Üí Accelerator ‚Üí **GPU T4 x2** ‚Üí Save\n",
    "2. **Dataset**: Google Drive link already configured (automatic download)\n",
    "3. **Run**: Execute cells 1-6 in order OR click \"Run All\"\n",
    "\n",
    "## ‚è±Ô∏è Training Info:\n",
    "- **Time**: ~10 hours (100 epochs)\n",
    "- **Expected mAP**: 70-72%\n",
    "- **No restarts needed!** ‚úÖ (after initial NumPy fix)\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8830adbe",
   "metadata": {},
   "source": [
    "## Cell 1: Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38942eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete environment setup\n",
    "import os\n",
    "import sys\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"üîß CELL 1: Environment Setup\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Check and fix NumPy version FIRST (before any other imports)\n",
    "print(\"\\n[Step 1/4] Checking NumPy version...\")\n",
    "try:\n",
    "    import numpy as np\n",
    "    numpy_ver = np.__version__\n",
    "    \n",
    "    if numpy_ver.startswith('2.'):\n",
    "        print(f\"  ‚ö†Ô∏è  NumPy {numpy_ver} detected (INCOMPATIBLE!)\")\n",
    "        print(\"  üîß Fixing NumPy version...\")\n",
    "        \n",
    "        # Force uninstall and reinstall\n",
    "        !pip uninstall -y numpy > /dev/null 2>&1\n",
    "        !pip install -q numpy==1.26.4\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"‚úÖ NumPy 1.26.4 Installed!\")\n",
    "        print(\"=\"*70)\n",
    "        print(\"\\nüî¥ CRITICAL: RESTART KERNEL NOW!\")\n",
    "        print(\"\\n   Steps:\")\n",
    "        print(\"   1. Click: Session ‚Üí Restart Session\")\n",
    "        print(\"   2. Or use: Runtime ‚Üí Restart Runtime\")\n",
    "        print(\"   3. Then re-run ALL cells from Cell 1\")\n",
    "        print(\"\\nüí° Why? NumPy is already loaded in memory.\")\n",
    "        print(\"   Kernel restart loads the new NumPy 1.26.4\")\n",
    "        print(\"\\n‚õî DO NOT continue without restarting!\")\n",
    "        print(\"=\"*70)\n",
    "        raise SystemExit(\"‚õî STOP: Restart kernel before continuing!\")\n",
    "    else:\n",
    "        print(f\"  ‚úÖ NumPy {numpy_ver} - Compatible!\")\n",
    "except SystemExit:\n",
    "    raise  # Re-raise to stop execution\n",
    "except Exception as e:\n",
    "    print(f\"  ‚ö†Ô∏è  Error: {e}\")\n",
    "    print(\"  üì¶ Installing NumPy 1.26.4...\")\n",
    "    !pip install -q numpy==1.26.4\n",
    "    print(\"\\n‚ö†Ô∏è  RESTART KERNEL after this cell completes!\")\n",
    "    raise SystemExit(\"‚õî Restart kernel to load NumPy 1.26.4\")\n",
    "\n",
    "# Setup directories\n",
    "print(\"\\n[Step 2/4] Setting up directories...\")\n",
    "WORK_DIR = '/kaggle/working'\n",
    "REPO_DIR = f'{WORK_DIR}/YOLO-UDD-v2.0'\n",
    "\n",
    "os.chdir(WORK_DIR)\n",
    "print(f\"  ‚úÖ Working directory: {WORK_DIR}\")\n",
    "\n",
    "# Clone repository\n",
    "print(\"\\n[Step 3/4] Cloning repository...\")\n",
    "if os.path.exists(REPO_DIR):\n",
    "    import shutil\n",
    "    shutil.rmtree(REPO_DIR)\n",
    "    print(f\"  üóëÔ∏è  Removed old repository\")\n",
    "\n",
    "!git clone -q https://github.com/kshitijkhede/YOLO-UDD-v2.0.git\n",
    "\n",
    "if os.path.exists(REPO_DIR):\n",
    "    os.chdir(REPO_DIR)\n",
    "    if REPO_DIR not in sys.path:\n",
    "        sys.path.insert(0, REPO_DIR)\n",
    "    print(f\"  ‚úÖ Repository cloned: {REPO_DIR}\")\n",
    "else:\n",
    "    raise Exception(\"‚ùå Clone failed! Check GitHub URL\")\n",
    "\n",
    "# Verify NumPy one more time\n",
    "print(\"\\n[Step 4/4] Final NumPy verification...\")\n",
    "import numpy as np\n",
    "print(f\"  ‚úÖ NumPy {np.__version__} loaded in kernel\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"‚úÖ Cell 1 Complete - Environment Ready!\")\n",
    "print(\"=\"*70)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d42fbc1c",
   "metadata": {},
   "source": [
    "## Cell 2: Verify & Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d42523a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify setup and install dependencies\n",
    "import os\n",
    "import torch\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"üì¶ CELL 2: Verification & Dependencies\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Verify repository structure\n",
    "print(\"\\n[Step 1/3] Verifying repository...\")\n",
    "required = ['models/', 'scripts/', 'utils/', 'configs/', 'scripts/train.py']\n",
    "all_ok = True\n",
    "for item in required:\n",
    "    if os.path.exists(item):\n",
    "        print(f\"  ‚úÖ {item}\")\n",
    "    else:\n",
    "        print(f\"  ‚ùå {item} MISSING\")\n",
    "        all_ok = False\n",
    "\n",
    "if not all_ok:\n",
    "    raise Exception(\"Repository incomplete! Re-run Cell 1\")\n",
    "\n",
    "# Check GPU\n",
    "print(\"\\n[Step 2/3] Checking GPU...\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"  ‚úÖ GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"  ‚úÖ Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
    "else:\n",
    "    print(\"  ‚ùå NO GPU! Enable: Settings ‚Üí GPU T4 x2\")\n",
    "    raise RuntimeError(\"GPU required!\")\n",
    "\n",
    "# Install dependencies\n",
    "print(\"\\n[Step 3/3] Installing dependencies (this takes ~2 min)...\")\n",
    "!pip install -q torch>=2.0.0 torchvision>=0.15.0 albumentations>=1.3.0 \\\n",
    "    opencv-python-headless>=4.7.0 pycocotools>=2.0.6 tensorboard>=2.12.0 \\\n",
    "    tqdm pyyaml scikit-learn matplotlib seaborn\n",
    "\n",
    "print(\"  ‚úÖ Dependencies installed\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"‚úÖ Cell 2 Complete - System Ready!\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86fc8028",
   "metadata": {},
   "source": [
    "## Cell 3: Setup Dataset\n",
    "\n",
    "**Dataset will download automatically from Google Drive (~170 MB, takes 2-3 min)**\n",
    "\n",
    "Alternative: Upload your own dataset to Kaggle and set `USE_KAGGLE_DATASET = True`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b863aad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset setup\n",
    "import os\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"üì¶ CELL 3: Dataset Setup\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# ============================================\n",
    "# CONFIGURATION\n",
    "# ============================================\n",
    "USE_KAGGLE_DATASET = False  # Set True if you added dataset to Kaggle\n",
    "KAGGLE_DATASET_PATH = '/kaggle/input/trashcan-dataset'\n",
    "\n",
    "USE_GDRIVE = True  # ‚úÖ Default: Download from Google Drive\n",
    "GDRIVE_FILE_ID = '17oRYriPgBnW9zowwmhImxdUpmHwOjgIp'  # Your uploaded dataset\n",
    "# ============================================\n",
    "\n",
    "DATASET_PATH = None\n",
    "\n",
    "if USE_KAGGLE_DATASET:\n",
    "    print(\"\\nüìÇ Using Kaggle Dataset...\")\n",
    "    if os.path.exists(KAGGLE_DATASET_PATH):\n",
    "        if os.path.isfile(KAGGLE_DATASET_PATH):\n",
    "            print(\"  üì¶ Extracting...\")\n",
    "            !unzip -o -q {KAGGLE_DATASET_PATH} -d /kaggle/working/\n",
    "            DATASET_PATH = '/kaggle/working/trashcan'\n",
    "        else:\n",
    "            trashcan = os.path.join(KAGGLE_DATASET_PATH, 'trashcan')\n",
    "            DATASET_PATH = trashcan if os.path.exists(trashcan) else KAGGLE_DATASET_PATH\n",
    "        print(f\"  ‚úÖ Dataset: {DATASET_PATH}\")\n",
    "    else:\n",
    "        print(f\"  ‚ùå NOT FOUND: {KAGGLE_DATASET_PATH}\")\n",
    "\n",
    "elif USE_GDRIVE:\n",
    "    print(\"\\n‚òÅÔ∏è  Downloading from Google Drive...\")\n",
    "    print(\"  üì¶ Installing gdown...\")\n",
    "    !pip install -q gdown\n",
    "    \n",
    "    print(\"  ‚¨áÔ∏è  Downloading dataset (~170 MB, 2-3 min)...\")\n",
    "    !gdown --id {GDRIVE_FILE_ID} -O /kaggle/working/trashcan.zip --quiet\n",
    "    \n",
    "    if os.path.exists('/kaggle/working/trashcan.zip'):\n",
    "        size = os.path.getsize('/kaggle/working/trashcan.zip') / 1024 / 1024\n",
    "        print(f\"  ‚úÖ Downloaded: {size:.1f} MB\")\n",
    "        \n",
    "        print(\"  üì¶ Extracting (auto-overwrite enabled)...\")\n",
    "        !unzip -o -q /kaggle/working/trashcan.zip -d /kaggle/working/\n",
    "        \n",
    "        if os.path.exists('/kaggle/working/trashcan'):\n",
    "            DATASET_PATH = '/kaggle/working/trashcan'\n",
    "            print(f\"  ‚úÖ Dataset: {DATASET_PATH}\")\n",
    "        else:\n",
    "            print(\"  ‚ùå Extraction failed\")\n",
    "    else:\n",
    "        print(\"  ‚ùå Download failed\")\n",
    "else:\n",
    "    print(\"\\n‚ùå No method selected! Set USE_KAGGLE_DATASET or USE_GDRIVE = True\")\n",
    "\n",
    "# Verify dataset\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "if DATASET_PATH and os.path.exists(DATASET_PATH):\n",
    "    print(f\"‚úÖ DATASET READY: {DATASET_PATH}\")\n",
    "    \n",
    "    # Count images\n",
    "    train_imgs = len([f for f in os.listdir(f\"{DATASET_PATH}/images/train\") if f.endswith(('.jpg', '.png'))])\n",
    "    val_imgs = len([f for f in os.listdir(f\"{DATASET_PATH}/images/val\") if f.endswith(('.jpg', '.png'))])\n",
    "    \n",
    "    print(f\"  üìä Training images: {train_imgs:,}\")\n",
    "    print(f\"  üìä Validation images: {val_imgs:,}\")\n",
    "    \n",
    "    # Check annotations\n",
    "    train_ann = f\"{DATASET_PATH}/instances_train_trashcan.json\"\n",
    "    val_ann = f\"{DATASET_PATH}/instances_val_trashcan.json\"\n",
    "    \n",
    "    if os.path.exists(train_ann) and os.path.exists(val_ann):\n",
    "        print(f\"  ‚úÖ Annotations found\")\n",
    "    else:\n",
    "        print(f\"  ‚ùå Annotations missing!\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"‚úÖ Cell 3 Complete - Dataset Ready!\")\n",
    "    print(\"=\"*70)\n",
    "else:\n",
    "    print(\"‚ùå DATASET NOT READY!\")\n",
    "    raise Exception(\"Dataset setup failed!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17f5b365",
   "metadata": {},
   "source": [
    "## Cell 4: Build & Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a49ee2e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build and test YOLO-UDD model\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"üèóÔ∏è  CELL 4: Build Model\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Ensure correct paths\n",
    "REPO_DIR = '/kaggle/working/YOLO-UDD-v2.0'\n",
    "os.chdir(REPO_DIR)\n",
    "if REPO_DIR not in sys.path:\n",
    "    sys.path.insert(0, REPO_DIR)\n",
    "\n",
    "print(\"\\n[Step 1/2] Building model...\")\n",
    "from models.yolo_udd import build_yolo_udd\n",
    "\n",
    "model = build_yolo_udd(num_classes=22)  # TrashCAN has 22 classes\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"  ‚úÖ Model: YOLO-UDD v2.0\")\n",
    "print(f\"  ‚úÖ Classes: 22\")\n",
    "print(f\"  ‚úÖ Device: {device}\")\n",
    "print(f\"  ‚úÖ Parameters: {total_params:,}\")\n",
    "\n",
    "# Test forward pass\n",
    "print(\"\\n[Step 2/2] Testing model...\")\n",
    "x = torch.randn(1, 3, 640, 640).to(device)\n",
    "with torch.no_grad():\n",
    "    predictions, turb_score = model(x)\n",
    "\n",
    "print(f\"  ‚úÖ Forward pass successful\")\n",
    "print(f\"  ‚úÖ Turbidity score: {turb_score.item():.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"‚úÖ Cell 4 Complete - Model Ready!\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c24cc93",
   "metadata": {},
   "source": [
    "## Cell 5: Start Training üöÄ\n",
    "\n",
    "**This will take ~10 hours for 100 epochs on GPU T4 x2**\n",
    "\n",
    "The training will:\n",
    "- Save checkpoints every 5 epochs\n",
    "- Display training progress with loss/mAP metrics\n",
    "- Auto-save best model based on validation mAP\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e82fa8c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start training\n",
    "import subprocess\n",
    "import sys\n",
    "import os\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"üöÄ CELL 5: Start Training\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Training parameters\n",
    "EPOCHS = 100\n",
    "BATCH_SIZE = 8\n",
    "IMG_SIZE = 640\n",
    "LEARNING_RATE = 0.001\n",
    "SAVE_DIR = '/kaggle/working/checkpoints'\n",
    "DATASET_PATH = '/kaggle/working/trashcan'\n",
    "\n",
    "# Verify dataset exists\n",
    "if not os.path.exists(DATASET_PATH):\n",
    "    print(f\"‚ùå ERROR: Dataset not found at {DATASET_PATH}\")\n",
    "    print(\"Please run Cell 3 to download the dataset!\")\n",
    "    raise FileNotFoundError(f\"Dataset not found: {DATASET_PATH}\")\n",
    "\n",
    "print(f\"\\nüìã Training Configuration:\")\n",
    "print(f\"  Epochs:        {EPOCHS}\")\n",
    "print(f\"  Batch Size:    {BATCH_SIZE}\")\n",
    "print(f\"  Image Size:    {IMG_SIZE}\")\n",
    "print(f\"  Learning Rate: {LEARNING_RATE}\")\n",
    "print(f\"  Dataset:       {DATASET_PATH}\")\n",
    "print(f\"  Save Dir:      {SAVE_DIR}\")\n",
    "\n",
    "# Verify annotations exist\n",
    "train_ann = os.path.join(DATASET_PATH, 'instances_train_trashcan.json')\n",
    "val_ann = os.path.join(DATASET_PATH, 'instances_val_trashcan.json')\n",
    "print(f\"\\nüîç Checking annotations:\")\n",
    "print(f\"  instances_train_trashcan.json: {'‚úÖ Found' if os.path.exists(train_ann) else '‚ùå NOT FOUND'}\")\n",
    "print(f\"  instances_val_trashcan.json:   {'‚úÖ Found' if os.path.exists(val_ann) else '‚ùå NOT FOUND'}\")\n",
    "\n",
    "if not os.path.exists(train_ann) or not os.path.exists(val_ann):\n",
    "    print(\"\\n‚ùå ERROR: Annotation files missing!\")\n",
    "    print(\"Please run Cell 3 to download the dataset!\")\n",
    "    raise FileNotFoundError(\"Annotations not found\")\n",
    "\n",
    "# Build training command\n",
    "cmd = [\n",
    "    sys.executable, 'scripts/train.py',\n",
    "    '--config', 'configs/train_config.yaml',\n",
    "    '--img-size', str(IMG_SIZE),\n",
    "    '--batch-size', str(BATCH_SIZE),\n",
    "    '--epochs', str(EPOCHS),\n",
    "    '--num-workers', '4',\n",
    "    '--device', 'cuda'\n",
    "]\n",
    "\n",
    "print(\"\\nüöÄ Starting training...\")\n",
    "print(f\"Command: {' '.join(cmd)}\")\n",
    "print(f\"‚è±Ô∏è  Estimated time: ~10 hours for {EPOCHS} epochs\")\n",
    "print(f\"üí° Training progress will appear below...\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "# Run training\n",
    "result = subprocess.run(cmd)\n",
    "\n",
    "if result.returncode == 0:\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"‚úÖ Training completed successfully!\")\n",
    "    print(f\"üìÅ Results saved to: {SAVE_DIR}\")\n",
    "    print(\"üì• Download checkpoints from Output section ‚Üí\")\n",
    "    print(\"=\"*70)\n",
    "else:\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"‚ùå Training failed - check error messages above\")\n",
    "    print(\"=\"*70)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c74b7f0c",
   "metadata": {},
   "source": [
    "## Cell 6: View Results & Download Model\n",
    "\n",
    "After training completes, run this cell to:\n",
    "- View training statistics\n",
    "- Locate checkpoint files\n",
    "- Get download instructions for trained model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f06a0c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check training results and download model\n",
    "import os\n",
    "from IPython.display import FileLink\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"üìä CELL 6: Training Results\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "SAVE_DIR = '/kaggle/working/checkpoints'\n",
    "\n",
    "if os.path.exists(SAVE_DIR):\n",
    "    print(f\"\\n‚úÖ Results found: {SAVE_DIR}\\n\")\n",
    "    \n",
    "    # List all checkpoints\n",
    "    checkpoints = [f for f in os.listdir(SAVE_DIR) if f.endswith('.pth')]\n",
    "    \n",
    "    if checkpoints:\n",
    "        print(f\"üì¶ Found {len(checkpoints)} checkpoint(s):\\n\")\n",
    "        for ckpt in sorted(checkpoints):\n",
    "            ckpt_path = os.path.join(SAVE_DIR, ckpt)\n",
    "            size = os.path.getsize(ckpt_path) / (1024*1024)\n",
    "            print(f\"  ‚Ä¢ {ckpt} ({size:.1f} MB)\")\n",
    "        \n",
    "        # Check for best model\n",
    "        best_model = os.path.join(SAVE_DIR, 'best_model.pth')\n",
    "        if os.path.exists(best_model):\n",
    "            size = os.path.getsize(best_model) / (1024*1024)\n",
    "            print(\"\\n\" + \"=\"*70)\n",
    "            print(\"üèÜ TRAINING COMPLETE!\")\n",
    "            print(\"=\"*70)\n",
    "            print(f\"\\n‚úÖ Best Model: best_model.pth\")\n",
    "            print(f\"üì¶ Size: {size:.1f} MB\")\n",
    "            print(f\"\\nüì• Download your trained model:\")\n",
    "            print(f\"   1. Click 'Output' in right sidebar\")\n",
    "            print(f\"   2. Look for '{SAVE_DIR}'\")\n",
    "            print(f\"   3. Download 'best_model.pth'\")\n",
    "            print(f\"\\nüéØ Expected Performance: 70-72% mAP@0.5\")\n",
    "            print(f\"üéâ Success! Model ready for deployment!\")\n",
    "            print(\"=\"*70)\n",
    "        else:\n",
    "            print(\"\\n‚ö†Ô∏è  best_model.pth not found\")\n",
    "            print(\"Training may still be running or incomplete\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è  No checkpoint files found\")\n",
    "        print(\"Training may have failed or not started yet\")\n",
    "else:\n",
    "    print(f\"\\n‚ùå Results directory not found: {SAVE_DIR}\")\n",
    "    print(\"Training has not been run yet or failed to create checkpoints\")\n",
    "    print(\"\\nTo train the model:\")\n",
    "    print(\"  1. Make sure Cell 5 completed without errors\")\n",
    "    print(\"  2. Check GPU is enabled (Settings ‚Üí GPU T4 x2)\")\n",
    "    print(\"  3. Re-run Cell 5 to start training\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9105259f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify repository structure\n",
    "import os\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"üìÇ Repository Structure\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "required_dirs = ['models', 'scripts', 'data', 'utils', 'configs']\n",
    "required_files = ['requirements.txt', 'models/__init__.py', 'scripts/train.py']\n",
    "\n",
    "for dir_name in required_dirs:\n",
    "    status = \"‚úì\" if os.path.exists(dir_name) else \"‚úó\"\n",
    "    print(f\"{status} {dir_name}/\")\n",
    "\n",
    "print()\n",
    "for file_name in required_files:\n",
    "    status = \"‚úì\" if os.path.exists(file_name) else \"‚úó\"\n",
    "    print(f\"{status} {file_name}\")\n",
    "\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "314a5f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIX: Force add models to Python path\n",
    "import os\n",
    "import sys\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"üîß Fixing Module Import Path\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Get current directory\n",
    "current_dir = os.getcwd()\n",
    "print(f\"\\nCurrent directory: {current_dir}\")\n",
    "\n",
    "# Check if we're in the repo directory\n",
    "if 'YOLO-UDD-v2.0' not in current_dir:\n",
    "    print(\"\\n‚ö†Ô∏è  Not in YOLO-UDD-v2.0 directory!\")\n",
    "    \n",
    "    # Try to find and change to it\n",
    "    possible_paths = [\n",
    "        '/kaggle/working/YOLO-UDD-v2.0',\n",
    "        '/kaggle/YOLO-UDD-v2.0',\n",
    "        os.path.join(os.getcwd(), 'YOLO-UDD-v2.0')\n",
    "    ]\n",
    "    \n",
    "    for path in possible_paths:\n",
    "        if os.path.exists(path):\n",
    "            os.chdir(path)\n",
    "            current_dir = os.getcwd()\n",
    "            print(f\"‚úì Changed to: {current_dir}\")\n",
    "            break\n",
    "    else:\n",
    "        print(\"‚úó Could not find YOLO-UDD-v2.0 directory!\")\n",
    "        print(\"  Please re-run the clone cell (Cell 3)\")\n",
    "\n",
    "# Ensure repo is in Python path\n",
    "if current_dir not in sys.path:\n",
    "    sys.path.insert(0, current_dir)\n",
    "    print(f\"‚úì Added to sys.path: {current_dir}\")\n",
    "\n",
    "# Verify models can be imported\n",
    "print(\"\\nüîç Verifying module availability...\")\n",
    "models_path = os.path.join(current_dir, 'models')\n",
    "if os.path.exists(models_path):\n",
    "    print(f\"  ‚úì models/ exists at: {models_path}\")\n",
    "    \n",
    "    # Check for required files\n",
    "    required_files = ['__init__.py', 'yolo_udd.py', 'psem.py', 'sdwh.py', 'tafm.py']\n",
    "    all_present = True\n",
    "    for file in required_files:\n",
    "        file_path = os.path.join(models_path, file)\n",
    "        if os.path.exists(file_path):\n",
    "            print(f\"  ‚úì {file}\")\n",
    "        else:\n",
    "            print(f\"  ‚úó {file} MISSING!\")\n",
    "            all_present = False\n",
    "    \n",
    "    if all_present:\n",
    "        print(\"\\n‚úÖ All model files present - import should work!\")\n",
    "    else:\n",
    "        print(\"\\n‚ùå Some files missing - clone may be incomplete\")\n",
    "        print(\"   ‚Üí Re-run Cell 3 (Clone Repository)\")\n",
    "else:\n",
    "    print(f\"  ‚úó models/ NOT FOUND at: {models_path}\")\n",
    "    print(\"\\n  Available directories:\")\n",
    "    for item in os.listdir(current_dir):\n",
    "        if os.path.isdir(os.path.join(current_dir, item)):\n",
    "            print(f\"    üìÅ {item}/\")\n",
    "    print(\"\\n‚ùå Repository clone failed!\")\n",
    "    print(\"   ‚Üí Re-run Cell 3 (Clone Repository)\")\n",
    "\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12beceaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CRITICAL FIX: Force NumPy 1.x Installation\n",
    "# ============================================================\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"üîß FIXING NumPy Compatibility Issue\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Check current NumPy version\n",
    "import numpy as np\n",
    "current_version = np.__version__\n",
    "print(f\"\\nüìå Current NumPy version: {current_version}\")\n",
    "\n",
    "if current_version.startswith('2.'):\n",
    "    print(\"\\n‚ö†Ô∏è  NumPy 2.x detected - this WILL crash TensorFlow/scikit-learn!\")\n",
    "    print(\"Forcing downgrade to NumPy 1.x...\\n\")\n",
    "    \n",
    "    # Force uninstall NumPy 2.x\n",
    "    import sys\n",
    "    !{sys.executable} -m pip uninstall -y numpy\n",
    "    \n",
    "    # Install NumPy 1.x with force reinstall\n",
    "    !{sys.executable} -m pip install 'numpy==1.26.4' --force-reinstall --no-cache-dir\n",
    "    \n",
    "    # Verify the fix worked\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"‚úÖ NumPy 1.26.4 Installation Complete!\")\n",
    "    print(\"=\"*60)\n",
    "    print(\"\\nüî¥ CRITICAL: YOU MUST RESTART THE KERNEL NOW! üî¥\")\n",
    "    print(\"\\n   Steps:\")\n",
    "    print(\"   1. Click: Session ‚Üí Restart Session (or Ctrl+O O)\")\n",
    "    print(\"   2. Run ALL cells again from Cell 1\")\n",
    "    print(\"   3. Training will work without crashes!\")\n",
    "    print(\"\\nüí° Why? NumPy is already loaded in memory.\")\n",
    "    print(\"   Restarting clears memory and loads NumPy 1.26.4\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Stop execution here - user must restart\n",
    "    raise SystemExit(\"\\n‚õî STOP: Restart kernel now before continuing!\")\n",
    "else:\n",
    "    print(f\"\\n‚úì NumPy 1.x already installed ({current_version})\")\n",
    "    print(\"‚úì Training should work correctly!\")\n",
    "    print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91396242",
   "metadata": {},
   "source": [
    "## Step 2: Install Dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a5e355a",
   "metadata": {},
   "source": [
    "## Step 3: Setup Dataset\n",
    "\n",
    "**Choose ONE of the following methods:**\n",
    "\n",
    "### **METHOD 1: Kaggle Dataset (Recommended)**\n",
    "1. Go to: https://www.kaggle.com/datasets\n",
    "2. Click \"New Dataset\"\n",
    "3. Upload TrashCAN dataset ZIP\n",
    "4. Add to notebook: \"Add Data\" ‚Üí Search for your dataset\n",
    "5. Update `DATASET_PATH` in the cell below\n",
    "\n",
    "### **METHOD 2: Google Drive (Alternative - Easiest)**\n",
    "1. Upload your TrashCAN dataset folder to Google Drive\n",
    "2. Share the folder/file publicly\n",
    "3. Get the file ID from the share link\n",
    "4. Update `GDRIVE_FILE_ID` in the cell below\n",
    "5. Set `USE_GDRIVE = True`\n",
    "\n",
    "### **METHOD 3: Direct Upload in Notebook**\n",
    "1. ZIP your dataset locally\n",
    "2. Upload ZIP to Kaggle notebook directly (< 500MB recommended)\n",
    "3. Set `USE_LOCAL_UPLOAD = True`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f7ea95e",
   "metadata": {},
   "source": [
    "## Step 4: Build Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5f4c728",
   "metadata": {},
   "source": [
    "## Step 5: Training Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ad57a9",
   "metadata": {},
   "source": [
    "## Step 6: Start Training\n",
    "\n",
    "**‚è±Ô∏è Estimated Time**: ~10 hours for 100 epochs on T4 GPU\n",
    "\n",
    "**üí° Tips**:\n",
    "- Training will save checkpoints automatically\n",
    "- You can monitor progress in real-time\n",
    "- Results saved to `/kaggle/working/runs/train/`\n",
    "- Download best checkpoint from Output folder after training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae4ee96f",
   "metadata": {},
   "source": [
    "## Step 7: Download Results\n",
    "\n",
    "After training completes, download the trained model checkpoint."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc4b0f0a",
   "metadata": {},
   "source": [
    "## üéâ Training Complete!\n",
    "\n",
    "### Next Steps:\n",
    "1. **Download Checkpoint**: Download `best.pt` from Output folder\n",
    "2. **Evaluate Model**: Run evaluation script locally with downloaded checkpoint\n",
    "3. **Test Detections**: Test on new images\n",
    "\n",
    "### Expected Results:\n",
    "- mAP@50:95: **70-72%** (22 classes)\n",
    "- Training Time: **~10 hours** (100 epochs)\n",
    "- Checkpoint Size: **~200-300 MB**\n",
    "\n",
    "---\n",
    "\n",
    "**üìß Issues?** Check the GitHub repository: https://github.com/kshitijkhede/YOLO-UDD-v2.0"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
