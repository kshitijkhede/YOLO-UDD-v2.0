{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸŒŠ YOLO-UDD v2.0 - Underwater Debris Detection (KAGGLE)\n",
    "\n",
    "**Complete Training Pipeline on Kaggle with GPU** âš¡\n",
    "\n",
    "## ðŸš€ Quick Start:\n",
    "1. **Upload Dataset**: Add TrashCAN dataset as Kaggle Dataset\n",
    "2. **Enable GPU**: Settings â†’ Accelerator â†’ GPU T4 x2 â†’ Save\n",
    "3. **Run All**: Run all cells sequentially\n",
    "4. **Download Results**: Download trained model from Output folder\n",
    "\n",
    "## âš™ï¸ Configuration:\n",
    "- **Epochs**: 100 (reduced for faster training ~10 hours)\n",
    "- **Batch Size**: 8\n",
    "- **Classes**: 22 (matches TrashCAN dataset)\n",
    "- **Expected mAP**: 70-72%\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Setup Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone repository\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Kaggle uses /kaggle/working directory\n",
    "WORK_DIR = '/kaggle/working'\n",
    "REPO_DIR = f'{WORK_DIR}/YOLO-UDD-v2.0'\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"Step 1: Cloning Repository\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Ensure we're in working directory\n",
    "try:\n",
    "    os.chdir(WORK_DIR)\n",
    "    print(f\"âœ“ Changed to working directory: {os.getcwd()}\")\n",
    "except Exception as e:\n",
    "    print(f\"âœ— Error changing directory: {e}\")\n",
    "    raise\n",
    "\n",
    "# Remove existing directory if present\n",
    "if os.path.exists(REPO_DIR):\n",
    "    import shutil\n",
    "    shutil.rmtree(REPO_DIR)\n",
    "    print(\"âœ“ Cleaned existing directory\")\n",
    "\n",
    "# Clone repository\n",
    "print(\"\\nCloning repository from GitHub...\")\n",
    "!git clone https://github.com/kshitijkhede/YOLO-UDD-v2.0.git\n",
    "\n",
    "# Verify clone succeeded\n",
    "if not os.path.exists(REPO_DIR):\n",
    "    print(f\"\\nâœ— ERROR: Repository not cloned!\")\n",
    "    print(f\"   Expected location: {REPO_DIR}\")\n",
    "    raise FileNotFoundError(\"Failed to clone repository. Please check internet connection and repository URL.\")\n",
    "\n",
    "# Change to repo directory\n",
    "try:\n",
    "    os.chdir(REPO_DIR)\n",
    "    print(f\"\\nâœ“ Changed to repository directory: {os.getcwd()}\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nâœ— Error changing to repo directory: {e}\")\n",
    "    raise\n",
    "\n",
    "# Add to Python path\n",
    "if REPO_DIR not in sys.path:\n",
    "    sys.path.insert(0, REPO_DIR)\n",
    "    print(f\"âœ“ Added to Python path: {REPO_DIR}\")\n",
    "\n",
    "# Verify we're in the right place\n",
    "print(f\"\\nâœ“ Current directory: {os.getcwd()}\")\n",
    "print(f\"âœ“ Python path includes: {REPO_DIR}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify repository structure\n",
    "import os\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"ðŸ“‚ Repository Structure\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "required_dirs = ['models', 'scripts', 'data', 'utils', 'configs']\n",
    "required_files = ['requirements.txt', 'models/__init__.py', 'scripts/train.py']\n",
    "\n",
    "for dir_name in required_dirs:\n",
    "    status = \"âœ“\" if os.path.exists(dir_name) else \"âœ—\"\n",
    "    print(f\"{status} {dir_name}/\")\n",
    "\n",
    "print()\n",
    "for file_name in required_files:\n",
    "    status = \"âœ“\" if os.path.exists(file_name) else \"âœ—\"\n",
    "    print(f\"{status} {file_name}\")\n",
    "\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify Python can find modules\n",
    "import os\n",
    "import sys\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"ðŸ” Module Import Diagnostics\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\\nCurrent working directory:\")\n",
    "print(f\"  {os.getcwd()}\")\n",
    "\n",
    "print(f\"\\nPython sys.path (first 3 entries):\")\n",
    "for i, path in enumerate(sys.path[:3]):\n",
    "    print(f\"  {i+1}. {path}\")\n",
    "\n",
    "print(f\"\\nChecking for models module:\")\n",
    "models_path = os.path.join(os.getcwd(), 'models')\n",
    "if os.path.exists(models_path):\n",
    "    print(f\"  âœ“ models/ directory exists at: {models_path}\")\n",
    "    if os.path.exists(os.path.join(models_path, '__init__.py')):\n",
    "        print(f\"  âœ“ models/__init__.py exists\")\n",
    "    if os.path.exists(os.path.join(models_path, 'yolo_udd.py')):\n",
    "        print(f\"  âœ“ models/yolo_udd.py exists\")\n",
    "else:\n",
    "    print(f\"  âœ— models/ directory NOT FOUND!\")\n",
    "    print(f\"  âœ— Expected at: {models_path}\")\n",
    "    print(f\"\\n  Available directories:\")\n",
    "    for item in os.listdir(os.getcwd()):\n",
    "        if os.path.isdir(item):\n",
    "            print(f\"    ðŸ“ {item}/\")\n",
    "\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIX: Force add models to Python path\n",
    "import os\n",
    "import sys\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"ðŸ”§ Fixing Module Import Path\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Get current directory\n",
    "current_dir = os.getcwd()\n",
    "print(f\"\\nCurrent directory: {current_dir}\")\n",
    "\n",
    "# Check if we're in the repo directory\n",
    "if 'YOLO-UDD-v2.0' not in current_dir:\n",
    "    print(\"\\nâš ï¸  Not in YOLO-UDD-v2.0 directory!\")\n",
    "    \n",
    "    # Try to find and change to it\n",
    "    possible_paths = [\n",
    "        '/kaggle/working/YOLO-UDD-v2.0',\n",
    "        '/kaggle/YOLO-UDD-v2.0',\n",
    "        os.path.join(os.getcwd(), 'YOLO-UDD-v2.0')\n",
    "    ]\n",
    "    \n",
    "    for path in possible_paths:\n",
    "        if os.path.exists(path):\n",
    "            os.chdir(path)\n",
    "            current_dir = os.getcwd()\n",
    "            print(f\"âœ“ Changed to: {current_dir}\")\n",
    "            break\n",
    "    else:\n",
    "        print(\"âœ— Could not find YOLO-UDD-v2.0 directory!\")\n",
    "        print(\"  Please re-run the clone cell (Cell 3)\")\n",
    "\n",
    "# Ensure repo is in Python path\n",
    "if current_dir not in sys.path:\n",
    "    sys.path.insert(0, current_dir)\n",
    "    print(f\"âœ“ Added to sys.path: {current_dir}\")\n",
    "\n",
    "# Verify models can be imported\n",
    "print(\"\\nðŸ” Verifying module availability...\")\n",
    "models_path = os.path.join(current_dir, 'models')\n",
    "if os.path.exists(models_path):\n",
    "    print(f\"  âœ“ models/ exists at: {models_path}\")\n",
    "    \n",
    "    # Check for required files\n",
    "    required_files = ['__init__.py', 'yolo_udd.py', 'psem.py', 'sdwh.py', 'tafm.py']\n",
    "    all_present = True\n",
    "    for file in required_files:\n",
    "        file_path = os.path.join(models_path, file)\n",
    "        if os.path.exists(file_path):\n",
    "            print(f\"  âœ“ {file}\")\n",
    "        else:\n",
    "            print(f\"  âœ— {file} MISSING!\")\n",
    "            all_present = False\n",
    "    \n",
    "    if all_present:\n",
    "        print(\"\\nâœ… All model files present - import should work!\")\n",
    "    else:\n",
    "        print(\"\\nâŒ Some files missing - clone may be incomplete\")\n",
    "        print(\"   â†’ Re-run Cell 3 (Clone Repository)\")\n",
    "else:\n",
    "    print(f\"  âœ— models/ NOT FOUND at: {models_path}\")\n",
    "    print(\"\\n  Available directories:\")\n",
    "    for item in os.listdir(current_dir):\n",
    "        if os.path.isdir(os.path.join(current_dir, item)):\n",
    "            print(f\"    ðŸ“ {item}/\")\n",
    "    print(\"\\nâŒ Repository clone failed!\")\n",
    "    print(\"   â†’ Re-run Cell 3 (Clone Repository)\")\n",
    "\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CRITICAL FIX: NumPy Compatibility\n",
    "\n",
    "**âš ï¸ IMPORTANT**: Kaggle has NumPy 2.x by default, but TensorFlow/scikit-learn require NumPy 1.x.\n",
    "This fix prevents training crashes!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CRITICAL FIX: Force NumPy 1.x Installation\n",
    "# ============================================================\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"ðŸ”§ FIXING NumPy Compatibility Issue\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Check current NumPy version\n",
    "import numpy as np\n",
    "current_version = np.__version__\n",
    "print(f\"\\nðŸ“Œ Current NumPy version: {current_version}\")\n",
    "\n",
    "if current_version.startswith('2.'):\n",
    "    print(\"\\nâš ï¸  NumPy 2.x detected - this WILL crash TensorFlow/scikit-learn!\")\n",
    "    print(\"Forcing downgrade to NumPy 1.x...\\n\")\n",
    "    \n",
    "    # Force uninstall NumPy 2.x\n",
    "    import sys\n",
    "    !{sys.executable} -m pip uninstall -y numpy\n",
    "    \n",
    "    # Install NumPy 1.x with force reinstall\n",
    "    !{sys.executable} -m pip install 'numpy==1.26.4' --force-reinstall --no-cache-dir\n",
    "    \n",
    "    # Verify the fix worked\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"âœ… NumPy 1.26.4 Installation Complete!\")\n",
    "    print(\"=\"*60)\n",
    "    print(\"\\nðŸ”´ CRITICAL: YOU MUST RESTART THE KERNEL NOW! ðŸ”´\")\n",
    "    print(\"\\n   Steps:\")\n",
    "    print(\"   1. Click: Session â†’ Restart Session (or Ctrl+O O)\")\n",
    "    print(\"   2. Run ALL cells again from Cell 1\")\n",
    "    print(\"   3. Training will work without crashes!\")\n",
    "    print(\"\\nðŸ’¡ Why? NumPy is already loaded in memory.\")\n",
    "    print(\"   Restarting clears memory and loads NumPy 1.26.4\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Stop execution here - user must restart\n",
    "    raise SystemExit(\"\\nâ›” STOP: Restart kernel now before continuing!\")\n",
    "else:\n",
    "    print(f\"\\nâœ“ NumPy 1.x already installed ({current_version})\")\n",
    "    print(\"âœ“ Training should work correctly!\")\n",
    "    print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU availability\n",
    "import torch\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"ðŸ”¥ GPU Status Check\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"âœ“ GPU Available: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"âœ“ GPU Count: {torch.cuda.device_count()}\")\n",
    "    print(f\"âœ“ CUDA Version: {torch.version.cuda}\")\n",
    "    print(f\"âœ“ PyTorch Version: {torch.__version__}\")\n",
    "    \n",
    "    # Get GPU memory info\n",
    "    gpu_mem = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
    "    print(f\"âœ“ GPU Memory: {gpu_mem:.1f} GB\")\n",
    "else:\n",
    "    print(\"âœ— GPU NOT AVAILABLE!\")\n",
    "    print(\"âš ï¸  Please enable GPU: Settings â†’ Accelerator â†’ GPU T4 x2 â†’ Save\")\n",
    "    raise RuntimeError(\"GPU not available. Training will be extremely slow on CPU.\")\n",
    "\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "print(\"Installing dependencies...\\n\")\n",
    "\n",
    "# Install from requirements.txt\n",
    "!pip install -q torch>=2.0.0 torchvision>=0.15.0\n",
    "!pip install -q albumentations>=1.3.0\n",
    "!pip install -q opencv-python-headless>=4.7.0\n",
    "!pip install -q pycocotools>=2.0.6\n",
    "!pip install -q tensorboard>=2.12.0\n",
    "!pip install -q tqdm pyyaml\n",
    "!pip install -q scikit-learn matplotlib seaborn\n",
    "\n",
    "print(\"\\nâœ“ All dependencies installed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Setup Dataset\n",
    "\n",
    "**Choose ONE of the following methods:**\n",
    "\n",
    "### **METHOD 1: Kaggle Dataset (Recommended)**\n",
    "1. Go to: https://www.kaggle.com/datasets\n",
    "2. Click \"New Dataset\"\n",
    "3. Upload TrashCAN dataset ZIP\n",
    "4. Add to notebook: \"Add Data\" â†’ Search for your dataset\n",
    "5. Update `DATASET_PATH` in the cell below\n",
    "\n",
    "### **METHOD 2: Google Drive (Alternative - Easiest)**\n",
    "1. Upload your TrashCAN dataset folder to Google Drive\n",
    "2. Share the folder/file publicly\n",
    "3. Get the file ID from the share link\n",
    "4. Update `GDRIVE_FILE_ID` in the cell below\n",
    "5. Set `USE_GDRIVE = True`\n",
    "\n",
    "### **METHOD 3: Direct Upload in Notebook**\n",
    "1. ZIP your dataset locally\n",
    "2. Upload ZIP to Kaggle notebook directly (< 500MB recommended)\n",
    "3. Set `USE_LOCAL_UPLOAD = True`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure dataset - Choose your method\n",
    "import os\n",
    "\n",
    "# ============================================================\n",
    "# CONFIGURATION - Choose ONE method\n",
    "# ============================================================\n",
    "\n",
    "# METHOD 1: Kaggle Dataset (â­ RECOMMENDED - Upload once, use forever!)\n",
    "USE_KAGGLE_DATASET = True  # â­ Set to True to use Kaggle Dataset\n",
    "KAGGLE_DATASET_PATH = '/kaggle/input/trashcan-dataset'  # Update with your dataset name\n",
    "\n",
    "# METHOD 2: Google Drive (Backup option - downloads each time)\n",
    "USE_GDRIVE = False  # Set to True to download from Google Drive\n",
    "GDRIVE_FILE_ID = '10PCbGqgVi0-XQn0EfGTTfSjwNS0JXR99'  # âœ… Your File ID (already set!)\n",
    "\n",
    "# ============================================================\n",
    "# Automatic Dataset Setup\n",
    "# ============================================================\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"ðŸ“¦ Dataset Setup\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "DATASET_PATH = None\n",
    "\n",
    "# METHOD 1: Kaggle Dataset\n",
    "if USE_KAGGLE_DATASET:\n",
    "    print(\"\\nðŸŒŸ Using Kaggle Dataset (Permanent Storage)...\")\n",
    "    print(\"   âœ… Fast: Instant access (no download)\")\n",
    "    print(\"   âœ… Persistent: Never deleted\")\n",
    "    print(\"   âœ… Free: No quota usage\")\n",
    "    \n",
    "    # Check if dataset exists\n",
    "    if os.path.exists(KAGGLE_DATASET_PATH):\n",
    "        # Check if it's the ZIP or extracted folder\n",
    "        if os.path.isfile(KAGGLE_DATASET_PATH):\n",
    "            print(f\"\\nâœ“ Found ZIP at: {KAGGLE_DATASET_PATH}\")\n",
    "            print(\"Extracting...\")\n",
    "            !unzip -q {KAGGLE_DATASET_PATH} -d /kaggle/working/\n",
    "            \n",
    "            if os.path.exists('/kaggle/working/trashcan'):\n",
    "                DATASET_PATH = '/kaggle/working/trashcan'\n",
    "                print(f\"âœ“ Extracted to: {DATASET_PATH}\")\n",
    "        elif os.path.isdir(KAGGLE_DATASET_PATH):\n",
    "            # Check for trashcan subdirectory\n",
    "            trashcan_path = os.path.join(KAGGLE_DATASET_PATH, 'trashcan')\n",
    "            if os.path.exists(trashcan_path):\n",
    "                DATASET_PATH = trashcan_path\n",
    "            else:\n",
    "                DATASET_PATH = KAGGLE_DATASET_PATH\n",
    "            print(f\"âœ“ Using dataset at: {DATASET_PATH}\")\n",
    "    else:\n",
    "        print(f\"\\nâŒ Dataset NOT FOUND at: {KAGGLE_DATASET_PATH}\")\n",
    "        print(\"\\nðŸ“ FIRST TIME SETUP REQUIRED:\")\n",
    "        print(\"   1. Go to: https://www.kaggle.com/datasets\")\n",
    "        print(\"   2. Click: '+ New Dataset'\")\n",
    "        print(\"   3. Upload: trashcan.zip (170 MB)\")\n",
    "        print(\"   4. Title: 'TrashCAN Dataset'\")\n",
    "        print(\"   5. Click: 'Create'\")\n",
    "        print(\"   6. In this notebook: '+ Add Data' â†’ Search 'TrashCAN'\")\n",
    "        print(\"   7. Update KAGGLE_DATASET_PATH above if needed\")\n",
    "        print(\"\\n   Available datasets:\")\n",
    "        if os.path.exists('/kaggle/input'):\n",
    "            for item in os.listdir('/kaggle/input'):\n",
    "                print(f\"     ðŸ“ /kaggle/input/{item}\")\n",
    "\n",
    "# METHOD 2: Google Drive\n",
    "elif USE_GDRIVE:\n",
    "    print(\"\\nðŸ”„ Using Google Drive (Downloads Each Time)...\")\n",
    "    print(\"   âš ï¸  Slower: 2-3 min download\")\n",
    "    print(\"   âš ï¸  Temporary: Deleted after session\")\n",
    "    print(\"   âš ï¸  Quota: Uses internet quota\")\n",
    "    print(\"\\nðŸ’¡ TIP: Consider using Kaggle Dataset instead!\")\n",
    "    \n",
    "    print(\"\\nInstalling gdown...\")\n",
    "    !pip install -q gdown\n",
    "    \n",
    "    print(f\"\\nDownloading dataset from Google Drive...\")\n",
    "    print(f\"File ID: {GDRIVE_FILE_ID}\")\n",
    "    !gdown --id {GDRIVE_FILE_ID} -O /kaggle/working/trashcan.zip\n",
    "    \n",
    "    if os.path.exists('/kaggle/working/trashcan.zip'):\n",
    "        file_size = os.path.getsize('/kaggle/working/trashcan.zip') / 1024 / 1024\n",
    "        print(f\"\\nâœ“ Downloaded: {file_size:.1f} MB\")\n",
    "        print(\"Extracting...\")\n",
    "        !unzip -q /kaggle/working/trashcan.zip -d /kaggle/working/\n",
    "        \n",
    "        if os.path.exists('/kaggle/working/trashcan'):\n",
    "            DATASET_PATH = '/kaggle/working/trashcan'\n",
    "            print(f\"âœ“ Extracted to: {DATASET_PATH}\")\n",
    "        else:\n",
    "            print(\"âŒ Extraction failed - trashcan folder not found\")\n",
    "            print(\"   Check ZIP structure\")\n",
    "    else:\n",
    "        print(\"âŒ Download failed! Check:\")\n",
    "        print(\"  1. File ID is correct\")\n",
    "        print(\"  2. File sharing: 'Anyone with link can view'\")\n",
    "        print(\"  3. Internet is enabled in Kaggle settings\")\n",
    "\n",
    "else:\n",
    "    print(\"\\nâŒ NO METHOD SELECTED!\")\n",
    "    print(\"   Please set either USE_KAGGLE_DATASET or USE_GDRIVE to True above\")\n",
    "\n",
    "# Verify final dataset\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "if DATASET_PATH and os.path.exists(DATASET_PATH):\n",
    "    print(f\"âœ… DATASET READY: {DATASET_PATH}\")\n",
    "    print(\"\\nðŸ“‚ Dataset structure:\")\n",
    "    for item in os.listdir(DATASET_PATH):\n",
    "        item_path = os.path.join(DATASET_PATH, item)\n",
    "        if os.path.isdir(item_path):\n",
    "            count = len(os.listdir(item_path))\n",
    "            print(f\"  ðŸ“ {item}/ ({count} items)\")\n",
    "        else:\n",
    "            size = os.path.getsize(item_path) / 1024\n",
    "            print(f\"  ðŸ“„ {item} ({size:.1f} KB)\")\n",
    "    \n",
    "    # Verify it has the correct structure\n",
    "    if os.path.exists(os.path.join(DATASET_PATH, 'images')):\n",
    "        img_count = len([f for f in os.listdir(os.path.join(DATASET_PATH, 'images')) if f.endswith('.jpg')])\n",
    "        print(f\"\\nâœ… Found {img_count:,} images\")\n",
    "    if os.path.exists(os.path.join(DATASET_PATH, 'annotations')):\n",
    "        ann_files = os.listdir(os.path.join(DATASET_PATH, 'annotations'))\n",
    "        print(f\"âœ… Found {len(ann_files)} annotation files\")\n",
    "else:\n",
    "    print(\"âŒ DATASET NOT READY!\")\n",
    "    print(\"\\nPlease:\")\n",
    "    print(\"1. Choose ONE method above (set to True)\")\n",
    "    print(\"2. Follow the setup instructions\")\n",
    "    print(\"3. Re-run this cell\")\n",
    "    \n",
    "print(\"=\"*60)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build YOLO-UDD model\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Ensure we're in the repo directory and it's in the path\n",
    "REPO_DIR = '/kaggle/working/YOLO-UDD-v2.0'\n",
    "\n",
    "# Change to repo directory if not already there\n",
    "if os.getcwd() != REPO_DIR:\n",
    "    if os.path.exists(REPO_DIR):\n",
    "        os.chdir(REPO_DIR)\n",
    "        print(f\"âœ“ Changed to: {os.getcwd()}\")\n",
    "    else:\n",
    "        print(f\"âŒ ERROR: Repository not found at {REPO_DIR}\")\n",
    "        print(\"   Please run Cell 3 (Clone Repository) first!\")\n",
    "        raise FileNotFoundError(f\"Repository not found. Run Cell 3 first.\")\n",
    "\n",
    "# Ensure repo is in Python path\n",
    "if REPO_DIR not in sys.path:\n",
    "    sys.path.insert(0, REPO_DIR)\n",
    "    print(f\"âœ“ Added to path: {REPO_DIR}\")\n",
    "\n",
    "# Now import the model\n",
    "from models.yolo_udd import build_yolo_udd\n",
    "import torch\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"ðŸ—ï¸  Building YOLO-UDD v2.0 Model\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Build model with 22 classes (TrashCAN dataset)\n",
    "model = build_yolo_udd(num_classes=22)\n",
    "\n",
    "# Move to GPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "\n",
    "print(f\"âœ“ Model built successfully\")\n",
    "print(f\"âœ“ Device: {device}\")\n",
    "print(f\"âœ“ Number of classes: 22\")\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"âœ“ Total parameters: {total_params:,}\")\n",
    "print(f\"âœ“ Trainable parameters: {trainable_params:,}\")\n",
    "\n",
    "# Test forward pass\n",
    "print(\"\\nðŸ§ª Testing forward pass...\")\n",
    "x = torch.randn(1, 3, 640, 640).to(device)\n",
    "with torch.no_grad():\n",
    "    predictions, turb_score = model(x)\n",
    "\n",
    "print(f\"âœ“ Forward pass successful!\")\n",
    "print(f\"âœ“ Turbidity Score: {turb_score.item():.4f}\")\n",
    "print(f\"âœ“ Detection scales: {len(predictions)}\")\n",
    "\n",
    "print(\"=\"*60)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Training Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training hyperparameters - Reduced for faster training\n",
    "EPOCHS = 100  # Reduced from 300 (10 hours instead of 30 hours)\n",
    "BATCH_SIZE = 8\n",
    "LEARNING_RATE = 0.01\n",
    "NUM_WORKERS = 2\n",
    "SAVE_DIR = '/kaggle/working/runs/train'\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"âš™ï¸  Training Configuration\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Epochs: {EPOCHS}\")\n",
    "print(f\"Batch Size: {BATCH_SIZE}\")\n",
    "print(f\"Learning Rate: {LEARNING_RATE}\")\n",
    "print(f\"Number of Workers: {NUM_WORKERS}\")\n",
    "print(f\"Save Directory: {SAVE_DIR}\")\n",
    "print(f\"Dataset Path: {DATASET_PATH}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Create save directory\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "print(f\"\\nâœ“ Save directory created: {SAVE_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Start Training\n",
    "\n",
    "**â±ï¸ Estimated Time**: ~10 hours for 100 epochs on T4 GPU\n",
    "\n",
    "**ðŸ’¡ Tips**:\n",
    "- Training will save checkpoints automatically\n",
    "- You can monitor progress in real-time\n",
    "- Results saved to `/kaggle/working/runs/train/`\n",
    "- Download best checkpoint from Output folder after training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start training\n",
    "print(\"=\"*60)\n",
    "print(\"ðŸš€ Starting Training...\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Training for {EPOCHS} epochs (~10 hours)\")\n",
    "print(f\"Expected mAP: 70-72%\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Run training script - CORRECTED ARGUMENTS\n",
    "!python scripts/train.py \\\n",
    "    --config configs/train_config.yaml \\\n",
    "    --data-dir {DATASET_PATH} \\\n",
    "    --epochs {EPOCHS} \\\n",
    "    --batch-size {BATCH_SIZE} \\\n",
    "    --lr {LEARNING_RATE} \\\n",
    "    --save-dir {SAVE_DIR}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Download Results\n",
    "\n",
    "After training completes, download the trained model checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check training results\n",
    "import os\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"ðŸ“Š Training Results\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "if os.path.exists(SAVE_DIR):\n",
    "    print(f\"\\nðŸ“ Results directory: {SAVE_DIR}\")\n",
    "    print(\"\\nContents:\")\n",
    "    for root, dirs, files in os.walk(SAVE_DIR):\n",
    "        level = root.replace(SAVE_DIR, '').count(os.sep)\n",
    "        indent = ' ' * 2 * level\n",
    "        print(f\"{indent}{os.path.basename(root)}/\")\n",
    "        subindent = ' ' * 2 * (level + 1)\n",
    "        for file in files:\n",
    "            size = os.path.getsize(os.path.join(root, file)) / (1024*1024)\n",
    "            print(f\"{subindent}{file} ({size:.1f} MB)\")\n",
    "    \n",
    "    # Check for best checkpoint\n",
    "    best_checkpoint = os.path.join(SAVE_DIR, 'best.pt')\n",
    "    if os.path.exists(best_checkpoint):\n",
    "        size = os.path.getsize(best_checkpoint) / (1024*1024)\n",
    "        print(f\"\\nâœ“ Best checkpoint: {best_checkpoint} ({size:.1f} MB)\")\n",
    "        print(\"\\nðŸ“¥ Download this file from the Output section!\")\n",
    "    else:\n",
    "        print(\"\\nâš ï¸  Best checkpoint not found. Check if training completed successfully.\")\n",
    "else:\n",
    "    print(f\"âœ— Results directory not found: {SAVE_DIR}\")\n",
    "\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸŽ‰ Training Complete!\n",
    "\n",
    "### Next Steps:\n",
    "1. **Download Checkpoint**: Download `best.pt` from Output folder\n",
    "2. **Evaluate Model**: Run evaluation script locally with downloaded checkpoint\n",
    "3. **Test Detections**: Test on new images\n",
    "\n",
    "### Expected Results:\n",
    "- mAP@50:95: **70-72%** (22 classes)\n",
    "- Training Time: **~10 hours** (100 epochs)\n",
    "- Checkpoint Size: **~200-300 MB**\n",
    "\n",
    "---\n",
    "\n",
    "**ðŸ“§ Issues?** Check the GitHub repository: https://github.com/kshitijkhede/YOLO-UDD-v2.0"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
