{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00b34df9",
   "metadata": {},
   "source": [
    "# üöÄ YOLO-UDD v2.0 Training on Kaggle - Fixed Version\n",
    "\n",
    "**Last Updated:** November 2, 2025\n",
    "\n",
    "## üìã Prerequisites\n",
    "1. Upload **TrashCAN annotations** dataset to Kaggle\n",
    "2. Upload **TrashCAN images** dataset to Kaggle\n",
    "3. Enable **GPU** in notebook settings (T4 or P100)\n",
    "4. Enable **Internet** in notebook settings\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68bf4842",
   "metadata": {},
   "source": [
    "## üîß Step 2: Install Dependencies\n",
    "**Uses albumentations 1.3.1** - Stable version without albucore dependency issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b4e710e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Clone repository\n",
    "if [ ! -d \"YOLO-UDD-v2.0\" ]; then\n",
    "    git clone https://github.com/kshitijkhede/YOLO-UDD-v2.0.git\n",
    "fi\n",
    "cd YOLO-UDD-v2.0\n",
    "echo \"‚úÖ Repository cloned\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e20c03f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd YOLO-UDD-v2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67c59ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EMERGENCY FIX: Install dependencies with AGGRESSIVE NumPy 1.26.4 locking\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "print(\"üì¶ Installing dependencies with LOCKED versions...\\n\")\n",
    "print(\"‚ö†Ô∏è  CRITICAL: NumPy MUST be 1.26.4 (NOT 2.x) to prevent TensorBoard/scikit-learn crashes\\n\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# STEP 1: Nuclear uninstall - remove ALL potentially conflicting packages\n",
    "print(\"üóëÔ∏è  Step 1/6: Removing all conflicting packages...\")\n",
    "subprocess.run([sys.executable, '-m', 'pip', 'uninstall', '-y', \n",
    "                'numpy', 'scipy', 'scikit-learn', 'tensorflow', \n",
    "                'tensorboard', 'keras', 'matplotlib', 'albumentations', 'albucore'],\n",
    "               stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n",
    "\n",
    "# STEP 2: Install NumPy 1.26.4 FIRST (CRITICAL!)\n",
    "print(\"üìç Step 2/6: Installing NumPy 1.26.4 (LOCKED with --no-cache-dir)...\")\n",
    "subprocess.run([sys.executable, '-m', 'pip', 'install', \n",
    "                '--no-cache-dir', '--force-reinstall', 'numpy==1.26.4', '-q'],\n",
    "               check=True)\n",
    "\n",
    "# STEP 3: Install PyTorch with CUDA 11.8\n",
    "print(\"üî• Step 3/6: Installing PyTorch 2.2.2 with CUDA 11.8...\")\n",
    "subprocess.run([sys.executable, '-m', 'pip', 'install', '-q',\n",
    "                'torch==2.2.2', 'torchvision==0.17.2', 'torchaudio==2.2.2',\n",
    "                '--index-url', 'https://download.pytorch.org/whl/cu118'],\n",
    "               check=True)\n",
    "\n",
    "# STEP 4: Install compatible scipy and matplotlib\n",
    "print(\"\udcca Step 4/6: Installing scipy 1.11.4 and matplotlib 3.7.5...\")\n",
    "subprocess.run([sys.executable, '-m', 'pip', 'install', '--no-cache-dir', '-q',\n",
    "                'scipy==1.11.4', 'matplotlib==3.7.5'],\n",
    "               check=True)\n",
    "\n",
    "# STEP 5: Install other core packages\n",
    "print(\"\ud83düì¶ Step 5/6: Installing core packages...\")\n",
    "subprocess.run([sys.executable, '-m', 'pip', 'install', '-q',\n",
    "                'opencv-python-headless==4.9.0.80', 'pillow==10.3.0', \n",
    "                'pycocotools==2.0.7', 'pyyaml==6.0.1', 'tqdm==4.66.4'],\n",
    "               check=True)\n",
    "\n",
    "print(\"üìä Installing TensorBoard 2.16.2...\")\n",
    "subprocess.run([sys.executable, '-m', 'pip', 'install', '--no-cache-dir', '-q',\n",
    "                'tensorboard==2.16.2'],\n",
    "               check=True)\n",
    "\n",
    "print(\"üé® Installing albumentations 1.3.1 (no albucore dependency)...\")\n",
    "subprocess.run([sys.executable, '-m', 'pip', 'install', '-q',\n",
    "                'albumentations==1.3.1'],\n",
    "               check=True)\n",
    "\n",
    "print(\"‚öôÔ∏è  Installing timm 0.9.16...\")\n",
    "subprocess.run([sys.executable, '-m', 'pip', 'install', '-q',\n",
    "                'timm==0.9.16'],\n",
    "               check=True)\n",
    "\n",
    "# STEP 6: Install scikit-learn LAST (needs stable NumPy)\n",
    "print(\"üî¨ Step 6/6: Installing scikit-learn 1.3.2 (LAST)...\")\n",
    "subprocess.run([sys.executable, '-m', 'pip', 'install', '--no-cache-dir', '-q',\n",
    "                'scikit-learn==1.3.2'],\n",
    "               check=True)\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"‚úÖ Dependencies installed!\\n\")\n",
    "print(\"‚ö†Ô∏è  You may see warnings about cesium/tsfresh/umap-learn - IGNORE THEM!\")\n",
    "print(\"    These are harmless Kaggle pre-installed packages that won't interfere.\\n\")\n",
    "print(\"üîç Next cell will verify NumPy 1.26.4 is correctly installed...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dd1859e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify critical dependencies and NumPy version\n",
    "print(\"üîç Verifying installations...\\n\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import cv2\n",
    "import albumentations as A\n",
    "\n",
    "print(f\"‚úÖ NumPy: {np.__version__}\")\n",
    "print(f\"‚úÖ PyTorch: {torch.__version__}\")\n",
    "print(f\"‚úÖ OpenCV: {cv2.__version__}\")\n",
    "print(f\"‚úÖ Albumentations: {A.__version__}\")\n",
    "\n",
    "# CRITICAL CHECK: NumPy MUST be 1.x (not 2.x)\n",
    "numpy_version = np.__version__\n",
    "if numpy_version.startswith('2.'):\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"‚ùå CRITICAL ERROR: NumPy 2.x detected!\")\n",
    "    print(f\"   Current version: {numpy_version}\")\n",
    "    print(\"   This will cause TensorBoard and scikit-learn to crash!\")\n",
    "    print(\"\\nüîß EMERGENCY FIX: Running aggressive NumPy downgrade...\")\n",
    "    print(\"=\"*70 + \"\\n\")\n",
    "    \n",
    "    # Emergency fix\n",
    "    import subprocess\n",
    "    import sys\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'uninstall', '-y', \n",
    "                    'numpy', 'scipy', 'scikit-learn'],\n",
    "                   stdout=subprocess.DEVNULL)\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', '--no-cache-dir',\n",
    "                    'numpy==1.26.4', 'scipy==1.11.4'],\n",
    "                   check=True)\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', '--no-cache-dir',\n",
    "                    'scikit-learn==1.3.2'],\n",
    "                   check=True)\n",
    "    \n",
    "    print(\"\\n‚úÖ NumPy downgraded to 1.26.4\")\n",
    "    print(\"‚ö†Ô∏è  IMPORTANT: Click 'Kernel' ‚Üí 'Restart & Run All' NOW!\")\n",
    "    raise SystemExit(\"NumPy fixed - RESTART KERNEL and re-run all cells!\")\n",
    "\n",
    "print(f\"‚úÖ NumPy version is CORRECT: {numpy_version}\")\n",
    "\n",
    "# Test scikit-learn\n",
    "try:\n",
    "    import sklearn\n",
    "    print(f\"‚úÖ scikit-learn: {sklearn.__version__}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è  scikit-learn import warning: {e}\")\n",
    "\n",
    "# Test TensorBoard (this is the critical one that crashes with NumPy 2.x)\n",
    "try:\n",
    "    from torch.utils.tensorboard import SummaryWriter\n",
    "    print(\"‚úÖ TensorBoard: Import successful (NumPy compatibility confirmed)\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå TensorBoard import FAILED: {e}\")\n",
    "    print(\"   This means NumPy is incompatible - restart kernel!\")\n",
    "    raise\n",
    "\n",
    "# Test albumentations\n",
    "try:\n",
    "    transform = A.Compose([A.HorizontalFlip(p=0.5)])\n",
    "    print(\"‚úÖ Albumentations: Transform test passed\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Albumentations test failed: {e}\")\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"‚úÖ ALL VERIFICATIONS PASSED!\")\n",
    "print(\"üìç NumPy 1.26.4 is locked and compatible with all packages\")\n",
    "print(\"üöÄ Safe to proceed with training!\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e69788",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FINAL SAFETY CHECK: Verify NumPy before training starts\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "print(\"üîí Final NumPy version check before training...\\n\")\n",
    "\n",
    "result = subprocess.run([sys.executable, '-m', 'pip', 'show', 'numpy'], \n",
    "                       capture_output=True, text=True)\n",
    "current_numpy = None\n",
    "for line in result.stdout.split('\\n'):\n",
    "    if line.startswith('Version:'):\n",
    "        current_numpy = line.split(':')[1].strip()\n",
    "        break\n",
    "\n",
    "if current_numpy and current_numpy.startswith('2.'):\n",
    "    print(\"=\"*70)\n",
    "    print(f\"‚ùå CRITICAL: NumPy {current_numpy} detected!\")\n",
    "    print(\"   This will crash during training!\")\n",
    "    print(\"\\nüîß EMERGENCY DOWNGRADE IN PROGRESS...\")\n",
    "    print(\"=\"*70 + \"\\n\")\n",
    "    \n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'uninstall', '-y', \n",
    "                    'numpy', 'scipy', 'scikit-learn'],\n",
    "                   stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', '--no-cache-dir',\n",
    "                    'numpy==1.26.4', 'scipy==1.11.4'],\n",
    "                   check=True)\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', '--no-cache-dir',\n",
    "                    'scikit-learn==1.3.2'],\n",
    "                   check=True)\n",
    "    \n",
    "    print(\"‚úÖ Fixed! Downgraded to NumPy 1.26.4\")\n",
    "    print(\"‚ö†Ô∏è  NOW: Click 'Kernel' ‚Üí 'Restart & Run All'\")\n",
    "    raise SystemExit(\"NumPy fixed - restart required!\")\n",
    "else:\n",
    "    print(f\"‚úÖ NumPy {current_numpy} is CORRECT\")\n",
    "    print(\"‚úÖ TensorBoard and scikit-learn will work properly\")\n",
    "    print(\"‚úÖ Safe to proceed with training\")\n",
    "    print(\"\\nüöÄ Starting training in next cell...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c637903",
   "metadata": {},
   "source": [
    "## üìä Step 2: Setup Dataset Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a8237a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import json\n",
    "\n",
    "print(\"üîç Setting up dataset paths...\\n\")\n",
    "\n",
    "# Create directory structure\n",
    "os.makedirs('data/trashcan/annotations', exist_ok=True)\n",
    "os.makedirs('data/trashcan/images', exist_ok=True)\n",
    "\n",
    "# === MODIFY THESE PATHS TO MATCH YOUR KAGGLE DATASETS ===\n",
    "ANNOTATIONS_PATH = '/kaggle/input/trashcan-annotations-coco-format/annotations'\n",
    "IMAGES_PATH = '/kaggle/input/trashcan/images'\n",
    "\n",
    "# Alternative paths (uncomment and modify if needed)\n",
    "# ANNOTATIONS_PATH = '/kaggle/input/YOUR-ANNOTATIONS-DATASET-NAME/'\n",
    "# IMAGES_PATH = '/kaggle/input/YOUR-IMAGES-DATASET-NAME/'\n",
    "\n",
    "print(f\"Annotations source: {ANNOTATIONS_PATH}\")\n",
    "print(f\"Images source: {IMAGES_PATH}\")\n",
    "print(\"\\n\" + \"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba393603",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Link annotations\n",
    "print(\"üìã Copying annotations...\")\n",
    "\n",
    "train_json = os.path.join(ANNOTATIONS_PATH, 'train.json')\n",
    "val_json = os.path.join(ANNOTATIONS_PATH, 'val.json')\n",
    "\n",
    "if os.path.exists(train_json) and os.path.exists(val_json):\n",
    "    shutil.copy(train_json, 'data/trashcan/annotations/train.json')\n",
    "    shutil.copy(val_json, 'data/trashcan/annotations/val.json')\n",
    "    \n",
    "    # Verify\n",
    "    with open('data/trashcan/annotations/train.json', 'r') as f:\n",
    "        train_data = json.load(f)\n",
    "    with open('data/trashcan/annotations/val.json', 'r') as f:\n",
    "        val_data = json.load(f)\n",
    "    \n",
    "    print(f\"‚úÖ Train: {len(train_data['images'])} images, {len(train_data['annotations'])} annotations\")\n",
    "    print(f\"‚úÖ Val: {len(val_data['images'])} images, {len(val_data['annotations'])} annotations\")\n",
    "    print(f\"‚úÖ Categories: {len(train_data['categories'])}\")\n",
    "else:\n",
    "    print(f\"‚ùå Annotations not found!\")\n",
    "    print(f\"   Looking for: {train_json}\")\n",
    "    print(f\"   Please update ANNOTATIONS_PATH in the cell above\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73664a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Link images (symbolic links to save space)\n",
    "print(\"üñºÔ∏è  Linking images...\")\n",
    "\n",
    "train_imgs_src = os.path.join(IMAGES_PATH, 'train')\n",
    "val_imgs_src = os.path.join(IMAGES_PATH, 'val')\n",
    "\n",
    "train_imgs_dst = 'data/trashcan/images/train'\n",
    "val_imgs_dst = 'data/trashcan/images/val'\n",
    "\n",
    "# Remove old links\n",
    "for path in [train_imgs_dst, val_imgs_dst]:\n",
    "    if os.path.exists(path):\n",
    "        if os.path.islink(path):\n",
    "            os.unlink(path)\n",
    "        else:\n",
    "            shutil.rmtree(path)\n",
    "\n",
    "# Create symbolic links\n",
    "if os.path.exists(train_imgs_src) and os.path.exists(val_imgs_src):\n",
    "    os.symlink(train_imgs_src, train_imgs_dst)\n",
    "    os.symlink(val_imgs_src, val_imgs_dst)\n",
    "    \n",
    "    train_count = len([f for f in os.listdir(train_imgs_dst) if f.endswith('.jpg')])\n",
    "    val_count = len([f for f in os.listdir(val_imgs_dst) if f.endswith('.jpg')])\n",
    "    \n",
    "    print(f\"‚úÖ Train images: {train_count}\")\n",
    "    print(f\"‚úÖ Val images: {val_count}\")\n",
    "    \n",
    "    if train_count > 0 and val_count > 0:\n",
    "        print(\"\\nüéâ Dataset is ready for training!\")\n",
    "else:\n",
    "    print(f\"‚ùå Images not found!\")\n",
    "    print(f\"   Looking for: {train_imgs_src}\")\n",
    "    print(f\"   Please update IMAGES_PATH in the cell above\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee5a93e0",
   "metadata": {},
   "source": [
    "## üîç Step 3: Verify GPU and PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e23a5c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "print(f\"PyTorch Version: {torch.__version__}\")\n",
    "print(f\"CUDA Available: {torch.cuda.is_available()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"‚úÖ GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"‚úÖ GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "    torch.cuda.empty_cache()\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  WARNING: GPU not available!\")\n",
    "    print(\"   Go to Settings ‚Üí Accelerator ‚Üí Select GPU T4 or P100\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59ff1ad0",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è Step 4: Create Optimized Training Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4247bd07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "# Create optimized config for Kaggle\n",
    "config = {\n",
    "    'model': {\n",
    "        'name': 'YOLO-UDD-v2.0',\n",
    "        'num_classes': 22,\n",
    "        'pretrained_path': None\n",
    "    },\n",
    "    'data': {\n",
    "        'dataset_name': 'TrashCAN-1.0',\n",
    "        'data_dir': 'data/trashcan',\n",
    "        'img_size': 640,\n",
    "        'class_names': [\n",
    "            \"rov\", \"plant\", \"animal_fish\", \"animal_starfish\", \"animal_shells\",\n",
    "            \"animal_crab\", \"animal_eel\", \"animal_etc\", \"trash_clothing\", \"trash_pipe\",\n",
    "            \"trash_bottle\", \"trash_bag\", \"trash_snack_wrapper\", \"trash_can\", \"trash_cup\",\n",
    "            \"trash_container\", \"trash_unknown_instance\", \"trash_branch\", \"trash_wreckage\",\n",
    "            \"trash_tarp\", \"trash_rope\", \"trash_net\"\n",
    "        ]\n",
    "    },\n",
    "    'training': {\n",
    "        'epochs': 100,\n",
    "        'batch_size': 8,           # Optimized for T4 GPU\n",
    "        'num_workers': 2,\n",
    "        'optimizer': 'AdamW',\n",
    "        'learning_rate': 0.001,    # Lower initial LR for stability\n",
    "        'weight_decay': 0.0005,\n",
    "        'scheduler': 'CosineAnnealing',\n",
    "        'lr_min': 0.00001,\n",
    "        'early_stopping_patience': 30,\n",
    "        'grad_clip_norm': 10.0,\n",
    "        'use_amp': True            # Mixed precision\n",
    "    },\n",
    "    'loss': {\n",
    "        'lambda_box': 5.0,\n",
    "        'lambda_obj': 1.0,\n",
    "        'lambda_cls': 1.0,\n",
    "        'focal_loss_gamma': 2.0,\n",
    "        'iou_type': 'CIoU'\n",
    "    },\n",
    "    'augmentation': {\n",
    "        'use_augmentation': True,\n",
    "        'horizontal_flip_prob': 0.5,\n",
    "        'color_jitter': True,\n",
    "        'gaussian_blur': False,     # Disabled to reduce training time\n",
    "        'underwater_augmentation': True\n",
    "    },\n",
    "    'checkpoints': {\n",
    "        'save_dir': '/kaggle/working/checkpoints',\n",
    "        'save_interval': 10,\n",
    "        'save_best_only': False\n",
    "    },\n",
    "    'logging': {\n",
    "        'use_tensorboard': True,\n",
    "        'log_dir': '/kaggle/working/runs',\n",
    "        'log_interval': 50\n",
    "    },\n",
    "    'eval': {\n",
    "        'conf_threshold': 0.001,\n",
    "        'nms_threshold': 0.6,\n",
    "        'eval_interval': 5\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save config\n",
    "os.makedirs('configs', exist_ok=True)\n",
    "with open('configs/kaggle_config.yaml', 'w') as f:\n",
    "    yaml.dump(config, f, default_flow_style=False)\n",
    "\n",
    "print(\"‚úÖ Training config created!\")\n",
    "print(\"\\nKey settings:\")\n",
    "print(f\"  - Batch size: {config['training']['batch_size']}\")\n",
    "print(f\"  - Epochs: {config['training']['epochs']}\")\n",
    "print(f\"  - Learning rate: {config['training']['learning_rate']}\")\n",
    "print(f\"  - Image size: {config['data']['img_size']}\")\n",
    "print(f\"  - Mixed precision: {config['training']['use_amp']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b06cfa09",
   "metadata": {},
   "source": [
    "## üöÄ Step 5: Start Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acafa2c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "\n",
    "# Check for existing checkpoints to resume from\n",
    "checkpoint_dir = '/kaggle/working/runs/train/checkpoints'\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "# Look for latest.pt checkpoint (new format)\n",
    "latest_checkpoint = os.path.join(checkpoint_dir, 'latest.pt')\n",
    "\n",
    "if os.path.exists(latest_checkpoint):\n",
    "    print(f\"üîÑ Found checkpoint: {latest_checkpoint}\")\n",
    "    print(\"   Will resume training from this checkpoint\\n\")\n",
    "    \n",
    "    # Load checkpoint to show progress\n",
    "    import torch\n",
    "    ckpt = torch.load(latest_checkpoint, map_location='cpu')\n",
    "    print(f\"   \udcca Previous progress:\")\n",
    "    print(f\"      - Completed epoch: {ckpt['epoch']}\")\n",
    "    print(f\"      - Best mAP: {ckpt['best_map']:.4f}\")\n",
    "    print(f\"      - Resuming from epoch: {ckpt['epoch'] + 1}\\n\")\n",
    "    \n",
    "    resume_flag = f\"--resume {latest_checkpoint}\"\n",
    "else:\n",
    "    print(\"üÜï No previous checkpoint found\")\n",
    "    print(\"   Starting fresh training from epoch 0\\n\")\n",
    "    resume_flag = \"\"\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"üöÄ Starting/Resuming YOLO-UDD v2.0 Training\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12504dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run training\n",
    "!python scripts/train.py --config configs/kaggle_config.yaml {resume_flag}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0217e3f3",
   "metadata": {},
   "source": [
    "## üíæ Step 6: Check Checkpoints (Auto-Saved)\n",
    "**Checkpoints are automatically saved every epoch!** Use this cell to view them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a6a8558",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import glob\n",
    "import os\n",
    "\n",
    "print(\"üíæ Checkpoint Information...\\n\")\n",
    "\n",
    "# Check checkpoint directory\n",
    "checkpoint_dir = '/kaggle/working/runs/train/checkpoints'\n",
    "\n",
    "if os.path.exists(checkpoint_dir):\n",
    "    checkpoints = glob.glob(f'{checkpoint_dir}/*.pt')\n",
    "    \n",
    "    if checkpoints:\n",
    "        print(f\"‚úÖ Found {len(checkpoints)} checkpoint(s):\\n\")\n",
    "        \n",
    "        for ckpt in checkpoints:\n",
    "            size = os.path.getsize(ckpt) / (1024*1024)\n",
    "            name = os.path.basename(ckpt)\n",
    "            print(f\"   üì¶ {name} ({size:.1f} MB)\")\n",
    "            \n",
    "            # Show details for latest checkpoint\n",
    "            if 'latest.pt' in name:\n",
    "                import torch\n",
    "                ckpt_data = torch.load(ckpt, map_location='cpu')\n",
    "                print(f\"      - Epoch: {ckpt_data['epoch']}\")\n",
    "                print(f\"      - Best mAP: {ckpt_data['best_map']:.4f}\")\n",
    "        \n",
    "        print(f\"\\n‚úÖ Checkpoints are in: {checkpoint_dir}\")\n",
    "        print(\"üí° These checkpoints persist and allow training to resume!\")\n",
    "        \n",
    "        # Show download instructions\n",
    "        print(\"\\nüì• To download checkpoints:\")\n",
    "        print(\"   1. Click 'Output' in right sidebar\")\n",
    "        print(\"   2. Find checkpoint files\")\n",
    "        print(\"   3. Click download icon\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è  No .pt checkpoint files found\")\n",
    "        print(\"   Training may still be in progress or just started\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Checkpoint directory not found\")\n",
    "    print(\"   Training has not started yet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc213298",
   "metadata": {},
   "source": [
    "## üìä Step 7: View Training Logs (TensorBoard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1565c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load TensorBoard\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir /kaggle/working/runs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c94239a",
   "metadata": {},
   "source": [
    "## üéØ Step 8: Evaluate Model (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2cba062",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find best checkpoint\n",
    "import glob\n",
    "\n",
    "best_ckpt = glob.glob('/kaggle/working/checkpoints/best.pth')\n",
    "\n",
    "if best_ckpt:\n",
    "    print(f\"üìä Evaluating model: {best_ckpt[0]}\\n\")\n",
    "    !python scripts/evaluate.py \\\n",
    "        --checkpoint {best_ckpt[0]} \\\n",
    "        --data-dir data/trashcan \\\n",
    "        --split val\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  No 'best.pth' checkpoint found\")\n",
    "    print(\"   Training may still be in progress\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea44dfed",
   "metadata": {},
   "source": [
    "## üñºÔ∏è Step 9: Run Detection on Sample Images (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcd28665",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run detection on validation images\n",
    "import glob\n",
    "\n",
    "best_ckpt = glob.glob('/kaggle/working/checkpoints/best.pth')\n",
    "\n",
    "if best_ckpt:\n",
    "    print(f\"üéØ Running detection with: {best_ckpt[0]}\\n\")\n",
    "    !python scripts/detect.py \\\n",
    "        --checkpoint {best_ckpt[0]} \\\n",
    "        --source data/trashcan/images/val/ \\\n",
    "        --output /kaggle/working/results/ \\\n",
    "        --max-images 10\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  No checkpoint found for detection\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e8a2eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display detection results\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import glob\n",
    "import os\n",
    "\n",
    "result_images = glob.glob('/kaggle/working/results/*.jpg')[:6]\n",
    "\n",
    "if result_images:\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for idx, img_path in enumerate(result_images):\n",
    "        img = Image.open(img_path)\n",
    "        axes[idx].imshow(img)\n",
    "        axes[idx].axis('off')\n",
    "        axes[idx].set_title(f'Detection {idx+1}')\n",
    "    \n",
    "    # Hide empty subplots\n",
    "    for idx in range(len(result_images), 6):\n",
    "        axes[idx].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  No detection results found\")\n",
    "    print(\"   Run the detection cell above first\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "957bfa64",
   "metadata": {},
   "source": [
    "## üì• Step 10: Download Checkpoints (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e53d69f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all available checkpoints\n",
    "import glob\n",
    "import os\n",
    "\n",
    "checkpoints = glob.glob('/kaggle/working/checkpoints/*.pth')\n",
    "\n",
    "if checkpoints:\n",
    "    print(\"üì¶ Available checkpoints:\\n\")\n",
    "    for ckpt in sorted(checkpoints):\n",
    "        size = os.path.getsize(ckpt) / (1024*1024)\n",
    "        print(f\"  - {os.path.basename(ckpt)} ({size:.1f} MB)\")\n",
    "    \n",
    "    print(\"\\nüí° To download, you can:\")\n",
    "    print(\"  1. Use Kaggle's file browser (right sidebar)\")\n",
    "    print(\"  2. Navigate to /kaggle/working/checkpoints/\")\n",
    "    print(\"  3. Right-click on files to download\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  No checkpoints found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "711bb6cd",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéâ Training Complete!\n",
    "\n",
    "### Next Steps:\n",
    "1. **Download checkpoints** from `/kaggle/working/checkpoints/`\n",
    "2. **View TensorBoard** logs to analyze training\n",
    "3. **Run evaluation** to see final metrics\n",
    "4. **Test on new images** using `detect.py`\n",
    "\n",
    "### Tips for Better Results:\n",
    "- Train for more epochs (increase `epochs` in config)\n",
    "- Adjust learning rate if loss plateaus\n",
    "- Try different batch sizes based on GPU memory\n",
    "- Enable more augmentations for better generalization\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19923deb",
   "metadata": {},
   "source": [
    "## üîÑ How Checkpoint Resume Works\n",
    "\n",
    "**Your training is protected!** Checkpoints are saved automatically.\n",
    "\n",
    "### What Gets Saved:\n",
    "- ‚úÖ **latest.pt** - Saved after every epoch\n",
    "- ‚úÖ **best.pt** - Saved when validation improves\n",
    "- üìÅ **Location**: `/kaggle/working/runs/train/checkpoints/`\n",
    "\n",
    "### Auto-Resume:\n",
    "If training stops (timeout, disconnect, etc.), just **re-run the notebook**:\n",
    "1. Re-run cells 1-4 (dependencies, setup)\n",
    "2. Cell 16 will **auto-detect** `latest.pt`\n",
    "3. Training **continues** from where it stopped!\n",
    "\n",
    "### Manual Resume (if needed):\n",
    "```python\n",
    "!python scripts/train.py \\\n",
    "    --config configs/kaggle_config.yaml \\\n",
    "    --resume /kaggle/working/runs/train/checkpoints/latest.pt\n",
    "```\n",
    "\n",
    "**No progress lost!** üéâ"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
